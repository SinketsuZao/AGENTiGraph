export const UNION_ANNOTATION_TEXT = [
  {
    id: 1,
    content:
      "For instance, we may wish to retrieve documents containing the term carrot when the user types the query carot. Google reports (http://www.google.com/jobs/britney.html) that the following are all treated as misspellings of the query britney spears: britian spears, britney\\'s spears, brandy spears and prittany spears. We look at two steps to solving this problem: the first based on edit distance and the second based on $k$-gram overlap. Before getting into the algorithmic details of these methods, we first review how search engines provide spell-correction as part of a user experience.",
  },
  {
    id: 2,
    content:
      "Structured prediction or structured (output) learning is an umbrella term for supervised machine learning techniques that involves predicting structured objects, rather than scalar discrete or real values.Similar to commonly used supervised learning techniques, structured prediction models are typically trained by means of observed data in which the true prediction value is used to adjust model parameters. Due to the complexity of the model and the interrelations of predicted variables the process of prediction using a trained model and of training itself is often computationally infeasible and approximate inference and learning methods are used.",
  },
  {
    id: 3,
    content:
      "Pointer networks are a variation of the sequence-to-sequence model with attention. Instead of translating one sequence into another, they yield a succession of pointers to the elements of the input series. The most basic use of this is ordering the elements of a variable-length sequence or set.",
  },
  {
    id: 4,
    content:
      'Spectral methods are a class of techniques used in applied mathematics and scientific computing to numerically solve certain differential equations, potentially involving the use of the fast Fourier transform. The idea is to write the solution of the differential equation as a sum of certain "basis functions" (for example, as a Fourier series which is a sum of sinusoids) and then to choose the coefficients in the sum in order to satisfy the differential equation as well as possible.\nSpectral methods and finite element methods are closely related and built on the same ideas; the main difference between them is that spectral methods use basis functions that are nonzero over the whole domain, while finite element methods use basis functions that are nonzero only on small subdomains. In other words, spectral methods take on a global approach while finite element methods use a local approach. Partially for this reason, spectral methods have excellent error properties, with the so-called "exponential convergence" being the fastest possible, when the solution is smooth. However, there are no known three-dimensional single domain spectral shock capturing results (shock waves are not smooth). In the finite element community, a method where the degree of the elements is very high or increases as the grid parameter h decreases to zero is sometimes called a spectral element method.\nSpectral methods can be used to solve ordinary differential equations (ODEs), partial differential equations (PDEs) and eigenvalue problems involving differential equations. When applying spectral methods to time-dependent PDEs, the solution is typically written as a sum of basis functions with time-dependent coefficients; substituting this in the PDE yields a system of ODEs in the coefficients which can be solved using any numerical method for ODEs. Eigenvalue problems for ODEs are similarly converted to matrix eigenvalue problems.\nSpectral methods were developed in a long series of papers by Steven Orszag starting in 1969 including, but not limited to, Fourier series methods for periodic geometry problems, polynomial spectral methods for finite and unbounded geometry problems, pseudospectral methods for highly nonlinear problems, and spectral iteration methods for fast solution of steady state problems. The implementation of the spectral method is normally accomplished either with collocation or a Galerkin or a Tau approach.\nSpectral methods are computationally less expensive than finite element methods, but become less accurate for problems with complex geometries and discontinuous coefficients. This increase in error is a consequence of the Gibbs phenomenon.',
  },
  {
    id: 5,
    content:
      "A Graph Neural Network, also known as a Graph Convolutional Network (GCN), is an image classification method. In this article, we’ll provide an introduction to the concepts of graphs, convolutional neural networks, and Graph Neural Networks.",
  },
  {
    id: 6,
    content:
      "In linguistics, syntax () is the set of rules, principles, and processes that govern the structure of sentences (sentence structure) in a given language, usually including word order. The term syntax is also used to refer to the study of such principles and processes. The goal of many syntacticians is to discover the syntactic rules common to all languages.",
  },
  {
    id: 7,
    content:
      "A Markov decision process (MDP) is a discrete time stochastic control process. It provides a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker. MDPs are useful for studying optimization problems solved via dynamic programming and reinforcement learning. MDPs were known at least as early as the 1950s; a core body of research on Markov decision processes resulted from Ronald Howard's 1960 book, Dynamic Programming and Markov Processes. They are used in many disciplines, including robotics, automatic control, economics and manufacturing. The name of MDPs comes from the Russian mathematician Andrey Markov as they are an extension of the Markov chains.\nAt each time step, the process is in some state \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  , and the decision maker may choose any action \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n   that is available in state \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  . The process responds at the next time step by randomly moving into a new state \n  \n    \n      \n        \n          s\n          \u2032\n        \n      \n    \n    {\\displaystyle s'}\n  , and giving the decision maker a corresponding reward \n  \n    \n      \n        \n          R\n          \n            a\n          \n        \n        (\n        s\n        ,\n        \n          s\n          \u2032\n        \n        )\n      \n    \n    {\\displaystyle R_{a}(s,s')}\n  .\nThe probability that the process moves into its new state \n  \n    \n      \n        \n          s\n          \u2032\n        \n      \n    \n    {\\displaystyle s'}\n   is influenced by the chosen action. Specifically, it is given by the state transition function \n  \n    \n      \n        \n          P\n          \n            a\n          \n        \n        (\n        s\n        ,\n        \n          s\n          \u2032\n        \n        )\n      \n    \n    {\\displaystyle P_{a}(s,s')}\n  . Thus, the next state \n  \n    \n      \n        \n          s\n          \u2032\n        \n      \n    \n    {\\displaystyle s'}\n   depends on the current state \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n   and the decision maker's action \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  . But given \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n   and \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  , it is conditionally independent of all previous states and actions; in other words, the state transitions of an MDP satisfy the Markov property.\nMarkov decision processes are an extension of Markov chains; the difference is the addition of actions (allowing choice) and rewards (giving motivation). Conversely, if only one action exists for each state (e.g. \"wait\") and all rewards are the same (e.g. \"zero\"), a Markov decision process reduces to a Markov chain.",
  },
  {
    id: 8,
    content:
      "Sentiment analysis (also known as opinion mining or emotion AI) refers to the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. Sentiment analysis is widely applied to voice of the customer materials such as reviews and survey responses, online and social media, and healthcare materials for applications that range from marketing to customer service to clinical medicine.\n\n",
  },
  {
    id: 9,
    content:
      'Planning is the process of thinking about the activities required to achieve a desired goal. It is the first and foremost activity to achieve desired results. It involves the creation and maintenance of a plan, such as psychological aspects that require conceptual skills. There are even a couple of tests to measure someone\u2019s capability of planning well. As such, planning is a fundamental property of intelligent behavior.  An important further meaning, often just called "planning" is the legal context of permitted building developments.\nAlso, planning has a specific process and is necessary for multiple occupations (particularly in fields such as management, business, etc.). In each field there are different types of plans that help companies achieve efficiency and effectiveness. An important, albeit often ignored aspect of planning, is the relationship it holds to forecasting. Forecasting can be described as predicting what the future will look like, whereas planning predicts what the future should look like for multiple scenarios. Planning combines forecasting with preparation of scenarios and how to react to them. Planning is one of the most important project management and time management techniques. Planning is preparing a sequence of action steps to achieve some specific goal. If a person does it effectively, they can reduce much the necessary time and effort of achieving the goal. A plan is like a map. When following a plan, a person can see how much they have progressed towards their project goal and how far they are from their destination.',
  },
  {
    id: 10,
    content:
      'Calculus, originally called infinitesimal calculus or "the calculus of infinitesimals", is the mathematical study of continuous change, in the same way that geometry is the study of shape and algebra is the study of generalizations of arithmetic operations.\nIt has two major branches, differential calculus and integral calculus; the former concerns instantaneous rates of change, and the slopes of curves, while  integral calculus concerns accumulation of quantities, and areas under or between curves. These two branches are related to each other by the fundamental theorem of calculus, and they make use of the fundamental notions of convergence of infinite sequences and infinite series to a well-defined limit.Infinitesimal calculus was developed independently in the late 17th century by Isaac Newton and Gottfried Wilhelm Leibniz. Today, calculus has widespread uses in science, engineering, and economics.In mathematics education, calculus denotes courses of elementary mathematical analysis, which are mainly  devoted to the study of functions and limits. The word calculus (plural calculi) is a  Latin word, meaning originally "small pebble" (this meaning is kept in medicine). Because such pebbles were used for calculation, the meaning of the word has evolved and today usually means a method of computation. It is therefore used for naming specific methods of calculation and related theories, such as propositional calculus, Ricci calculus, calculus of variations, lambda calculus, and process calculus.',
  },
  {
    id: 11,
    content:
      'Shallow parsing (also chunking, "light parsing") is an analysis of a sentence which first identifies constituent parts of sentences (nouns, verbs, adjectives, etc.) and then links them to higher order units that have discrete grammatical meanings (noun groups or phrases, verb groups, etc.). While the most elementary chunking algorithms simply link constituent parts on the basis of elementary search patterns (e.g., as specified by regular expressions), approaches that use machine learning techniques (classifiers, topic modeling, etc.) can take contextual information into account and thus compose chunks in such a way that they better reflect the semantic relations between the basic constituents. That is, these more advanced methods get around the problem that combinations of elementary constituents can have different higher level meanings depending on the context of the sentence.  \nIt is a technique widely used in natural language processing. It is similar to the concept of lexical analysis for computer languages. Under the name, "shallow structure hypothesis", it is also used as an explanation for why second language learners often fail to parse complex sentences correctly.',
  },
  {
    id: 12,
    content:
      "In mathematics, mathematical physics and the theory of stochastic processes, a harmonic function is a twice continuously differentiable function f : U \u2192 R, where U is an open subset of Rn, that satisfies Laplace's equation, that is,\n\n  \n    \n      \n        \n          \n            \n              \n                \u2202\n                \n                  2\n                \n              \n              f\n            \n            \n              \u2202\n              \n                x\n                \n                  1\n                \n                \n                  2\n                \n              \n            \n          \n        \n        +\n        \n          \n            \n              \n                \u2202\n                \n                  2\n                \n              \n              f\n            \n            \n              \u2202\n              \n                x\n                \n                  2\n                \n                \n                  2\n                \n              \n            \n          \n        \n        +\n        \u22ef\n        +\n        \n          \n            \n              \n                \u2202\n                \n                  2\n                \n              \n              f\n            \n            \n              \u2202\n              \n                x\n                \n                  n\n                \n                \n                  2\n                \n              \n            \n          \n        \n        =\n        0\n      \n    \n    {\\displaystyle {\\frac {\\partial ^{2}f}{\\partial x_{1}^{2}}}+{\\frac {\\partial ^{2}f}{\\partial x_{2}^{2}}}+\\cdots +{\\frac {\\partial ^{2}f}{\\partial x_{n}^{2}}}=0}\n  everywhere on U. This is usually written as\n\n  \n    \n      \n        \n          \u2207\n          \n            2\n          \n        \n        f\n        =\n        0\n      \n    \n    {\\displaystyle \\nabla ^{2}f=0}\n  or\n\n  \n    \n      \n        \n          \u0394\n          f\n          =\n          0\n        \n      \n    \n    {\\displaystyle \\textstyle \\Delta f=0}",
  },
  {
    id: 13,
    content:
      "In natural language processing, language identification or language guessing is the problem of determining which natural language given content is in. Computational approaches to this problem view it as a special case of text categorization, solved with various statistical methods.",
  },
  {
    id: 14,
    content:
      "Semi-supervised learning is an approach to machine learning that combines a small amount of labeled data with a large amount of unlabeled data during training. Semi-supervised learning falls between unsupervised learning (with no labeled training data) and supervised learning (with only labeled training data).\nUnlabeled data, when used in conjunction with a small amount of labeled data, can produce considerable improvement in learning accuracy. The acquisition of labeled data for a learning problem often requires a skilled human agent (e.g. to transcribe an audio segment) or a physical experiment (e.g. determining the 3D structure of a protein or determining whether there is oil at a particular location). The cost associated with the labeling process thus may render large, fully labeled training sets infeasible, whereas acquisition of unlabeled data is relatively inexpensive. In such situations, semi-supervised learning can be of great practical value. Semi-supervised learning is also of theoretical interest in machine learning and as a model for human learning.\nA set of \n  \n    \n      \n        l\n      \n    \n    {\\displaystyle l}\n   independently identically distributed examples \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            l\n          \n        \n        \u2208\n        X\n      \n    \n    {\\displaystyle x_{1},\\dots ,x_{l}\\in X}\n   with corresponding labels \n  \n    \n      \n        \n          y\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          y\n          \n            l\n          \n        \n        \u2208\n        Y\n      \n    \n    {\\displaystyle y_{1},\\dots ,y_{l}\\in Y}\n   and \n  \n    \n      \n        u\n      \n    \n    {\\displaystyle u}\n   unlabeled examples \n  \n    \n      \n        \n          x\n          \n            l\n            +\n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            l\n            +\n            u\n          \n        \n        \u2208\n        X\n      \n    \n    {\\displaystyle x_{l+1},\\dots ,x_{l+u}\\in X}\n   are processed. Semi-supervised learning combines this information to surpass the classification performance that can be obtained either by discarding the unlabeled data and doing supervised learning or by discarding the labels and doing unsupervised learning.\nSemi-supervised learning may refer to either transductive learning or inductive learning. The goal of transductive learning is to infer the correct labels for the given unlabeled data \n  \n    \n      \n        \n          x\n          \n            l\n            +\n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            l\n            +\n            u\n          \n        \n      \n    \n    {\\displaystyle x_{l+1},\\dots ,x_{l+u}}\n   only. The goal of inductive learning is to infer the correct mapping from \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   to \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  .\nIntuitively, the learning problem can be seen as an exam and labeled data as sample problems that the teacher solves for the class as an aid in solving another set of problems. In the transductive setting, these unsolved problems act as exam questions. In the inductive setting, they become practice problems of the sort that will make up the exam.\nIt is unnecessary (and, according to Vapnik's principle, imprudent) to perform transductive learning by way of inferring a classification rule over the entire input space; however, in practice, algorithms formally designed for transduction or induction are often used interchangeably.",
  },
  {
    id: 15,
    content:
      "A dialogue system, or conversational agent (CA), is a computer system intended to converse with a human. Dialogue systems employed one or more of text, speech, graphics, haptics, gestures, and other modes for communication on both the input and output channel.\nThe elements of a dialogue system are not defined, however they are different from chatbot.. The typical GUI wizard engages in a sort of dialog, but it includes very few of the common dialogue system components, and dialog state is trivial.\n\n",
  },
  {
    id: 16,
    content:
      "Activity recognition aims to recognize the actions and goals of one or more agents from a series of observations on the agents' actions and the environmental conditions. Since the 1980s, this research field has captured the attention of several computer science communities due to its strength in providing personalized support for many different applications and its connection to many different fields of study such as medicine, human-computer interaction, or sociology.\nDue to its many-faceted nature, different fields may refer to activity recognition as plan recognition, goal recognition, intent recognition, behavior recognition, location estimation and location-based services.",
  },
  {
    id: 17,
    content:
      "In computer science, the Cocke–Younger–Kasami algorithm (alternatively called CYK, or CKY) is a parsing algorithm for context-free grammars, named after its inventors, John Cocke, Daniel Younger and Tadao Kasami. It employs bottom-up parsing and dynamic programming. The standard version of CYK operates only on context-free grammars given in Chomsky normal form (CNF). However any context-free grammar may be transformed to a CNF grammar expressing the same language (Sipser 1997).",
  },
  {
    id: 18,
    content:
      'A* (pronounced "A-star") is a graph traversal and path search algorithm, which is often used in computer science due to its completeness, optimality, and optimal efficiency.[1] One major practical drawback is its {\\displaystyle O(b^{d})}O(b^d) space complexity, as it stores all generated nodes in memory. Thus, in practical travel-routing systems, it is generally outperformed by algorithms which can pre-process the graph to attain better performance,[2] as well as memory-bounded approaches; however, A* is still the best solution in many cases.It can be seen as an extension of Edsger Dijkstra 1959 algorithm. A* achieves better performance by using heuristics to guide its search.',
  },
  {
    id: 19,
    content:
      "Automated essay scoring (AES) is the use of specialized computer programs to assign grades to essays written in an educational setting. It is a form of educational assessment and an application of natural language processing. Its objective is to classify a large set of textual entities into a small number of discrete categories, corresponding to the possible grades, for example, the numbers 1 to 6. Therefore, it can be considered a problem of statistical classification.\nSeveral factors have contributed to a growing interest in AES. Among them are cost, accountability, standards, and technology. Rising education costs have led to pressure to hold the educational system accountable for results by imposing standards. The advance of information technology promises to measure educational achievement at reduced cost.\nThe use of AES for high-stakes testing in education has generated significant backlash, with opponents pointing to research that computers cannot yet grade writing accurately and arguing that their use for such purposes promotes teaching writing in reductive ways (i.e. teaching to the test).",
  },
  {
    id: 20,
    content:
      "In mathematics, matrix multiplication is a binary operation that produces a matrix from two matrices. For matrix multiplication, the number of columns in the first matrix must be equal to the number of rows in the second matrix. The result matrix, known as the matrix product, has the number of rows of the first and the number of columns of the second matrix.\nMatrix multiplication was first described by the French mathematician Jacques Philippe Marie Binet in 1812, to represent the composition of linear maps that are represented by matrices. Matrix multiplication is thus a basic tool of linear algebra, and as such has numerous applications in many areas of mathematics, as well as in applied mathematics, statistics, physics, economics, and engineering.\nComputing matrix products is a central operation in all computational applications of linear algebra.",
  },
  {
    id: 21,
    content:
      "Statistical machine translation (SMT) is a machine translation paradigm where translations are generated on the basis of statistical models whose parameters are derived from the analysis of bilingual text corpora.  The statistical approach contrasts with the rule-based approaches to machine translation as well as with example-based machine translation.The first ideas of statistical machine translation were introduced by Warren Weaver in 1949, including the ideas of applying Claude Shannon's information theory. Statistical machine translation was re-introduced in the late 1980s and early 1990s by researchers at IBM's Thomas J. Watson Research Center and has contributed to the significant resurgence in interest in machine translation in recent years. Before the introduction of neural machine translation, it was by far the most widely studied machine translation method.\n\n",
  },
  {
    id: 22,
    content:
      "A Neural Turing machine (NTMs) is a recurrent neural network model. The approach was published by Alex Graves et. al. in 2014. NTMs combine the fuzzy pattern matching capabilities of neural networks with the algorithmic power of programmable computers. An NTM has a neural network controller coupled to external memory resources, which it interacts with through attentional mechanisms. The memory interactions are differentiable end-to-end, making it possible to optimize them using gradient descent. An NTM with a long short-term memory (LSTM) network controller can infer simple algorithms such as copying, sorting, and associative recall from examples alone.The authors of the original NTM paper did not publish their source code. The first stable open-source implementation was published in 2018 at the 27th International Conference on Artificial Neural Networks, receiving a best-paper award.  Other open source implementations of NTMs exist but are not sufficiently stable for production use. The developers either report that the gradients of their implementation sometimes become NaN during training for unknown reasons and cause training to fail; report slow convergence; or do not report the speed of learning of their implementation.Differentiable neural computers are an outgrowth of Neural Turing machines, with attention mechanisms that control where the memory is active, and improve performance.",
  },
  {
    id: 23,
    content:
      "In mathematics, graph theory is the study of graphs, which are mathematical structures used to model pairwise relations between objects. A graph in this context is made up of vertices (also called nodes or points) which are connected by edges (also called links or lines). A distinction is made between undirected graphs, where edges link two vertices symmetrically, and directed graphs, where edges link two vertices asymmetrically; see Graph (discrete mathematics) for more detailed definitions and for other variations in the types of graph that are commonly considered. Graphs are one of the prime objects of study in discrete mathematics.\nRefer to the glossary of graph theory for basic definitions in graph theory.",
  },
  {
    id: 24,
    content:
      "Semantic parsing is the task of converting a natural language utterance to a logical form: a machine-understandable representation of its meaning. Semantic parsing can thus be understood as extracting the precise meaning of an utterance. Applications of semantic parsing include machine translation, question answering, ontology induction, automated reasoning, and code generation. The phrase was first used in the 1970s by Yorick Wilks as the basis for machine translation programs working with only semantic representations.",
  },
  {
    id: 25,
    content:
      "In computer science and information science, an ontology encompasses a representation, formal naming and definition of the categories, properties and relations between the concepts, data and entities that substantiate one, many or all domains of discourse. More simply, an ontology is a way of showing the properties of a subject area and how they are related, by defining a set of concepts and categories that represent the subject.\nEvery academic discipline or field creates ontologies to limit complexity and organize data into information and knowledge. New ontologies improve problem solving within that domain. Translating research papers within every field is a problem made easier when experts from different countries maintain a controlled vocabulary of jargon between each of their languages.Since Google started an initiative called Knowledge Graph in 2012, a substantial amount of research has used the phrase knowledge graph as a generalized term. Although there is no clear definition for the term knowledge graph, it is sometimes used as synonym for ontology. One common interpretation is that a knowledge graph represents a collection of interlinked descriptions of entities \u2013 real-world objects, events, situations or abstract concepts. Unlike ontologies, knowledge graphs, such as Google's Knowledge Graph, often contain large volumes of factual information with less formal semantics. In some contexts, the term knowledge graph is used to refer to any knowledge base that is represented as a graph.",
  },
  {
    id: 26,
    content:
      "Statistical machine translation (SMT) is a machine translation paradigm where translations are generated on the basis of statistical models whose parameters are derived from the analysis of bilingual text corpora.  The statistical approach contrasts with the rule-based approaches to machine translation as well as with example-based machine translation.The first ideas of statistical machine translation were introduced by Warren Weaver in 1949, including the ideas of applying Claude Shannon's information theory. Statistical machine translation was re-introduced in the late 1980s and early 1990s by researchers at IBM's Thomas J. Watson Research Center and has contributed to the significant resurgence in interest in machine translation in recent years. Before the introduction of neural machine translation, it was by far the most widely studied machine translation method.\n\n",
  },
  {
    id: 27,
    content:
      'A greedy algorithm is any algorithm that follows the problem-solving heuristic of making the locally optimal choice at each stage with the intent of finding a global optimum. In many problems, a greedy strategy does not usually produce an optimal solution, but nonetheless a greedy heuristic may yield locally optimal solutions that approximate a globally optimal solution in a reasonable amount of time.\nFor example, a greedy strategy for the travelling salesman problem (which is of a high computational complexity) is the following heuristic: "At each step of the journey, visit the nearest unvisited city." This heuristic does not intend to find a best solution, but it terminates in a reasonable number of steps; finding an optimal solution to such a complex problem typically requires unreasonably many steps. In mathematical optimization, greedy algorithms optimally solve combinatorial problems having the properties of matroids, and give constant-factor approximations to optimization problems with submodular structure.',
  },
  {
    id: 28,
    content:
      "In traditional grammar, a part of speech is a category of words (or, more generally, of lexical items) that have similar grammatical properties. Words that are assigned to the same part of speech generally display similar syntactic behavior\u2014they play similar roles within the grammatical structure of sentences\u2014and sometimes similar morphology in that they undergo inflection for similar properties.\nCommonly listed English parts of speech are noun, verb, adjective, adverb, pronoun, preposition, conjunction, interjection, numeral, article, or determiner. Other Indo-European languages also have essentially all these word classes; one exception to this generalization is that most Slavic languages as well as Latin and Sanskrit do not have articles. Beyond the Indo-European family, such other European languages as Hungarian and Finnish, both of which belong to the Uralic family, completely lack prepositions or have only very few of them; rather, they have postpositions.\nOther terms than part of speech\u2014particularly in modern linguistic classifications, which often make more precise distinctions than the traditional scheme does\u2014include word class, lexical class, and lexical category. Some authors restrict the term lexical category to refer only to a particular type of syntactic category; for them the term excludes those parts of speech that are considered to be functional, such as pronouns. The term form class is also used, although this has various conflicting definitions. Word classes may be classified as open or closed: open classes (like nouns, verbs and adjectives) acquire new members constantly, while closed classes (such as pronouns and conjunctions) acquire new members infrequently, if at all.\nAlmost all languages have the word classes noun and verb, but beyond these two there are significant variations among different languages. For example,\n\nJapanese has as many as three classes of adjectives, where English has one (not to be confused with the seven types of English adjectives, or the fact that English adjectives can modify both nouns and pronouns);Chinese, Korean, Japanese and Vietnamese have a class of nominal classifiers; andMany languages do not distinguish between adjectives and adverbs, or between adjectives and verbs (see stative verb).Because of such variation in the number of categories and their identifying properties, analysis of parts of speech must be done for each individual language. Nevertheless, the labels for each category are assigned on the basis of universal criteria.",
  },
  {
    id: 29,
    content:
      'A context-sensitive grammar (CSG) is a formal grammar in which the left-hand sides and right-hand sides of any production rules may be surrounded by a context of terminal and nonterminal symbols. Context-sensitive grammars are more general than context-free grammars, in the sense that there are languages that can be described by CSG but not by context-free grammars. Context-sensitive grammars are less general (in the same sense) than unrestricted grammars. Thus, CSG are positioned between context-free and unrestricted grammars in the Chomsky hierarchy.\nA formal language that can be described by a context-sensitive grammar, or, equivalently, by a noncontracting grammar or a linear bounded automaton, is called a context-sensitive language. Some textbooks actually define CSGs as non-contracting, although this is not how Noam Chomsky defined them in 1959. This choice of definition makes no difference in terms of the languages generated (i.e. the two definitions are weakly equivalent), but it does make a difference in terms of what grammars are structurally considered context-sensitive; the latter issue was analyzed by Chomsky in 1963.Chomsky introduced context-sensitive grammars as a way to describe the syntax of natural language where it is often the case that a word may or may not be appropriate in a certain place depending on the context. Walter Savitch has criticized the terminology "context-sensitive" as misleading and proposed "non-erasing" as better explaining the distinction between a CSG and an unrestricted grammar.Although it is well known that certain features of languages (e.g. cross-serial dependency) are not context-free, it is an open question how much of CSG\'s expressive power is needed to capture the context sensitivity found in natural languages. Subsequent research in this area has focused on the more computationally tractable mildly context-sensitive languages. The syntaxes of some visual programming languages can be described by context-sensitive graph grammars.',
  },
  {
    id: 30,
    content:
      "A recurrent neural network (RNN) is a class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior. Derived from feedforward neural networks, RNNs can use their internal state (memory) to process variable length sequences of inputs. This makes them applicable to tasks such as unsegmented, connected handwriting recognition or speech recognition.The term \u201crecurrent neural network\u201d is used indiscriminately to refer to two broad classes of networks with a similar general structure, where one is finite impulse and the other is infinite impulse. Both classes of networks exhibit temporal dynamic behavior. A finite impulse recurrent network is a directed acyclic graph that can be unrolled and replaced with a strictly feedforward neural network, while an infinite impulse recurrent network is a directed cyclic graph that can not be unrolled.\nBoth finite impulse and infinite impulse recurrent networks can have additional stored states, and the storage can be under direct control by the neural network. The storage can also be replaced by another network or graph, if that incorporates time delays or has feedback loops. Such controlled states are referred to as gated state or gated memory, and are part of long short-term memory networks (LSTMs) and gated recurrent units. This is also called Feedback Neural Network.",
  },
  {
    id: 31,
    content:
      "In computer science and operations research, a genetic algorithm (GA) is a metaheuristic inspired by   the process of natural selection that belongs to the larger class of evolutionary algorithms (EA). Genetic algorithms are commonly used to generate high-quality solutions to optimization and search problems by relying on biologically inspired operators such as mutation, crossover and selection. John Holland introduced genetic algorithms in 1960 based on the concept of Darwin\u2019s theory of evolution; his student David E. Goldberg further extended GA in 1989.",
  },
  {
    id: 32,
    content:
      "Transfer learning (TL) is a research problem in machine learning (ML) that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem. For example, knowledge gained while learning to recognize cars could apply when trying to recognize trucks.  This area of research bears some relation to the long history of psychological literature on transfer of learning, although formal ties between the two fields are limited. From the practical standpoint, reusing or transferring information from previously learned tasks for the learning of new tasks has the potential to significantly improve the sample efficiency of a reinforcement learning agent.Andrew Ng said in his NIPS 2016 tutorial  that TL will be next driver of ML commercial success after supervised learning to highlight the importance of TL.",
  },
  {
    id: 33,
    content:
      "Vector space model or term vector model is an algebraic model for representing text documents (and any objects, in general) as vectors of identifiers, such as, for example, index terms. It is used in information filtering, information retrieval, indexing and relevancy rankings.  Its first use was in the SMART Information Retrieval System.",
  },
  {
    id: 34,
    content:
      "Named-entity recognition (NER) (also known as entity identification, entity chunking and entity extraction) is a subtask of information extraction that seeks to locate and classify named entity mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc.\nMost research on NER systems has been structured as taking an unannotated block of text, such as this one:\n\nJim bought 300 shares of Acme Corp. in 2006.\nAnd producing an annotated block of text that highlights the names of entities:\n\n[Jim]Person bought 300 shares of [Acme Corp.]Organization in [2006]Time.\nIn this example, a person name consisting of one token, a two-token company name and a temporal expression have been detected and classified.\nState-of-the-art NER systems for English produce near-human performance. For example, the best system entering MUC-7 scored 93.39% of F-measure while human annotators scored 97.60% and 96.95%.",
  },
  {
    id: 35,
    content:
      "Lexical semantics (also known as lexicosemantics), is a subfield of linguistic semantics. The units of analysis in lexical semantics are lexical units which include not only words but also sub-words or sub-units such as affixes and even compound words and phrases. Lexical units include the catalogue of words in a language, the lexicon. Lexical semantics looks at how the meaning of the lexical units correlates with the structure of the language or syntax. This is referred to as syntax-semantic interface.The study of lexical semantics looks at:\n\nthe classification and decomposition of lexical items\nthe differences and similarities in lexical semantic structure cross-linguistically\nthe relationship of lexical meaning to sentence meaning and syntax.Lexical units, also referred to as syntactic atoms, can stand alone such as in the case of root words or parts of compound words or they necessarily attach to other units such as prefixes and suffixes do. The former are called free morphemes and the latter bound morphemes. They fall into a narrow range of meanings (semantic fields) and can combine with each other to generate new denotations.",
  },
  {
    id: 36,
    content:
      "Collaborative filtering (CF) is a technique used by recommender systems. Collaborative filtering has two senses, a narrow one and a more general one.In the newer, narrower sense, collaborative filtering is a method of making automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many users (collaborating). The underlying assumption of the collaborative filtering approach is that if a person A has the same opinion as a person B on an issue, A is more likely to have B's opinion on a different issue than that of a randomly chosen person. For example, a collaborative filtering recommendation system for television tastes could make predictions about which television show a user should like given a partial list of that user's tastes (likes or dislikes). Note that these predictions are specific to the user, but use information gleaned from many users. This differs from the simpler approach of giving an average (non-specific) score for each item of interest, for example based on its number of votes.\nIn the more general sense, collaborative filtering is the process of filtering for information or patterns using techniques involving collaboration among multiple agents, viewpoints, data sources, etc.  Applications of collaborative filtering typically involve very large data sets. Collaborative filtering methods have been applied to many different kinds of data including: sensing and monitoring data, such as in mineral exploration, environmental sensing over large areas or multiple sensors; financial data, such as financial service institutions that integrate many financial sources; or in electronic commerce and web applications  where the focus is on user data, etc. The remainder of this discussion focuses on collaborative filtering for user data, although some of the methods and approaches may apply to the other major applications as well.",
  },
  {
    id: 37,
    content:
      "A modeling language is any artificial language that can be used to express information or knowledge or systems in a structure that is defined by a consistent set of rules. The rules are used for interpretation of the meaning of components in the structure.",
  },
  {
    id: 38,
    content:
      "In statistics and machine learning, the bias\u2013variance tradeoff is the property of a set of predictive models whereby models with a lower bias in parameter estimation have a higher variance of the parameter estimates across samples, and vice versa. The bias\u2013variance dilemma or bias\u2013variance problem is the conflict in trying to simultaneously minimize these two sources of error that prevent supervised learning algorithms from generalizing beyond their training set:\n\nThe bias error is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).\nThe variance is an error from sensitivity to small fluctuations in the training set. High variance can cause an algorithm to model the random noise in the training data, rather than the intended outputs (overfitting).The bias\u2013variance decomposition is a way of analyzing a learning algorithm's expected generalization error with respect to a particular problem as a sum of three terms, the bias, variance, and a quantity called the irreducible error, resulting from noise in the problem itself.\nThis tradeoff applies to all forms of supervised learning: classification, regression (function fitting), and structured output learning, though, it does not apply in all learning algorithms. It has also been invoked to explain the effectiveness of heuristics in human learning.It is important to note that the bias-variance tradeoff is not universal. For example, both bias and variance decrease when increasing the width of a neural network. This means that it is not necessary to control the size of a neural network to control variance. This does not contradict the bias-variance decomposition because the bias-variance decomposition does not imply a bias-variance tradeoff.\n\n",
  },
  {
    id: 39,
    content:
      "A mathematical model is a description of a system using mathematical concepts and language. The process of developing a mathematical model is termed mathematical modeling. Mathematical models are used in the natural sciences (such as physics, biology, earth science, chemistry) and engineering disciplines (such as computer science, electrical engineering), as well as in the social sciences (such as economics, psychology, sociology, political science).\nA model may help to explain a system and to study the effects of different components, and to make predictions about behaviour.",
  },
  {
    id: 40,
    content:
      'A generative adversarial network (GAN) is a class of machine learning frameworks invented by Ian Goodfellow and his colleagues in 2014. Two neural networks contest with each other in a game (in the sense of game theory, often but not always in the form of a zero-sum game). Given a training set, this technique learns to generate new data with the same statistics as the training set. For example, a GAN trained on photographs can generate new photographs that look at least superficially authentic to human observers, having many realistic characteristics. Though originally proposed as a form of generative model for unsupervised learning, GANs have also proven useful for semi-supervised learning, fully supervised learning, and reinforcement learning. In a 2016 seminar, Yann LeCun described GANs as "the coolest idea in machine learning in the last twenty years".',
  },
  {
    id: 41,
    content:
      'The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity. The bag-of-words model has also been used for computer vision.The bag-of-words model is commonly used in methods of document classification where the (frequency of) occurrence of each word is used as a feature for training a classifier.An early reference to "bag of words" in a linguistic context can be found in Zellig Harris\'s 1954 article on Distributional Structure.',
  },
  {
    id: 42,
    content:
      "Gradient descent is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. To find a local minimum of a function using gradient descent, we take steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point. But if we instead take steps proportional to the positive of the gradient, we approach a local maximum of that function; the procedure is then known as gradient ascent. Gradient descent was originally proposed by Cauchy in 1847.Gradient descent is also known as steepest descent;  but gradient descent should not be confused with the method of steepest descent for approximating integrals.",
  },
  {
    id: 43,
    content:
      "In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, most commonly applied to analyzing visual imagery. They are also known as shift invariant or space invariant artificial neural networks (SIANN), based on their shared-weights architecture and translation invariance characteristics. They have applications in image and video recognition, recommender systems, image classification, medical image analysis, natural language processing, and financial time series.",
  },
  {
    id: 44,
    content:
      "Morphological disambiguation is one of the main tasks of automatic natural language processing. Its results can be used to improve accuracy and quality of the methods used in such tasks as text classification and clustering, machine translation, and information retrieval. ",
  },
  {
    id: 45,
    content:
      "In statistical mechanics, entropy is an extensive property of a thermodynamic system. It is closely related to the number \u03a9 of microscopic configurations (known as microstates) that are consistent with the macroscopic quantities that characterize the system (such as its volume, pressure and temperature). Entropy expresses the number \u03a9 of different configurations that a system defined by macroscopic variables could assume. Under the assumption that each microstate is equally probable, the entropy \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n   is the natural logarithm of the number of microstates, multiplied by the Boltzmann constant kB. Formally (assuming equiprobable microstates),\n\n  \n    \n      \n        S\n        =\n        \n          k\n          \n            \n              B\n            \n          \n        \n        ln\n        \u2061\n        \u03a9\n        .\n      \n    \n    {\\displaystyle S=k_{\\mathrm {B} }\\ln \\Omega .}\n  Macroscopic systems typically have a very large number \u03a9 of possible microscopic configurations.  For example, the entropy of an ideal gas is proportional to the number of gas molecules N.  The number of molecules in 22.4 liters of gas at standard temperature and pressure is roughly 6.022 \u00d7 1023 (the Avogadro number).\nThe second law of thermodynamics states that the entropy of an isolated system never decreases over time. Isolated systems spontaneously evolve towards thermodynamic equilibrium, the state with maximum entropy. Non-isolated systems, like organisms, may lose entropy, provided their environment's entropy increases by at least that amount so that the total entropy either increases or remains constant. Therefore, the entropy in a specific system can decrease as long as the total entropy of the Universe does not. Entropy is a function of the state of the system, so the change in entropy of a system is determined by its initial and final states. In the idealization that a process is reversible, the entropy does not change, while irreversible processes always increase the total entropy.\nBecause it is determined by the number of random microstates, entropy is related to the amount of additional information needed to specify the exact physical state of a system, given its macroscopic specification. For this reason, it is often said that entropy is an expression of the disorder, or randomness of a system, or of the lack of information about it. The concept of entropy plays a central role in information theory.",
  },
  {
    id: 46,
    content:
      "Question answering (QA) is a computer science discipline within the fields of information retrieval and natural language processing (NLP), which is concerned with building systems that automatically answer questions posed by humans in a natural language.",
  },
  {
    id: 47,
    content:
      "The Round Table class, also known as the Sir Lancelot class, was a British ship class designed for amphibious warfare missions in support of the main amphibious warfare ships. They were designated landing ship logistics (LSL).\nAll ships were named after Knights of the Round Table.",
  },
  {
    id: 48,
    content:
      "Temporal information retrieval (T-IR) is an emerging area of research related to the field of information retrieval (IR) and a considerable number of sub-areas, positioning itself, as an important dimension in the context of the user information needs.\nAccording to information theory science (Metzger, 2007), timeliness or currency is one of the key five aspects that determine a document's credibility besides relevance, accuracy, objectivity and coverage. One can provide many examples when the returned search results are of little value due to temporal problems such as obsolete data on weather, outdated information about a given company\u2019s earnings or information on already-happened or invalid predictions.\nT-IR, in general, aims at satisfying these temporal needs and at combining traditional notions of document relevance with the so-called temporal relevance. This will enable the return of temporally relevant documents, thus providing a temporal overview of the results in the form of timeliness or similar structures. It also shows to be very useful for query understanding, query disambiguation, query classification, result diversification and so on.\nThis page contains a list of the most important research in temporal information retrieval (T-IR) and its related sub-areas. As several of the referred works are related with different research areas a single article can be found in more than one different table. For ease of reading the articles are categorized in a number of different sub-areas referring to its main scope, in detail.",
  },
  {
    id: 49,
    content:
      "Articulatory phonology is a linguistic theory originally proposed in 1986 by Catherine Browman of Haskins Laboratories and Louis M. Goldstein of Yale University and Haskins. The theory identifies theoretical discrepancies between phonetics and phonology and aims to unify the two by treating them as low- and high-dimensional descriptions of a single system.\nUnification can be achieved by incorporating into a single model the idea that the physical system (identified with phonetics) constrains the underlying abstract system (identified with phonology), making the units of control at the abstract planning level the same as those at the physical level.\nThe plan of an utterance is formatted as a gestural score, which provides the input to a physically based model of speech production - the task dynamic model of Elliot Saltzman. The gestural score graphs locations within the vocal tract where constriction can occur, indicating the planned or target degree of constriction. A computational model of speech production developed at Haskins Laboratories combines articulatory phonology, task dynamics, and the Haskins articulatory synthesis system developed by Philip Rubin and colleagues.",
  },
  {
    id: 50,
    content:
      "Parsing, syntax analysis, or syntactic analysis is the process of analyzing a string of symbols, either in natural language, computer languages or data structures, conforming to the rules of a formal grammar. The term parsing comes from Latin pars (orationis), meaning part (of speech).",
  },
  {
    id: 51,
    content:
      "Deep learning  (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised.Deep learning architectures such as deep neural networks, deep belief networks, recurrent neural networks and convolutional neural networks have been applied to fields including computer vision, speech recognition, natural language processing, audio recognition, social network filtering, machine translation, bioinformatics, drug design, medical image analysis, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.Artificial neural networks (ANNs) were inspired by information processing and distributed communication nodes in biological systems. ANNs have various differences from biological brains.  Specifically, neural networks tend to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) and analog.\n\n",
  },
  {
    id: 52,
    content:
      'Neuro-linguistic programming (NLP) is a pseudoscientific approach to communication, personal development, and psychotherapy created by Richard Bandler and John Grinder in California, United States, in the 1970s. NLP\'s creators claim there is a connection between neurological processes (neuro-), language (linguistic) and behavioral patterns learned through experience (programming), and that these can be changed to achieve specific goals in life. Bandler and Grinder also claim that NLP methodology can "model" the skills of exceptional people, allowing anyone to acquire those skills. They claim as well that, often in a single session, NLP can treat problems such as phobias, depression, tic disorders, psychosomatic illnesses, near-sightedness, allergy, common cold, and learning disorders. NLP has been adopted by some hypnotherapists and also by companies that run seminars marketed as leadership training to businesses and government agencies.There is no scientific evidence supporting the claims made by NLP advocates and it has been discredited as a pseudoscience. Scientific reviews state that NLP is based on outdated metaphors of how the brain works that are inconsistent with current neurological theory and contain numerous factual errors. Reviews also found that all of the supportive research on NLP contained significant methodological flaws and that there were three times as many studies of a much higher quality that failed to reproduce the "extraordinary claims" made by Bandler, Grinder, and other NLP practitioners.',
  },
  {
    id: 53,
    content:
      "Speech processing is the study of speech signals and the processing methods of  signals. The signals are usually processed in a digital representation, so speech processing can be regarded as a special case of digital signal processing, applied to speech signals. Aspects of speech processing includes the acquisition, manipulation, storage, transfer and output of speech signals. The input is called speech recognition and the output is called speech synthesis.",
  },
  {
    id: 54,
    content:
      "Discourse analysis (DA), or discourse studies, is an approach to the analysis of written, vocal, or sign language use, or any significant semiotic event.\nThe objects of discourse analysis (discourse, writing, conversation, communicative event) are variously defined in terms of coherent sequences of sentences, propositions, speech, or turns-at-talk. Contrary to much of traditional linguistics, discourse analysts not only study language use 'beyond the sentence boundary' but also prefer to analyze 'naturally occurring' language use, not invented examples. Text linguistics is a closely related field. The essential difference between discourse analysis and text linguistics is that discourse analysis aims at revealing socio-psychological characteristics of a person/persons rather than text structure.Discourse analysis has been taken up in a variety of disciplines in the humanities and social sciences, including linguistics, education, sociology, anthropology, social work, cognitive psychology, social psychology, area studies, cultural studies, international relations, human geography,  environmental science, communication studies, biblical studies, public relations and translation studies, each of which is subject to its own assumptions, dimensions of analysis, and methodologies.",
  },
  {
    id: 55,
    content:
      'Q-learning is a model-free reinforcement learning algorithm to learn a policy telling an agent what action to take under what circumstances. It does not require a model (hence the connotation "model-free") of the environment, and it can handle problems with stochastic transitions and rewards, without requiring adaptations.\nFor any finite Markov decision process (FMDP), Q-learning finds an optimal policy in the sense maximizing the expected value of the total reward over any and all successive steps, starting from the current state. Q-learning can identify an optimal action-selection policy for any given FMDP, given infinite exploration time and a partly-random policy. "Q" names the function that returns the reward used to provide the reinforcement and can be said to stand for the "quality" of an action taken in a given state.',
  },
  {
    id: 56,
    content:
      "Dependency parsing is the task of extracting a dependency parse of a sentence that represents its grammatical structure and defines the relationships between “head” words and words, which modify those heads.",
  },
  {
    id: 57,
    content:
      "In linguistics, prosody is concerned with those elements of speech that are not individual phonetic segments (vowels and consonants) but are properties of syllables and larger units of speech, including linguistic functions such as intonation, tone, stress, and rhythm. Such elements are known as suprasegmentals.",
  },
  {
    id: 58,
    content:
      "Bootstrap aggregating, also called bagging (from bootstrap aggregating), is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method. Bagging is a special case of the model averaging approach.",
  },
  {
    id: 59,
    content:
      "Domain adaptation is a field associated with machine learning and transfer learning. This scenario arises when we aim at learning from a source data distribution a well performing model on a different (but related) target data distribution. For instance, one of the tasks of the common spam filtering problem consists in adapting a model from one user (the source distribution) to a new one who receives significantly different emails (the target distribution). Domain adaptation has also been shown to be beneficial for learning unrelated sources.\nNote that, when more than one source distribution is available the problem is referred to as multi-source domain adaptation.",
  },
  {
    id: 60,
    content:
      'Transliteration is a type of conversion of a text from one script to another that involves swapping letters (thus trans- + liter-) in predictable ways, such as Greek \u27e8\u03b1\u27e9 \u2192 \u27e8a\u27e9, Cyrillic \u27e8\u0434\u27e9 \u2192 \u27e8d\u27e9, Greek \u27e8\u03c7\u27e9 \u2192 the digraph \u27e8ch\u27e9, Armenian \u27e8\u0576\u27e9 \u2192 \u27e8n\u27e9 or Latin \u27e8\u00e6\u27e9 \u2192 \u27e8ae\u27e9.\nFor instance, for the Modern Greek term "\u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ae \u0394\u03b7\u03bc\u03bf\u03ba\u03c1\u03b1\u03c4\u03af\u03b1", which is usually translated as "Hellenic Republic", the usual transliteration to Latin script is \u27e8Ell\u0113nik\u1e17 D\u0113mokrat\u00eda\u27e9, and the name for Russia in Cyrillic script, "\u0420\u043e\u0441\u0441\u0438\u044f", is usually transliterated as \u27e8Rossija\u27e9.\nTransliteration is not primarily concerned with representing the sounds of the original but rather with representing the characters, ideally accurately and unambiguously. Thus, in the Greek above example, \u27e8\u03bb\u03bb\u27e9 is transliterated \u27e8ll\u27e9 though it is pronounced [l], \u27e8\u0394\u27e9 is transliterated \u27e8D\u27e9 though pronounced [\u00f0], and \u27e8\u03b7\u27e9 is transliterated \u27e8\u0113\u27e9, though it is pronounced [i] (exactly like \u27e8\u03b9\u27e9) and is not long.\nConversely, transcription notes the sounds rather than the orthography of a text. So "\u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ae \u0394\u03b7\u03bc\u03bf\u03ba\u03c1\u03b1\u03c4\u03af\u03b1" could be transcribed as [elinik\u00ed \u00f0imokrat\u00eda], which does not specify which of the [i] sounds are written with the Greek letter \u27e8\u03b7\u27e9 and which with \u27e8\u03b9\u27e9. \nAngle brackets may be used to set off transliteration, as opposed to slashes and square brackets for phonetic transcription. Angle brackets may also be used to set of characters in the original script. Conventions and author preferences vary.',
  },
  {
    id: 61,
    content:
      'Ranking of query is one of the fundamental problems in information retrieval  (IR), the scientific/engineering discipline behind search engines. Given a query q and a collection D of documents that match the query, the problem is to rank, that is, sort, the documents in D according to some criterion so that the "best" results appear early in the result list displayed to the user. Ranking in terms of information retrieval is an important concept in computer science and is used in many different applications such as search engine queries and recommender systems. A majority of search engines use ranking algorithms to provide users with accurate and relevant results.',
  },
  {
    id: 62,
    content:
      "Bayesian Nonparametrics is a class of models with a potentially infinite number of parameters. High flexibility and expressive power of this approach enables better data modelling compared to parametric methods.\nBayesian Nonparametrics is used in problems where a dimension of interest grows with data, for example, in problems where the number of features is not fixed but allowed to vary as we observe more data. Another example is clustering where the number of clusters is automatically inferred from data.",
  },
  {
    id: 63,
    content:
      "A programming language is a formal language, which comprises a set of instructions that produce various kinds of output. Programming languages are used in computer programming to implement algorithms.\nMost programming languages consist of instructions for computers. There are programmable machines that use a set of specific instructions, rather than general programming languages. Early ones preceded the invention of the digital computer, the first probably being the automatic flute player described in the 9th century by the brothers Musa in Baghdad, during the Islamic Golden Age. Since the early 1800s, programs have been used to direct the behavior of machines such as Jacquard looms, music boxes and player pianos. The programs for these machines (such as a player piano's scrolls) did not produce different behavior in response to different inputs or conditions.\nThousands of different programming languages have been created, and more are being created every year. Many programming languages are written in an imperative form (i.e., as a sequence of operations to perform) while other languages use the declarative form (i.e. the desired result is specified, not how to achieve it).\nThe description of a programming language is usually split into the two components of syntax (form) and semantics (meaning). Some languages are defined by a specification document (for example, the C programming language is specified by an ISO Standard) while other languages (such as Perl) have a dominant implementation that is treated as a reference. Some languages have both, with the basic language defined by a standard and extensions taken from the dominant implementation being common.",
  },
  {
    id: 64,
    content:
      "A facial recognition system is a technology capable of identifying or verifying a person from a digital image or a video frame from a video source. There are multiple methods in which facial recognition systems work, but in general, they work by comparing selected facial features from given image with faces within a database. It is also described as a Biometric Artificial Intelligence based application that can uniquely identify a person by analyzing patterns based on the person's facial textures and shape.While initially a form of computer application, it has seen wider uses in recent times on mobile platforms and in other forms of technology, such as robotics. It is typically used as access control in security systems and can be compared to other biometrics such as fingerprint or eye iris recognition systems. Although the accuracy of facial recognition system as a biometric technology is lower than iris recognition and fingerprint recognition, it is widely adopted due to its contactless and non-invasive process. Recently, it has also become popular as a commercial identification and marketing tool. Other applications include advanced human-computer interaction, video surveillance, automatic indexing of images, and video database, among others.",
  },
  {
    id: 65,
    content:
      "Question answering (QA) is a computer science discipline within the fields of information retrieval and natural language processing (NLP), which is concerned with building systems that automatically answer questions posed by humans in a natural language.",
  },
  {
    id: 66,
    content:
      'Particle filters or Sequential Monte Carlo (SMC) methods are a set of Monte Carlo algorithms used to solve filtering problems arising in signal processing and Bayesian statistical inference. The filtering problem consists of estimating the internal states in dynamical systems when partial observations are made, and random perturbations are present in the sensors as well as in the dynamical system. The objective is to compute the posterior distributions of the states of some Markov process, given some noisy and partial observations.  The term "particle filters" was first coined in 1996 by Del Moral in reference to mean field interacting particle methods used in fluid mechanics since the beginning of the 1960s. The term "Sequential Monte Carlo" was coined by Liu and Chen in 1998.Particle filtering uses a set of particles (also called samples) to represent the posterior distribution of some stochastic process given noisy and/or partial observations. The state-space model can be nonlinear and the initial state and noise distributions can take any form required. Particle filter techniques provide a well-established methodology for generating samples from the required distribution without requiring assumptions about the state-space model or the state distributions. However, these methods do not perform well when applied to very high-dimensional systems.\nParticle filters update their prediction in an approximate (statistical) manner. The samples from the distribution are represented by a set of particles; each particle has a likelihood weight assigned to it that represents the probability of that particle being sampled from the probability density function. Weight disparity leading to weight collapse is a common issue encountered in these filtering algorithms; however it can be mitigated by including a resampling step before the weights become too uneven. Several adaptive resampling criteria can be used, including the variance of the weights and the relative entropy with respect to the uniform distribution. In the resampling step, the particles with negligible weights are replaced by new particles in the proximity of the particles with higher weights.\nFrom the statistical and probabilistic point of view, particle filters can be interpreted as mean field particle interpretations of Feynman-Kac probability measures. These particle integration techniques were developed in molecular chemistry and computational physics by Theodore E. Harris and Herman Kahn in 1951, Marshall N. Rosenbluth and Arianna W. Rosenbluth in 1955  and more recently by Jack H. Hetherington in 1984. In computational physics, these Feynman-Kac type path particle integration methods are also used in Quantum Monte Carlo, and more specifically Diffusion Monte Carlo methods. Feynman-Kac interacting particle methods are also strongly related to mutation-selection genetic algorithms currently used in evolutionary computing to solve complex optimization problems.\nThe particle filter methodology is used to solve Hidden Markov Model (HMM) and nonlinear filtering problems.  With the notable exception of linear-Gaussian signal-observation models (Kalman filter) or wider classes of models (Benes filter) Mireille Chaleyat-Maurel and Dominique Michel proved in 1984 that the sequence of posterior distributions of the random states of the signal given the observations (a.k.a. optimal filter)  have no finitely recursive recursion. Various other numerical methods based on fixed grid approximations, Markov Chain Monte Carlo techniques (MCMC), conventional linearization, extended Kalman filters, or determining the best linear system (in the expected cost-error sense) are unable to cope with large scale systems, unstable processes, or when the nonlinearities are not sufficiently smooth.\nParticle filters and Feynman-Kac particle methodologies find application in signal and image processing, Bayesian inference, machine learning, risk analysis and rare event sampling, engineering and robotics, artificial intelligence, bioinformatics, phylogenetics, computational science, Economics and mathematical finance, molecular chemistry, computational physics, pharmacokinetic and other fields.',
  },
  {
    id: 67,
    content:
      'Combinatory categorial grammar (CCG) is an efficiently parsable, yet linguistically expressive grammar formalism. It has a transparent interface between surface syntax and underlying semantic representation, including predicate-argument structure, quantification and information structure. The formalism generates constituency-based structures (as opposed to dependency-based ones) and is therefore a type of phrase structure grammar (as opposed to a dependency grammar).\nCCG relies on combinatory logic, which has the same expressive power as the lambda calculus, but builds its expressions differently. The first linguistic and psycholinguistic arguments for basing the grammar on combinators were put forth by Steedman and Szabolcsi. More recent prominent proponents of the approach are Pauline Jacobson and Jason Baldridge.\nFor example, the combinator B (the compositor) is useful in creating long-distance dependencies, as in "Who do you think Mary is talking about?" and the combinator W (the duplicator) is useful as the lexical interpretation of reflexive pronouns, as in "Mary talks about herself". Together with I (the identity mapping) and C (the permutator) these form a set of primitive, non-interdefinable combinators. Jacobson interprets personal pronouns as the combinator I, and their binding is aided by a complex combinator Z, as in "Mary lost her way". Z is definable using W and B.',
  },
  {
    id: 68,
    content:
      "A common feature in grammars is a sequence of a particular syntactic element. In EBNF, we'd write something like n+ to represent a sequence of one or more ns, and n* for zero or more. Happy doesn't support this syntax explicitly, but you can define the equivalent sequences using simple productions.",
  },
  {
    id: 69,
    content:
      "Information retrieval (IR) is the activity of obtaining information system resources that are relevant to an information need from a collection of those resources.  Searches can be based on full-text or other content-based indexing. Information retrieval is the science of searching for information in a document, searching for documents themselves, and also searching for the metadata that describes data, and for databases of texts, images or sounds.\nAutomated information retrieval systems are used to reduce what has been called information overload. An IR system is a software system that provides access to books, journals and other documents; stores and manages those documents. Web search engines are the most visible IR applications.\n\n",
  },
  {
    id: 70,
    content:
      "Big Mechanism is a $45 million DARPA research program, begun in 2014, aimed at developing software that will read cancer research papers, integrate them into a cancer model and frame new hypotheses by the end of 2017 through the automated collection of big data and integrating across various disciplines such as knowledge-based NLP, curation and ontology, systems and mathematical biology by reading research abstracts and papers to extract pieces of causal mechanisms.",
  },
  {
    id: 71,
    content:
      "Structured sparsity regularization is a class of methods, and an area of research in statistical learning theory, that extend and generalize sparsity regularization learning methods. Both sparsity and structured sparsity regularization methods seek to exploit the assumption that the output variable \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   (i.e., response, or dependent variable) to be learned can be described by a reduced number of variables in the input space \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   (i.e., the domain, space of features or explanatory variables). Sparsity regularization methods focus on selecting the input variables that best describe the output. Structured sparsity regularization methods generalize and extend sparsity regularization methods, by allowing for optimal selection over structures like groups or networks of input variables in \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  .Common motivation for the use of structured sparsity methods are model interpretability, high-dimensional learning (where dimensionality of \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   may be higher than the number of observations \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  ), and reduction of computational complexity. Moreover, structured sparsity methods allow to incorporate prior assumptions on the structure of the input variables, such as overlapping groups, non-overlapping groups, and acyclic graphs. Examples of uses of structured sparsity methods include face recognition, magnetic resonance image (MRI) processing, socio-linguistic analysis in natural language processing, and analysis of genetic expression in breast cancer.",
  },
  {
    id: 72,
    content:
      "In computational linguistics, word-sense disambiguation (WSD) is an open problem concerned with identifying which sense of a word is used in a sentence. The solution to this problem impacts other computer-related writing, such as discourse, improving relevance of search engines, anaphora resolution, coherence, and inference.\nThe human brain is quite proficient at word-sense disambiguation. That natural language is formed in a way that requires so much of it is a reflection of that neurologic reality. In other words, human language developed in a way that reflects (and also has helped to shape) the innate ability provided by the brain's neural networks. In computer science and the information technology that it enables, it has been a long-term challenge to develop the ability in computers to do natural language processing and machine learning.\nA rich variety of techniques have been researched, from dictionary-based methods that use the knowledge encoded in lexical resources, to supervised machine learning methods in which a classifier is trained for each distinct word on a corpus of manually sense-annotated examples, to completely unsupervised methods that cluster occurrences of words, thereby inducing word senses. Among these, supervised learning approaches have been the most successful algorithms to date.\nAccuracy of current algorithms is difficult to state without a host of caveats. In English, accuracy at the coarse-grained (homograph) level is routinely above 90%, with some methods on particular homographs achieving over 96%. On finer-grained sense distinctions, top accuracies from 59.1% to 69.0% have been reported in evaluation exercises (SemEval-2007, Senseval-2), where the baseline accuracy of the simplest possible algorithm of always choosing the most frequent sense was 51.4% and 57%, respectively.",
  },
  {
    id: 73,
    content:
      "One-shot learning is an object categorization problem, found mostly in computer vision.  Whereas most machine learning based object categorization algorithms require training on hundreds or thousands of samples/images and very large datasets, one-shot learning aims to learn information about object categories from one, or only a few, training samples/images.\nThe primary focus of this article will be on the solution to this problem presented by Fei-Fei Li, R. Fergus and P. Perona in IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol 28(4), 2006, which uses a generative object category model and variational Bayesian framework for representation and learning of visual object categories from a handful of training examples.  Another paper, presented at the International Conference on Computer Vision and Pattern Recognition (CVPR) 2000 by Erik Miller, Nicholas Matsakis, and Paul Viola will also be discussed.\n\n",
  },
  { id: 74, content: "" },
  {
    id: 75,
    content:
      "Advanced Placement Calculus (also known as AP Calculus, AP Calc, or simply AB / BC) is a set of two distinct Advanced Placement calculus courses and exams offered by the American not-for-profit organization College Board. AP Calculus AB covers basic introductions to limits, derivatives, and integrals. AP Calculus BC covers all AP Calculus AB topics plus additional topics (including more advanced integration techniques such as integration by parts, Taylor series, parametric equations, vector calculus , polar coordinate functions, and curve interpolations).",
  },
  {
    id: 76,
    content:
      'A paraphrase  is a restatement of the meaning of a text or passage using other words. The term itself is derived via Latin paraphrasis from Greek \u03c0\u03b1\u03c1\u03ac\u03c6\u03c1\u03b1\u03c3\u03b9\u03c2, meaning "additional manner of expression".  The act of paraphrasing is also called "paraphrasis".',
  },
  {
    id: 77,
    content:
      "Artificial Intelligence is the study of building agents that act rationally. Most of the time, these agents perform some kind of search algorithm in the background in order to achieve their tasks.",
  },
  {
    id: 78,
    content:
      "Multimodal learning suggests that when a number of our senses - visual, auditory, kinaesthetic - are being engaged during learning, we understand and remember more. By combining these modes, learners experience learning in a variety of ways to create a diverse learning style. ",
  },
  {
    id: 79,
    content:
      "Uncertainty refers to epistemic situations involving imperfect or unknown information. It applies to predictions of future events, to physical measurements that are already made, or to the unknown. Uncertainty arises in partially observable and/or stochastic environments, as well as due to ignorance, indolence, or both. It arises in any number of fields, including insurance, philosophy, physics, statistics, economics, finance, psychology, sociology, engineering, metrology, meteorology, ecology and information science.",
  },
  {
    id: 80,
    content:
      'Artificial neural networks (ANN) or connectionist systems are computing systems vaguely inspired by the biological neural networks that constitute animal brains. Such systems "learn" to perform tasks by considering examples, generally without being programmed with task-specific rules. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as "cat" or "no cat" and using the results to identify cats in other images. They do this without any prior knowledge of cats, for example, that they have fur, tails, whiskers and cat-like faces. Instead, they automatically generate identifying characteristics from the examples that they process.\nAn ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. An artificial neuron that receives a signal then processes it and can signal neurons connected to it.\nIn ANN implementations, the "signal" at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs. The connections are called edges. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Neurons may have a threshold such that a signal is sent only if the aggregate signal crosses that threshold. Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times.\nThe original goal of the ANN approach was to solve problems in the same way that a human brain would. But over time, attention moved to performing specific tasks, leading to deviations from biology. ANNs have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games, medical diagnosis, and even in activities that have traditionally been considered as reserved to humans, like painting.',
  },
  {
    id: 81,
    content:
      'A chatbot is a software application used to conduct an on-line chat conversation via text or text-to-speech, in lieu of providing direct contact with a live human agent.  Designed to convincingly simulate the way a human would behave as a conversational partner, chatbot systems typically require continuous tuning and testing, and many in production remain unable to adequately converse or pass the industry standard Turing test.  The term "ChatterBot" was originally coined by Michael Mauldin (creator of the first Verbot) in 1994 to describe these conversational programs.Chatbots are typically used in dialog systems for various purposes including customer service, request routing, or for information gathering.  While some chatbot applications use extensive word-classification processes, Natural Language processors, and sophisticated AI, others simply scan for general keywords and generate responses using common phrases obtained from an associated library or database.  \nToday, most chatbots are accessed on-line via website popups, or through virtual assistants such as Google Assistant, Amazon Alexa, or messaging apps such as Facebook Messenger or WeChat. Chatbots are typically classified into usage categories that include: commerce (e-commerce via chat), education, entertainment, finance, health, news, and productivity.',
  },
  {
    id: 82,
    content:
      'Information theory studies the quantification, storage, and communication of information.  It was originally proposed by Claude Shannon in 1948 to find fundamental limits on signal processing and communication operations such as data compression, in a landmark paper titled "A Mathematical Theory of Communication". Its impact has been crucial to the success of the Voyager missions to deep space, the invention of the compact disc, the feasibility of mobile phones, the development of the Internet, the study of linguistics and of human perception, the understanding of black holes, and numerous other fields.\nThe field is at the intersection of mathematics, statistics, computer science, physics, neurobiology, information engineering, and electrical engineering. The theory has also found applications in other areas, including statistical inference, natural language processing, cryptography, neurobiology, human vision, the evolution and function of molecular codes (bioinformatics), model selection in statistics, thermal physics, quantum computing, linguistics, plagiarism detection, pattern recognition, and anomaly detection. Important sub-fields of information theory include source coding, algorithmic complexity theory, algorithmic information theory, information-theoretic security, Grey system theory and measures of information.\nApplications of fundamental topics of information theory include lossless data compression (e.g. ZIP files), lossy data compression (e.g. MP3s and JPEGs), and channel coding (e.g. for DSL). Information theory is used in information retrieval, intelligence gathering, gambling, and even in musical composition.\nA key measure in information theory is entropy. Entropy quantifies the amount of uncertainty involved in the value of a random variable or the outcome of a random process. For example, identifying the outcome of a fair coin flip (with two equally likely outcomes) provides less information (lower entropy) than specifying the outcome from a roll of a die (with six equally likely outcomes). Some other important measures in information theory are mutual information, channel capacity, error exponents, and relative entropy.',
  },
  {
    id: 83,
    content:
      'Multi-task learning (MTL) is a subfield of machine learning in which multiple learning tasks are solved at the same time, while exploiting commonalities and differences across tasks. This can result in improved learning efficiency and prediction accuracy for the task-specific models, when compared to training the models separately. Early versions of MTL were called "hints".\n\nIn a widely cited 1997 paper, Rich Caruana gave the following characterization:Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better.\nIn the classification context, MTL aims to improve the performance of multiple classification tasks by learning them jointly. One example is a spam-filter, which can be treated as distinct but related classification tasks across different users. To make this more concrete, consider that different people have different distributions of features which distinguish spam emails from legitimate ones, for example an English speaker may find that all emails in Russian are spam, not so for Russian speakers. Yet there is a definite commonality in this classification task across users, for example one common feature might be text related to money transfer. Solving each user\'s spam classification problem jointly via MTL can let the solutions inform each other and improve performance. Further examples of settings for MTL include multiclass classification and multi-label classification.Multi-task learning works because regularization induced by requiring an algorithm to perform well on a related task can be superior to regularization that prevents overfitting by penalizing all complexity uniformly. One situation where MTL may be particularly helpful is if the tasks share significant commonalities and are generally slightly under sampled. However, as discussed below, MTL has also been shown to be beneficial for learning unrelated tasks.',
  },
  { id: 84, content: "" },
  {
    id: 85,
    content:
      "Automatic summarization is the process of shortening a set of data computationally, to create a subset (a summary) that represents the most important or relevant information within the original content. \nIn addition to text, images and videos can also be summarized. Text summarization finds the most informative sentences in a document; image summarization finds the most representative images within an image collection; video summarization extracts the most important frames from the video content.\n\n",
  },
  {
    id: 86,
    content:
      'Inferences are steps in reasoning, moving from premises to logical consequences; etymologically, the word infer means to "carry forward". Inference is theoretically traditionally divided into deduction and induction, a distinction that in Europe dates at least to Aristotle (300s BCE). Deduction is inference deriving logical conclusions from premises known or assumed to be true,  with the laws of valid inference being studied in logic.  Induction is inference from particular premises to a universal conclusion. A third type of inference is sometimes distinguished, notably by Charles Sanders Peirce, distinguishing abduction from induction.\nVarious fields study how inference is done in practice. Human inference (i.e. how humans draw conclusions) is traditionally studied within the field of cognitive psychology; artificial intelligence researchers develop automated inference systems to emulate human inference. Statistical inference uses mathematics to draw conclusions in the presence of uncertainty. This generalizes deterministic reasoning, with the absence of uncertainty as a special case. Statistical inference uses quantitative or qualitative (categorical) data which may be subject to random variations.',
  },
  {
    id: 87,
    content:
      "Long short-term memory (LSTM) is an artificial recurrent neural network (RNN) architecture used in the field of deep learning. Unlike standard feedforward neural networks, LSTM has feedback connections. It can not only process single data points (such as images), but also entire sequences of data (such as speech or video). For example, LSTM is applicable to tasks such as unsegmented, connected handwriting recognition, speech recognition[3][4] and anomaly detection in network traffic or IDS's (intrusion detection systems).",
  },
  {
    id: 88,
    content:
      "Robotics is an interdisciplinary research area at the interface of computer science and engineering. Robotics involves design, construction, operation, and use of robots. The goal of robotics is to design intelligent machines that can help and assist humans in their day-to-day lives and keep everyone safe. Robotics draws on the achievement of information engineering, computer engineering, mechanical engineering, electronic engineering and others.\nRobotics develops machines that can substitute for humans and replicate human actions. Robots can be used in many situations and for lots of purposes, but today many are used in dangerous environments (including inspection of radioactive materials, bomb detection and deactivation), manufacturing processes, or where humans cannot survive (e.g. in space, underwater, in high heat, and clean up and containment of hazardous materials and radiation). Robots can take on any form but some are made to resemble humans in appearance. This is said to help in the acceptance of a robot in certain replicative behaviors usually performed by people. Such robots attempt to replicate walking, lifting, speech, cognition, or any other human activity. Many of today's robots are inspired by nature, contributing to the field of bio-inspired robotics.\nThe concept of creating machines that can operate autonomously dates back to classical times, but research into the functionality and potential uses of robots did not grow substantially until the 20th century. Throughout history, it has been frequently assumed by various scholars, inventors, engineers, and technicians  that robots will one day be able to mimic human behavior and manage tasks in a human-like fashion. Today, robotics is a rapidly growing field, as technological advances continue; researching, designing, and building new robots serve various practical purposes, whether domestically, commercially, or militarily. Many robots are built to do jobs that are hazardous to people, such as defusing bombs, finding survivors in unstable ruins, and exploring mines and shipwrecks. Robotics is also used in STEM (science, technology, engineering, and mathematics) as a teaching aid. The advent of nanorobots, microscopic robots that can be injected into the human body, could revolutionize medicine and human health.Robotics is a branch of engineering that involves the conception, design, manufacture, and operation of robots. This field overlaps with computer engineering, computer science (especially artificial intelligence), electronics, mechatronics, nanotechnology and bioengineering.",
  },
  {
    id: 89,
    content:
      "Reinforcement learning (RL) is an area of machine learning concerned with how software agents ought to take actions in an environment in order to maximize the notion of cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning.\nReinforcement learning differs from supervised learning in not needing labelled input/output pairs be presented, and in not needing sub-optimal actions to be explicitly corrected. Instead the focus is on finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge).\nThe environment is typically stated in the form of a Markov decision process (MDP), because many reinforcement learning algorithms for this context utilize dynamic programming techniques. The main difference between the classical dynamic programming methods  and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the MDP and they target large MDPs where exact methods become infeasible.",
  },
  {
    id: 90,
    content:
      "Since the arrival of early social networking sites in the early 2000s, online social networking platforms have expanded exponentially, with the biggest names in social media in the mid-2010s being Facebook, Instagram, Twitter and Snapchat. The massive influx of personal information that has become available online and stored in the cloud has put user privacy at the forefront of discussion regarding the database's ability to safely store such personal information. The extent to which users and social media platform administrators can access user profiles has become a new topic of ethical consideration, and the legality, awareness, and boundaries of subsequent privacy violations are critical concerns in advance of the technological age.A social network is a social structure made up of a set of social actors (such as individuals or organizations), sets of dyadic ties, and other social interactions between actors. Privacy concerns with social networking services is a subset of data privacy, involving the right of mandating personal privacy concerning storing, re-purposing, provision to third parties, and displaying of information pertaining to oneself via the Internet. Social network security and privacy issues result from the large amounts of information these sites process each day. Features that invite users to participate in\u2014messages, invitations, photos, open platform applications and other applications are often the venues for others to gain access to a user's private information. In addition, the technologies needed to deal with user's information may intrude their privacy.\nThe advent of the Web 2.0 has caused social profiling and is a growing concern for internet privacy. Web 2.0 is the system that facilitates participatory information sharing and collaboration on the Internet, in social networking media websites like Facebook and MySpace. These social networking sites have seen a boom in their popularity beginning in the late 2000s. Through these websites many people are giving their personal information out on the internet. These social networks keep track of all interactions used on their sites and save them for later use. Issues include cyberstalking, location disclosure, social profiling, 3rd party personal information disclosure, and government use of social network websites in investigations without the safeguard of a search warrant.\n\n",
  },
  {
    id: 91,
    content:
      "Learning is the process of acquiring new understanding, knowledge, behaviors, skills, values, and preferences. The ability to learn is possessed by humans, animals, and some machines; there is also evidence for some kind of learning in certain plants. Some learning is immediate, induced by a single event (e.g. being burned by a hot stove), but much skill and knowledge accumulates from repeated experiences. The changes induced by learning often last a lifetime, and it is hard to distinguish learned material that seems to be \"lost\" from that which cannot be retrieved.Human learning starts at birth (it might even start before) and continues until death as a consequence of ongoing interactions between people and their environment. The nature and processes involved in learning are studied in many fields, including educational psychology, neuropsychology, experimental psychology, and pedagogy. Research in such fields has led to the identification of various sorts of learning. For example, learning may occur as a result of habituation, or classical conditioning, operant conditioning or as a result of more complex activities such as play, seen only in relatively intelligent animals. Learning may occur consciously or without conscious awareness. Learning that an aversive event can't be avoided nor escaped may result in a condition called learned helplessness. There is evidence for human behavioral learning prenatally, in which habituation has been observed as early as 32 weeks into gestation, indicating that the central nervous system is sufficiently developed and primed for learning and memory to occur very early on in development.Play has been approached by several theorists as the first form of learning. Children experiment with the world, learn the rules, and learn to interact through play. Lev Vygotsky agrees that play is pivotal for children's development, since they make meaning of their environment through playing educational games. For Vygotsky, however, play is the first form of learning language and communication and the stage where a child begins to understand rules and symbols.",
  },
  {
    id: 92,
    content:
      "Social network analysis (SNA) is the process of investigating social structures  through the use of networks and graph theory. It characterizes networked structures in terms of nodes (individual actors, people, or things within the network) and the ties, edges, or links (relationships or interactions) that connect them.  Examples of social structures commonly visualized through social network analysis include social media networks, memes spread, information circulation, friendship and acquaintance networks, business networks, knowledge networks, difficult working relationships, social networks, collaboration graphs, kinship, disease transmission, and sexual relationships. These networks are often visualized through sociograms in which nodes are represented as points and ties are represented as lines. These visualizations provide a means of qualitatively assessing networks by varying the visual representation of their nodes and edges to reflect attributes of interest.Social network analysis has emerged as a key technique in modern sociology.  It has also gained a significant following in anthropology, biology, demography, communication studies, economics, geography, history, information science, organizational studies, political science, public health,  social psychology, development studies, sociolinguistics, and computer science and is now commonly available as a consumer tool (see the list of SNA software).\n\n",
  },
  {
    id: 93,
    content:
      "In machine learning, the perceptron is an algorithm for supervised learning of binary classifiers.  A binary classifier is a function which can decide whether or not an input, represented by a vector of numbers, belongs to some specific class.  It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector.",
  },
  {
    id: 94,
    content:
      "Discourse analysis (DA), or discourse studies, is an approach to the analysis of written, vocal, or sign language use, or any significant semiotic event.\nThe objects of discourse analysis (discourse, writing, conversation, communicative event) are variously defined in terms of coherent sequences of sentences, propositions, speech, or turns-at-talk. Contrary to much of traditional linguistics, discourse analysts not only study language use 'beyond the sentence boundary' but also prefer to analyze 'naturally occurring' language use, not invented examples. Text linguistics is a closely related field. The essential difference between discourse analysis and text linguistics is that discourse analysis aims at revealing socio-psychological characteristics of a person/persons rather than text structure.Discourse analysis has been taken up in a variety of disciplines in the humanities and social sciences, including linguistics, education, sociology, anthropology, social work, cognitive psychology, social psychology, area studies, cultural studies, international relations, human geography,  environmental science, communication studies, biblical studies, public relations and translation studies, each of which is subject to its own assumptions, dimensions of analysis, and methodologies.",
  },
  {
    id: 95,
    content:
      "Text simplification is an operation used in natural language processing to modify, enhance,  classify or otherwise process an existing corpus of human-readable text in such a way that the grammar and structure of the prose is greatly simplified, while the underlying meaning and information remains the same. Text simplification is an important area of research, because natural human languages ordinarily contain large vocabularies and complex compound constructions that are not easily processed through automation. In terms of reducing language diversity, semantic compression can be employed to limit and simplify a set of words used in given texts.",
  },
  {
    id: 96,
    content:
      "A recursive neural network is a kind of deep neural network created by applying the same set of weights recursively over a structured input, to produce a structured prediction over variable-size input structures, or a scalar prediction on it, by traversing a given structure in topological order. RNNs have been successful, for instance, in learning sequence and tree structures in natural language processing, mainly phrase and sentence continuous representations based on word embedding. RNNs have first been introduced to learn distributed representations of structure, such as logical terms.\nModels and general frameworks have been developed in further works since the 1990s.\n\n",
  },
  {
    id: 97,
    content:
      'Probability is the branch of mathematics concerning numerical descriptions of how likely an event is to occur or how likely it is that a proposition is true.  Probability is a number between 0 and 1, where, roughly  speaking, 0 indicates impossibility and 1 indicates certainty. The higher the probability of an event, the more likely it is that the event will occur. A simple example is the tossing of a fair (unbiased) coin. Since the coin is fair, the two outcomes ("heads" and "tails") are both equally probable; the probability of "heads" equals the probability of "tails"; and since no other outcomes are possible, the probability  of either "heads" or "tails" is 1/2 (which could also be written as 0.5 or 50%).\nThese concepts have been given an axiomatic mathematical formalization in probability theory, which is used widely in such areas of study as mathematics, statistics, finance, gambling, science (in particular physics), artificial intelligence/machine learning, computer science, game theory, and philosophy to, for example, draw inferences about the expected frequency of events. Probability theory is also used to describe the underlying mechanics and regularities of complex systems.',
  },
  {
    id: 98,
    content:
      'Variational Bayesian methods are a family of techniques for approximating intractable integrals arising in Bayesian inference and machine learning.  They are typically used in complex statistical models consisting of observed variables (usually termed "data") as well as unknown parameters and latent variables, with various sorts of relationships among the three types of random variables, as might be described by a graphical model.  As typical in Bayesian inference, the parameters and latent variables are grouped together as "unobserved variables". Variational Bayesian methods are primarily used for two purposes:\n\nTo provide an analytical approximation to the posterior probability of the unobserved variables, in order to do statistical inference over these variables.\nTo derive a lower bound for the marginal likelihood (sometimes called the "evidence") of the observed data (i.e. the marginal probability of the data given the model, with marginalization performed over unobserved variables).  This is typically used for performing model selection, the general idea being that a higher marginal likelihood for a given model indicates a better fit of the data by that model and hence a greater probability that the model in question was the one that generated the data. (See also the Bayes factor article.)In the former purpose (that of approximating a posterior probability), variational Bayes is an alternative to Monte Carlo sampling methods \u2014 particularly, Markov chain Monte Carlo methods such as Gibbs sampling \u2014 for taking a fully Bayesian approach to statistical inference over complex distributions that are difficult to evaluate directly or sample. In particular, whereas Monte Carlo techniques provide a numerical approximation to the exact posterior using a set of samples, Variational Bayes provides a locally-optimal, exact analytical solution to an approximation of the posterior.\nVariational Bayes can be seen as an extension of the EM (expectation-maximization) algorithm from maximum a posteriori estimation (MAP estimation) of the single most probable value of each parameter to fully Bayesian estimation which computes (an approximation to) the entire posterior distribution of the parameters and latent variables.  As in EM, it finds a set of optimal parameter values, and it has the same alternating structure as does EM, based on a set of interlocked (mutually dependent) equations that cannot be solved analytically. \nFor many applications, variational Bayes produces solutions of comparable accuracy to Gibbs sampling at greater speed.  However, deriving the set of equations used to update the parameters iteratively often requires a large amount of work compared with deriving the comparable Gibbs sampling equations.  This is the case even for many models that are conceptually quite simple, as is demonstrated below in the case of a basic non-hierarchical model with only two parameters and no latent variables.',
  },
  {
    id: 99,
    content:
      "In computer science, Monte Carlo tree search (MCTS) is a heuristic search algorithm for some kinds of decision processes, most notably those employed in game play. MCTS was introduced in 2006 for computer Go. It has been used in other board games like chess and shogi, games with incomplete information such as bridge and poker, as well as in turn-based-strategy video games (such as Total War: Rome II's implementation in the high level campaign AI).",
  },
  {
    id: 100,
    content:
      "Natural language processing (NLP) is a subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.\nChallenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.",
  },
  {
    id: 101,
    content:
      "Epistemic modal logic is a subfield of modal logic that is concerned with reasoning about knowledge.  While epistemology has a long philosophical tradition dating back to Ancient Greece, epistemic logic is a much more recent development with applications in many fields, including philosophy, theoretical computer science, artificial intelligence, economics and linguistics.  While philosophers since Aristotle have discussed modal logic, and Medieval philosophers such as Avicenna, Ockham, and Duns Scotus developed many of their observations, it was C. I. Lewis who created the first symbolic and systematic approach to the topic, in 1912.  It continued to mature as a field, reaching its modern form in 1963 with the work of Kripke.\n\n",
  },
  {
    id: 102,
    content:
      'In computer science, a heap is a specialized tree-based data structure which is essentially an almost complete tree that satisfies the heap property: in a max heap, for any given node C, if P is a parent node of C, then the key (the value) of P is greater than or equal to the key of C. In a min heap, the key of P is less than or equal to the key of C. The node at the "top" of the heap (with no parents) is called the root node.\nThe heap is one maximally efficient implementation of an abstract data type called a priority queue, and in fact, priority queues are often referred to as "heaps", regardless of how they may be implemented. In a heap, the highest (or lowest) priority element is always stored at the root. However, a heap is not a sorted structure; it can be regarded as being partially ordered. A heap is a useful data structure when it is necessary to repeatedly remove the object with the highest (or lowest) priority.\nA common implementation of a heap is the binary heap, in which the tree is a binary tree (see figure). The heap data structure, specifically the binary heap, was introduced by J. W. J. Williams in 1964, as a data structure for the heapsort sorting algorithm. Heaps are also crucial in several efficient graph algorithms such as Dijkstra\'s algorithm. When a heap is a complete binary tree, it has a smallest possible height\u2014a heap with N nodes and for each node a branches always has loga N height.\nNote that, as shown in the graphic, there is no implied ordering between siblings or cousins and no implied sequence for an in-order traversal (as there would be in, e.g., a binary search tree). The heap relation mentioned above applies only between nodes and their parents, grandparents, etc.  The maximum number of children each node can have depends on the type of heap.',
  },
  {
    id: 103,
    content:
      "In computer science, a search algorithm is any algorithm which solves the search problem, namely, to retrieve information stored within some data structure, or calculated in the search space of a problem domain, either with discrete or continuous values. Specific applications of search algorithms include:\n\nProblems in combinatorial optimization, such as:\nThe vehicle routing problem, a form of shortest path problem\nThe knapsack problem: Given a set of items, each with a weight and a value, determine the number of each item to include in a collection so that the total weight is less than or equal to a given limit and the total value is as large as possible.\nThe nurse scheduling problem\nProblems in constraint satisfaction, such as:\nThe map coloring problem\nFilling in a sudoku or crossword puzzle\nIn game theory and especially combinatorial game theory, choosing the best move to make next (such as with the minmax algorithm)\nFinding a combination or password from the whole set of possibilities\nFactoring an integer (an important problem in cryptography)\nOptimizing an industrial process, such as a chemical reaction, by changing the parameters of the process (like temperature, pressure, and pH)\nRetrieving a record from a database\nFinding the maximum or minimum value in a list or array\nChecking to see if a given value is present in a set of valuesThe classic search problems described above and web search are both problems in information retrieval, but are generally studied as separate subfields and are solved and evaluated differently. Web search problems are generally focused on filtering and finding documents that are most relevant to human queries. Classic search algorithms are typically evaluated on how fast they can find a solution, and whether that solution is guaranteed to be optimal. Though information retrieval algorithms must be fast, the quality of ranking is more important, as is whether good results have been left out and bad results included.\nThe appropriate search algorithm often depends on the data structure being searched, and may also include prior knowledge about the data. Some database structures are specially constructed to make search algorithms faster or more efficient, such as a search tree, hash map, or a database index. Search algorithms can be classified based on their mechanism of searching. Linear search algorithms check every record for the one associated with a target key in a linear fashion. Binary, or half interval searches, repeatedly target the center of the search structure and divide the search space in half. Comparison search algorithms improve on linear searching by successively eliminating records based on comparisons of the keys until the target record is found, and can be applied on data structures with a defined order. Digital search algorithms work based on the properties of digits in data structures that use numerical keys. Finally, hashing directly maps keys to records based on a hash function. Searches outside a linear search require that the data be sorted in some way.\nAlgorithms are often evaluated by their computational complexity, or maximum theoretical run time. Binary search functions, for example, have a maximum complexity of O(log n), or logarithmic time. This means that the maximum number of operations needed to find the search target is a logarithmic function of the size of the search space.",
  },
  {
    id: 104,
    content:
      "Seq2seq is a family of machine learning approaches used for language processing. Applications include language translation, image captioning, conversational models and text summarization.",
  },
  { id: 105, content: "" },
  {
    id: 106,
    content:
      "In machine learning, feature learning or representation learning is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data. This replaces manual feature engineering and allows a machine to both learn the features  and use them to perform  a specific task.\nFeature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensor data has not yielded to attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms.\nFeature learning can be either supervised or unsupervised.\n\nIn supervised feature learning, features are learned using labeled input data. Examples include supervised neural networks, multilayer perceptron and (supervised) dictionary learning.\nIn unsupervised feature learning, features are learned with unlabeled input data.  Examples include dictionary learning, independent component analysis, autoencoders, matrix factorization and various forms of clustering.",
  },
  {
    id: 107,
    content:
      'In probability theory, conditional probability is a measure of the probability of an event occurring given that another event has (by assumption, presumption, assertion or evidence) occurred. If the event of interest is A and the event B is known or assumed to have occurred, "the conditional probability of A given B", or "the probability of A under the condition B", is usually written as P(A | B), or sometimes PB(A) or P(A / B). For example, the probability that any given person has a cough on any given day may be only 5%. But if we know or assume that the person has a cold, then they are much more likely to be coughing. The conditional probability that someone coughing is unwell might be 75%, then: P(Cough) = 5%; P(Sick | Cough) = 75%\nThe concept of conditional probability is one of the most fundamental and one of the most important in probability theory. But conditional probabilities can be quite slippery and require careful interpretation.  For example, there need not be a causal relationship between A and B, and they don\'t have to occur simultaneously.\nP(A | B) may or may not be equal to P(A) (the unconditional probability of A).  If P(A | B) = P(A), then events A and B are said to be "independent": in such a case, knowledge about either event does not give information on the other. P(A | B) (the conditional probability of A given B) typically differs from P(B | A). For example, if a person has dengue, they might have a 90% chance of testing positive for dengue.  In this case what is being measured is that if event B ("having dengue") has occurred, the probability of A (test is positive) given that B (having dengue) occurred is 90%: that is, P(A | B) = 90%.  Alternatively, if a person tests positive for dengue they may have only a 15% chance of actually having this rare disease because the false positive rate for the test may be high. In this case what is being measured is the probability of the event  B (having dengue) given that the event A (test is positive) has occurred: P(B | A) = 15%. Falsely equating the two probabilities causes various errors of reasoning such as the base rate fallacy. Conditional probabilities can be reversed using Bayes\' theorem.\nConditional probabilities can be displayed in a conditional probability table.',
  },
  {
    id: 108,
    content:
      "A Bayesian network, Bayes network, belief network, decision network, Bayes(ian) model or probabilistic directed acyclic graphical model is a probabilistic graphical model (a type of statistical model) that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG). Bayesian networks are ideal for taking an event that occurred and predicting the likelihood that any one of several possible known causes was the contributing factor.  For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases.\nEfficient algorithms can perform inference and learning in Bayesian networks. Bayesian networks that model sequences of variables (e.g. speech signals or protein sequences) are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams.",
  },
  {
    id: 109,
    content:
      "A decision tree  is a decision support tool that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements.\nDecision trees are commonly used in operations research, specifically in decision analysis, to help identify a strategy most likely to reach a goal, but are also a popular tool in machine learning.",
  },
  {
    id: 110,
    content:
      "Problem solving consists of using generic or ad hoc methods in an orderly manner to find solutions to problems. Some of the problem-solving techniques developed and used in philosophy, artificial intelligence, computer science, engineering, mathematics, or medicine are related to mental problem-solving techniques studied in psychology.",
  },
  {
    id: 111,
    content:
      'Hidden Markov Model (HMM) is a statistical Markov model in which the system being modeled is assumed to be a Markov process \u2013 call it \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   \u2013 with unobservable ("hidden") states. HMM assumes that there is another process \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   whose behavior "depends" on \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  . The goal is to learn about \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   by observing \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  . HMM stipulates that, for each time instance \n  \n    \n      \n        \n          n\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle n_{0}}\n  , the conditional probability distribution of \n  \n    \n      \n        \n          Y\n          \n            \n              n\n              \n                0\n              \n            \n          \n        \n      \n    \n    {\\displaystyle Y_{n_{0}}}\n   given the history \n  \n    \n      \n        {\n        \n          X\n          \n            n\n          \n        \n        =\n        \n          x\n          \n            n\n          \n        \n        \n          }\n          \n            n\n            \u2264\n            \n              n\n              \n                0\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\{X_{n}=x_{n}\\}_{n\\leq n_{0}}}\n   must NOT depend on \n  \n    \n      \n        {\n        \n          x\n          \n            n\n          \n        \n        \n          }\n          \n            n\n            <\n            \n              n\n              \n                0\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\{x_{n}\\}_{n<n_{0}}}\n  .\nHidden Markov models are known for their applications to reinforcement learning and temporal pattern recognition such as speech, handwriting, gesture recognition, part-of-speech tagging, musical score following, partial discharges and bioinformatics.\n\n',
  },
  {
    id: 112,
    content:
      'Mathematical optimization (alternatively spelt optimisation) or mathematical programming is the selection of a best element (with regard to some criterion) from some set of available alternatives. Optimization problems of sorts arise in all quantitative disciplines from computer science and engineering to operations research and economics, and the development of solution methods has been of interest in mathematics for centuries.In the simplest case, an optimization problem consists of maximizing or minimizing a real function by systematically choosing input values from within an allowed set and computing the value of the function. The generalization of optimization theory and techniques to other formulations constitutes a large area of applied mathematics. More generally, optimization includes finding "best available" values of some objective function given a defined domain (or input), including a variety of different types of objective functions and different types of domains.',
  },
  {
    id: 113,
    content:
      "Tokenization is the process of demarcating and possibly classifying sections of a string of input characters. The resulting tokens are then passed on to some other form of processing. The process can be considered a sub-task of parsing input.",
  },
  {
    id: 114,
    content:
      'In statistical classification, including machine learning, two main approaches are called the generative approach and the discriminative approach. These compute classifiers by different approaches, differing in the degree of statistical modelling. Terminology is inconsistent, but three major types can be distinguished, following Jebara (2004):\n\nGiven an observable variable X and a target variable Y, a generative model is a statistical model of the joint probability distribution on X \u00d7 Y, \n  \n    \n      \n        P\n        (\n        X\n        ,\n        Y\n        )\n      \n    \n    {\\displaystyle P(X,Y)}\n  ;\nA discriminative model is a model of the conditional probability of the target Y, given an observation x, symbolically, \n  \n    \n      \n        P\n        (\n        Y\n        \n          |\n        \n        X\n        =\n        x\n        )\n      \n    \n    {\\displaystyle P(Y|X=x)}\n  ; and\nClassifiers computed without using a probability model are also referred to loosely as "discriminative".The distinction between these last two classes is not consistently made; Jebara (2004) refers to these three classes as generative learning, conditional learning, and discriminative learning, but Ng & Jordan (2002) only distinguish two classes, calling them generative classifiers (joint distribution) and discriminative classifiers (conditional distribution or no distribution), not distinguishing between the latter two classes. Analogously, a classifier based on a generative model is a generative classifier, while a classifier based on a discriminative model is a discriminative classifier, though this term also refers to classifiers that are not based on a model.\nStandard examples of each, all of which are linear classifiers, are:\n\ngenerative classifiers:\nnaive Bayes classifier and\nlinear discriminant analysis\ndiscriminative model:\nlogistic regression\nnon-model classifier:\nperceptron and\nsupport vector machine.In application to classification, one wishes to go from an observation x to a label y (or probability distribution on labels). One can compute this directly, without using a probability distribution (distribution-free classifier); one can estimate the probability of a label given an observation, \n  \n    \n      \n        P\n        (\n        Y\n        \n          |\n        \n        X\n        =\n        x\n        )\n      \n    \n    {\\displaystyle P(Y|X=x)}\n   (discriminative model), and base classification on that; or one can estimate the joint distribution \n  \n    \n      \n        P\n        (\n        X\n        ,\n        Y\n        )\n      \n    \n    {\\displaystyle P(X,Y)}\n   (generative model), from that compute the conditional probability \n  \n    \n      \n        P\n        (\n        Y\n        \n          |\n        \n        X\n        =\n        x\n        )\n      \n    \n    {\\displaystyle P(Y|X=x)}\n  , and then base classification on that. These are increasingly indirect, but increasingly probabilistic, allowing more domain knowledge and probability theory to be applied. In practice different approaches are used, depending on the particular problem, and hybrids can combine strengths of multiple approaches.\n\n',
  },
  { id: 115, content: "" },
  {
    id: 116,
    content:
      'Speech recognition is an interdisciplinary subfield of computer science and computational linguistics that develops methodologies and technologies that enable the recognition and translation of spoken language into text by computers. It is also known as automatic speech recognition (ASR), computer speech recognition or speech to text (STT). It incorporates knowledge and research in the computer science, linguistics and computer engineering fields.\nSome speech recognition systems require "training" (also called "enrollment") where an individual speaker reads text or isolated vocabulary into the system. The system analyzes the person\'s specific voice and uses it to fine-tune the recognition of that person\'s speech, resulting in increased accuracy. Systems that do not use training are called "speaker independent" systems. Systems that use training are called "speaker dependent".\nSpeech recognition applications include voice user interfaces such as voice dialing (e.g. "call home"), call routing (e.g. "I would like to make a collect call"), domotic appliance control, search key words (e.g. find a podcast where particular words were spoken), simple data entry (e.g., entering a credit card number), preparation of structured documents (e.g. a radiology report), determining speaker characteristics, speech-to-text processing (e.g., word processors or emails), and aircraft (usually termed direct voice input).\nThe term voice recognition or speaker identification refers to identifying the speaker, rather than what they are saying. Recognizing the speaker can simplify the task of translating speech in systems that have been trained on a specific person\'s voice or it can be used to authenticate or verify the identity of a speaker as part of a security process.\nFrom the technology perspective, speech recognition has a long history with several waves of major innovations. Most recently, the field has benefited from advances in deep learning and big data. The advances are evidenced not only by the surge of academic papers published in the field, but more importantly by the worldwide industry adoption of a variety of deep learning methods in designing and deploying speech recognition systems.\n\n',
  },
  {
    id: 117,
    content:
      'A statistical language model is a probability distribution over sequences of words. Given such a sequence, say of length m, it assigns a probability \n  \n    \n      \n        P\n        (\n        \n          w\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          w\n          \n            m\n          \n        \n        )\n      \n    \n    {\\displaystyle P(w_{1},\\ldots ,w_{m})}\n   to the whole sequence.\nThe language model provides context to distinguish between words and phrases that sound similar. For example, in American English, the phrases "recognize speech" and "wreck a nice beach" sound similar, but mean different things.\nData sparsity is a major problem in building language models. Most possible word sequences are not observed in training. One solution is to make the assumption that the probability of a word only depends on the previous n words. This is known as an n-gram model or unigram model when n = 1. The unigram model is also known as the bag of words model.\nEstimating the relative likelihood of different phrases is useful in many natural language processing applications, especially those that generate text as an output. Language modeling is used in speech recognition, machine translation, part-of-speech tagging, parsing, Optical Character Recognition, handwriting recognition, information retrieval and other applications.\nIn speech recognition, sounds are matched with word sequences. Ambiguities are easier to resolve when evidence from the language model is integrated with a pronunciation model and an acoustic model.\nLanguage models are used in information retrieval in the query likelihood model. There, a separate language model is associated with each document in a collection. Documents are ranked based on the probability of the query Q in the document\'s language model \n  \n    \n      \n        \n          M\n          \n            d\n          \n        \n      \n    \n    {\\displaystyle M_{d}}\n  : \n  \n    \n      \n        P\n        (\n        Q\n        \u2223\n        \n          M\n          \n            d\n          \n        \n        )\n      \n    \n    {\\displaystyle P(Q\\mid M_{d})}\n  . Commonly, the unigram language model is used for this purpose.',
  },
  {
    id: 118,
    content:
      "In probability theory, a normal (or Gaussian or Gauss or Laplace\u2013Gauss) distribution is a type of continuous probability distribution for a real-valued random variable.  The general form of its probability density function is\n\n  \n    \n      \n        f\n        (\n        x\n        )\n        =\n        \n          \n            1\n            \n              \u03c3\n              \n                \n                  2\n                  \u03c0\n                \n              \n            \n          \n        \n        \n          e\n          \n            \u2212\n            \n              \n                1\n                2\n              \n            \n            \n              \n                (\n                \n                  \n                    \n                      x\n                      \u2212\n                      \u03bc\n                    \n                    \u03c3\n                  \n                \n                )\n              \n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle f(x)={\\frac {1}{\\sigma {\\sqrt {2\\pi }}}}e^{-{\\frac {1}{2}}\\left({\\frac {x-\\mu }{\\sigma }}\\right)^{2}}}\n  The parameter \n  \n    \n      \n        \u03bc\n      \n    \n    {\\displaystyle \\mu }\n   is the mean or expectation of the distribution (and also its median and mode); and \n  \n    \n      \n        \u03c3\n      \n    \n    {\\displaystyle \\sigma }\n   is its standard deviation.  The variance of the distribution is \n  \n    \n      \n        \n          \u03c3\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\sigma ^{2}}\n  . A random variable with a Gaussian distribution is said to be normally distributed and is called a normal deviate.\nNormal distributions are important in statistics and are often used in the natural and social sciences to represent real-valued random variables whose distributions are not known. Their importance is partly due to the central limit theorem. It states that, under some conditions, the average of many samples (observations) of a random variable with finite mean and variance is itself a random variable whose distribution converges to a normal distribution as the number of samples increases. Therefore, physical quantities that are expected to be the sum of many independent processes (such as measurement errors) often have distributions that are nearly normal.Moreover, Gaussian distributions have some unique properties that are valuable in analytic studies.  For instance, any linear combination of a fixed collection of normal deviates is a normal deviate.  Many results and methods (such as propagation of uncertainty and least squares parameter fitting) can be derived analytically in explicit form when the relevant variables are normally distributed.\nA normal distribution is sometimes informally called a bell curve. However, many other distributions are bell-shaped (such as the Cauchy, Student's t, and logistic distributions).\n\n",
  },
  {
    id: 119,
    content:
      'First-order logic\u2014also known as predicate logic, quantificational logic, and first-order predicate calculus\u2014is a collection of formal systems used in mathematics, philosophy, linguistics, and computer science. First-order logic uses quantified variables over non-logical objects and allows the use of sentences that contain variables, so that rather than propositions such as Socrates is a man one can have expressions in the form "there exists x such that x is Socrates and x is a man" and there exists is a quantifier while x is a variable. This distinguishes it from propositional logic, which does not use quantifiers or relations; in this sense, propositional logic is the foundation of first-order logic.\nA theory about a topic is usually a first-order logic together with a specified domain of discourse over which the quantified variables range, finitely many functions from that domain to itself, finitely many predicates defined on that domain, and a set of axioms believed to hold for those things. Sometimes "theory" is understood in a more formal sense, which is just a set of sentences in first-order logic.\nThe adjective "first-order" distinguishes first-order logic from higher-order logic in which there are predicates having predicates or functions as arguments, or in which one or both of predicate quantifiers or function quantifiers are permitted. In first-order theories, predicates are often associated with sets. In interpreted higher-order theories, predicates may be interpreted as sets of sets.\nThere are many deductive systems for first-order logic which are both sound (all provable statements are true in all models) and complete (all statements which are true in all models are provable). Although the logical consequence relation is only semidecidable, much progress has been made in automated theorem proving in first-order logic. First-order logic also satisfies several metalogical theorems that make it amenable to analysis in proof theory, such as the L\u00f6wenheim\u2013Skolem theorem and the compactness theorem.\nFirst-order logic is the standard for the formalization of mathematics into axioms and is studied in the foundations of mathematics.\nPeano arithmetic and Zermelo\u2013Fraenkel set theory are axiomatizations of number theory and set theory, respectively, into first-order logic.\nNo first-order theory, however, has the strength to uniquely describe  a structure with an infinite domain, such as the natural numbers or the real line. Axiom systems that do fully describe these two structures (that is, categorical axiom systems) can be obtained in stronger logics such as second-order logic.\nThe foundations of first-order logic were developed independently by Gottlob Frege and Charles Sanders Peirce. For a history of first-order logic and how it came to dominate formal logic, see Jos\u00e9 Ferreir\u00f3s (2001).',
  },
  {
    id: 120,
    content:
      "Object detection is a computer technology related to computer vision and image processing that deals with detecting instances of semantic objects of a certain class (such as humans, buildings, or cars) in digital images and videos. Well-researched domains of object detection include face detection and pedestrian detection. Object detection has applications in many areas of computer vision, including image retrieval and video surveillance.\n\n",
  },
  {
    id: 121,
    content:
      "In statistics, Gibbs sampling or a Gibbs sampler is a Markov chain Monte Carlo (MCMC) algorithm for obtaining a sequence of observations which are approximately from a specified multivariate probability distribution, when direct sampling is difficult.  This sequence can be used to approximate the joint distribution (e.g., to generate a histogram of the distribution); to approximate the marginal distribution of one of the variables, or some subset of the variables (for example, the unknown parameters or latent variables); or to compute an integral (such as the expected value of one of the variables).  Typically, some of the variables correspond to observations whose values are known, and hence do not need to be sampled.\nGibbs sampling is commonly used as a means of statistical inference, especially Bayesian inference.  It is a randomized algorithm (i.e. an algorithm that makes use of random numbers), and is an alternative to deterministic algorithms for statistical inference such as the expectation-maximization algorithm (EM).\nAs with other MCMC algorithms, Gibbs sampling generates a Markov chain of samples, each of which is correlated with nearby samples. As a result, care must be taken if independent samples are desired. Generally, samples from the beginning of the chain (the burn-in period) may not accurately represent the desired distribution and are usually discarded. It has been shown, however, that using a longer chain instead (e.g. a chain that is n times as long as the initially considered chain using a thinning factor of n) leads to better estimates of the true posterior. Thus, thinning should only be applied when time or computer memory are restricted.",
  },
  {
    id: 122,
    content:
      "In statistics, maximum likelihood estimation (MLE) is a method of estimating the parameters of a probability distribution by maximizing a likelihood function, so that under the assumed statistical model the observed data is most probable. The point in the parameter space that maximizes the likelihood function is called the maximum likelihood estimate. The logic of maximum likelihood is both intuitive and flexible, and as such the method has become a dominant means of statistical inference.If the likelihood function is differentiable, the derivative test for determining maxima can be applied. In some cases, the first-order conditions of the likelihood function can be solved explicitly; for instance, the ordinary least squares estimator maximizes the likelihood of the linear regression model. Under most circumstances, however, numerical methods will be necessary to find the maximum of the likelihood function.\nFrom the vantage point of Bayesian inference, MLE is a special case of maximum a posteriori estimation (MAP) that assumes a uniform prior distribution of the parameters. In frequentist inference, MLE is a special case of an extremum estimator, with the objective function being the likelihood.",
  },
  {
    id: 123,
    content:
      'Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees\' habit of overfitting to their training set.The first algorithm for random decision forests was created by Tin Kam Ho using the random subspace method, which, in Ho\'s formulation, is a way to implement the "stochastic discrimination" approach to classification proposed by Eugene Kleinberg.An extension of the algorithm was developed by Leo Breiman and Adele Cutler, who registered "Random Forests" as a trademark (as of 2019, owned by Minitab, Inc.). The extension combines Breiman\'s "bagging" idea and random selection of features, introduced first by Ho and later independently by Amit and Geman in order to construct a collection of decision trees with controlled variance.\n\n',
  },
  {
    id: 124,
    content:
      "WordNet is a lexical database of semantic relations between words in more than 200 languages. WordNet links words into semantic relations including synonyms, hyponyms, and meronyms. The synonyms are grouped into synsets  with short definitions and usage examples. WordNet can thus be seen as a combination and extension of a dictionary and thesaurus. While it is  accessible to human users via a web browser, its primary use is in automatic text analysis and artificial intelligence applications. WordNet was first created in the English language and the English WordNet database and software tools have been released under a BSD style license and are freely available for download from that WordNet website.",
  },
  {
    id: 125,
    content:
      'In computer science, beam search is a heuristic search algorithm that explores a graph by expanding the most promising node in a limited set. Beam search is an optimization of best-first search that reduces its memory requirements. Best-first search is a graph search which orders all partial solutions (states) according to some heuristic. But in beam search, only a predetermined number of best partial solutions are kept as candidates. It is thus a greedy algorithm.\nThe term "beam search" was coined by Raj Reddy of Carnegie Mellon University in 1977.\n\n',
  },
  {
    id: 126,
    content:
      "In mathematics, statistics, and computer science, particularly in machine learning and inverse problems, regularization is the process of adding information in order to solve an ill-posed problem or to prevent overfitting.Regularization applies to objective functions in ill-posed optimization problems.",
  },
  {
    id: 127,
    content:
      "Bidirectional Recurrent Neural Networks (BRNN) connect two hidden layers of opposite directions to the same output. With this form of generative deep learning, the output layer can get information from past (backwards) and future (forward) states simultaneously. Invented in 1997 by Schuster and Paliwal, BRNNs were introduced to increase the amount of input information available to the network. For example, multilayer perceptron (MLPs) and time delay neural network (TDNNs) have limitations on the input data flexibility, as they require their input data to be fixed. Standard recurrent neural network (RNNs) also have restrictions as the future input information cannot be reached from the current state. On the contrary, BRNNs do not require their input data to be fixed. Moreover, their future input information is reachable from the current state. BRNN are especially useful when the context of the input is needed. For example, in handwriting recognition, the performance can be enhanced by knowledge of the letters located before and after the current letter.",
  },
  {
    id: 128,
    content:
      "Tree-adjoining grammar (TAG) is a grammar formalism defined by Aravind Joshi.  Tree-adjoining grammars are somewhat similar to context-free grammars, but the elementary unit of rewriting is the tree rather than the symbol. Whereas context-free grammars have rules for rewriting symbols as strings of other symbols, tree-adjoining grammars have rules for rewriting the nodes of trees as other trees (see tree (graph theory) and tree (data structure)).",
  },
  {
    id: 129,
    content:
      "Robot locomotion is the collective name for the various methods that robots use to transport themselves from place to place.\nWheeled robots are typically quite energy efficient and simple to control. However, other forms of locomotion may be more appropriate for a number of reasons, for example traversing rough terrain, as well as moving and interacting in human environments. Furthermore, studying bipedal and insect-like robots may beneficially impact on biomechanics.\nA major goal in this field is in developing capabilities for robots to autonomously decide how, when, and where to move.  However, coordinating numerous robot joints for even simple matters, like negotiating stairs, is difficult. Autonomous robot locomotion is a major technological obstacle for many areas of robotics, such as humanoids (like Honda's Asimo).",
  },
  {
    id: 130,
    content:
      "Classification is a process related to categorization, the process in which ideas and objects are recognized, differentiated and understood.\nIt may refer to:",
  },
  { id: 131, content: "" },
  {
    id: 132,
    content:
      "A finite-state transducer (FST) is a finite-state machine with two memory tapes, following the terminology for Turing machines: an input tape and an output tape. This contrasts with an ordinary finite-state automaton, which has a single tape.  An FST is a type of finite-state automaton that maps between two sets of symbols.  An FST is more general than a finite-state automaton (FSA).  An FSA defines a formal language by defining a set of accepted strings, while an FST defines relations between sets of strings.\nAn FST will read a set of strings on the input tape and generates a set of relations on the output tape.  An FST can be thought of as a translator or relater between strings in a set.\nIn morphological parsing, an example would be inputting a string of letters into the FST, the FST would then output a string of morphemes.",
  },
  {
    id: 133,
    content:
      "Computer vision is an interdisciplinary scientific field that deals with how computers can gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to understand and automate tasks that the human visual system can do.Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the forms of decisions. Understanding in this context means the transformation of visual images (the input of the retina) into descriptions of the world that make sense to thought processes and can elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory.The scientific discipline of computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, multi-dimensional data from a 3D scanner or medical scanning device. The technological discipline of computer vision seeks to apply its theories and models to the construction of computer vision systems.\nSub-domains of computer vision include scene reconstruction, event detection, video tracking, object recognition, 3D pose estimation, learning, indexing, motion estimation, visual servoing, 3D scene modeling, and image restoration.",
  },
  {
    id: 134,
    content:
      "In linguistic morphology and information retrieval, stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form\u2014generally a written word form. The stem need not be identical to the morphological root of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root. Algorithms for stemming have been studied in computer science since the 1960s. Many search engines treat words with the same stem as synonyms as a kind of query expansion, a process called conflation.\nA computer program or subroutine that stems word may be called a stemming program, stemming algorithm, or stemmer.",
  },
  {
    id: 135,
    content:
      'In formal language theory, a context-free grammar (CFG) is a formal grammar in which every production rule is of the form\n\n  \n    \n      \n        A\n         \n        \u2192\n         \n        \u03b1\n      \n    \n    {\\displaystyle A\\ \\to \\ \\alpha }\n  where \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   is a single nonterminal symbol, and \n  \n    \n      \n        \u03b1\n      \n    \n    {\\displaystyle \\alpha }\n   is a string of terminals and/or nonterminals (\n  \n    \n      \n        \u03b1\n      \n    \n    {\\displaystyle \\alpha }\n   can be empty). A formal grammar is considered "context free" when its production rules can be applied regardless of the context of a nonterminal. No matter which symbols surround it, the single nonterminal on the left hand side can always be replaced by the right hand side. This is what distinguishes it from a context-sensitive grammar.\nA formal grammar is essentially a set of production rules that describe all possible strings in a given formal language. Production rules are simple replacements. For example, the first rule in the picture,\n\n  \n    \n      \n        \u27e8\n        \n          Stmt\n        \n        \u27e9\n        \u2192\n        \u27e8\n        \n          Id\n        \n        \u27e9\n        =\n        \u27e8\n        \n          Expr\n        \n        \u27e9\n        ;\n      \n    \n    {\\displaystyle \\langle {\\text{Stmt}}\\rangle \\to \\langle {\\text{Id}}\\rangle =\\langle {\\text{Expr}}\\rangle ;}\n  replaces \n  \n    \n      \n        \u27e8\n        \n          Stmt\n        \n        \u27e9\n      \n    \n    {\\displaystyle \\langle {\\text{Stmt}}\\rangle }\n   with \n  \n    \n      \n        \u27e8\n        \n          Id\n        \n        \u27e9\n        =\n        \u27e8\n        \n          Expr\n        \n        \u27e9\n        ;\n      \n    \n    {\\displaystyle \\langle {\\text{Id}}\\rangle =\\langle {\\text{Expr}}\\rangle ;}\n  . There can be multiple replacement rules for a given nonterminal symbol. The language generated by a grammar is the set of all strings of terminal symbols that can be derived, by repeated rule applications, from some particular nonterminal symbol ("start symbol").\nNonterminal symbols are used during the derivation process, but may not appear in its final result string. \nLanguages generated by context-free grammars are known as context-free languages (CFL). Different context-free grammars can generate the same context-free language. It is important to distinguish the properties of the language (intrinsic properties) from the properties of a particular grammar (extrinsic properties). The language equality question (do two given context-free grammars generate the same language?) is undecidable.\nContext-free grammars arise in linguistics where they are used to describe the structure of sentences and words in a natural language, and they were in fact invented by the linguist Noam Chomsky for this purpose. By contrast, in computer science, as the use of recursively-defined concepts increased, they were used more and more. In an early application, grammars are used to describe the structure of programming languages. In a newer application, they are used in an essential part of the Extensible Markup Language (XML) called the Document Type Definition.In linguistics, some authors use the term phrase structure grammar to refer to context-free grammars, whereby phrase-structure grammars are distinct from dependency grammars. In computer science, a popular notation for context-free grammars is Backus\u2013Naur form, or BNF.',
  },
  {
    id: 136,
    content:
      "In the field of mathematical modeling, a radial basis function network is an artificial neural network that uses radial basis functions as activation functions. The output of the network is a linear combination of radial basis functions of the inputs and neuron parameters. Radial basis function networks have many uses, including function approximation, time series prediction, classification, and system control. They were first formulated in a 1988 paper by Broomhead and Lowe, both researchers at the Royal Signals and Radar Establishment.",
  },
  { id: 137, content: "" },
  {
    id: 138,
    content:
      "Broadbent's filter model is an early selection theory of attention.",
  },
  {
    id: 139,
    content:
      "A random walk is a mathematical object, known as a stochastic or random process, that describes a path that consists of a succession of random steps on some mathematical space such as the integers. An elementary example of a random walk is the random walk on the integer number line, \n  \n    \n      \n        \n          Z\n        \n      \n    \n    {\\displaystyle \\mathbb {Z} }\n  , which starts at 0 and at each step moves +1 or \u22121 with equal probability. Other examples include the path traced by a molecule as it travels in a liquid or a gas, the search path of a foraging animal, the price of a fluctuating stock and the financial status of a gambler: all can be approximated by random walk models, even though they may not be truly random in reality. As illustrated by those examples, random walks have applications to engineering and many scientific fields including ecology, psychology, computer science, physics, chemistry, biology as well as economics. Random walks explain the observed behaviors of many processes in these fields, and thus serve as a fundamental model for the recorded stochastic activity. As a more mathematical application, the value of \u03c0 can be approximated by the use of random walk in an agent-based modeling environment. The term random walk was first introduced by Karl Pearson in 1905.Various types of random walks are of interest, which can differ in several ways. The term itself most often refers to a special category of Markov chains or Markov processes, but many time-dependent processes are referred to as random walks, with a modifier indicating their specific properties. Random walks (Markov or not) can also take place on a variety of spaces: commonly studied ones include graphs, others on the integers or the real line, in the plane or higher-dimensional vector spaces, on curved surfaces or higher-dimensional Riemannian manifolds, and also on groups finite, finitely generated or Lie. The time parameter can also be manipulated. In the simplest context the walk is in discrete time, that is a sequence of random variables (Xt) = (X1, X2, ...) indexed by the natural numbers. However, it is also possible to define random walks which take their steps at random times, and in that case, the position Xt has to be defined for all times t \u2208 [0,+\u221e). Specific cases or limits of random walks include the L\u00e9vy flight and diffusion models such as Brownian motion.\nRandom walks are a fundamental topic in discussions of Markov processes. Their mathematical study has been extensive. Several properties, including dispersal distributions, first-passage or hitting times, encounter rates, recurrence or transience, have been introduced to quantify their behavior.",
  },
  {
    id: 140,
    content:
      "In the field of mathematical optimization, Lagrangian relaxation is a relaxation method which approximates a difficult problem of constrained optimization by a simpler problem. A solution to the relaxed problem is an approximate solution to the original problem, and provides useful information.\nThe method penalizes violations of inequality constraints using a Lagrange multiplier, which imposes a cost on violations. These added costs are used instead of the strict inequality constraints in the optimization. In practice, this relaxed problem can often be solved more easily than the original problem.\nThe problem of maximizing the Lagrangian function of the dual variables (the Lagrangian multipliers) is the Lagrangian dual problem.\n\n",
  },
  {
    id: 141,
    content:
      'Constraint satisfaction problems (CSPs) are mathematical questions defined as a set of objects whose state must satisfy a number of constraints or limitations. CSPs represent the entities in a problem as a homogeneous collection of finite constraints over variables, which is solved by constraint satisfaction methods. CSPs are the subject of intense research in both artificial intelligence and operations research, since the regularity in their formulation provides a common basis to analyze and solve problems of many seemingly unrelated families. CSPs often exhibit high complexity, requiring a combination of heuristics and combinatorial search methods to be solved in a reasonable time. Constraint Programming (CP) is the field of research that specifically focuses on tackling with this kind of problems. Additionally, boolean satisfiability problem (SAT), the satisfiability modulo theories (SMT), mixed integer programming (MIP) and answer set programming (ASP) are all fields of research focusing on the resolution of particular forms of the constraint satisfaction problem.\nExamples of simple problems that can be modeled as a constraint satisfaction problem include:\n\nEight queens puzzle\nMap coloring problem\nSudoku, Crosswords, Futoshiki, Kakuro (Cross Sums), Numbrix, Hidato and many other logic puzzlesThese are often provided with tutorials of CP, ASP, Boolean SAT and SMT solvers. In the general case, constraint problems can be much harder, and may not be expressible in some of these simpler systems. "Real life" examples include automated planning, lexical disambiguation, musicology and resource allocation.The existence of a solution to a CSP can be viewed as a decision problem. This can be decided by finding a solution, or failing to find a solution after exhaustive search (stochastic algorithms typically never reach an exhaustive conclusion, while directed searches often do, on sufficiently small problems). In some cases the CSP might be known to have solutions beforehand, through some other mathematical inference process.\n\n',
  },
  {
    id: 142,
    content:
      'Parsing, syntax analysis, or syntactic analysis is the process of analyzing a string of symbols, either in natural language, computer languages or data structures, conforming to the rules of a formal grammar.  The term parsing comes from Latin pars (orationis), meaning part (of speech).The term has slightly different meanings in different branches of linguistics and computer science.  Traditional sentence parsing is often performed as a method of understanding the exact meaning of a sentence or word, sometimes with the aid of devices such as sentence diagrams.  It usually emphasizes the importance of grammatical divisions such as subject and predicate.\nWithin computational linguistics the term is used to refer to the formal analysis by a computer of a sentence or other string of words into its constituents, resulting in a parse tree showing their syntactic relation to each other, which may also contain semantic and other information. Some parsing algorithms may generate a parse forest or list of parse trees for a syntactically ambiguous input.The term is also used in psycholinguistics when describing language comprehension.  In this context, parsing refers to the way that human beings analyze a sentence or phrase (in spoken language or text) "in terms of grammatical constituents, identifying the parts of speech, syntactic relations, etc."  This term is especially common when discussing what linguistic cues help speakers to interpret garden-path sentences.\nWithin computer science, the term is used in the analysis of computer languages, referring to the syntactic analysis of the input code into its component parts in order to facilitate the writing of compilers and interpreters. The term may also be used to describe a split or separation.',
  },
  {
    id: 143,
    content:
      "In pattern recognition, the k-nearest neighbors algorithm (k-NN) is a non-parametric method used for classification and regression. In both cases, the input consists of the k closest training examples in the feature space. The output depends on whether k-NN is used for classification or regression:\n\nIn k-NN classification, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.In k-NN regression, the output is the property value for the object. This value is the average of the values of k nearest neighbors.k-NN is a type of instance-based learning, or lazy learning, where the function is only approximated locally and all computation is deferred until function evaluation.\nBoth for classification and regression, a useful technique can be to assign weights to the contributions of the neighbors, so that the nearer neighbors contribute more to the average than the more distant ones. For example, a common weighting scheme consists in giving each neighbor a weight of 1/d, where d is the distance to the neighbor.The neighbors are taken from a set of objects for which the class (for k-NN classification) or the object property value (for k-NN regression) is known. This can be thought of as the training set for the algorithm, though no explicit training step is required.\nA peculiarity of the k-NN algorithm is that it is sensitive to the local structure of the data.\n\n",
  },
  {
    id: 144,
    content:
      "An autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore signal \u201cnoise\u201d. Along with the reduction side, a reconstructing side is learnt, where the autoencoder tries to generate from the reduced encoding a representation as close as possible to its original input, hence its name. Several variants exist to the basic model, with the aim of forcing the learned representations of the input to assume useful properties. Examples are the regularized autoencoders (Sparse, Denoising and Contractive autoencoders), proven effective in learning representations for subsequent classification tasks, and Variational autoencoders, with their recent applications as generative models. Autoencoders are effectively used for solving many applied problems, from face recognition to acquiring the semantic meaning of words.",
  },
  {
    id: 145,
    content:
      'Parsing, syntax analysis, or syntactic analysis is the process of analyzing a string of symbols, either in natural language, computer languages or data structures, conforming to the rules of a formal grammar.  The term parsing comes from Latin pars (orationis), meaning part (of speech).The term has slightly different meanings in different branches of linguistics and computer science.  Traditional sentence parsing is often performed as a method of understanding the exact meaning of a sentence or word, sometimes with the aid of devices such as sentence diagrams.  It usually emphasizes the importance of grammatical divisions such as subject and predicate.\nWithin computational linguistics the term is used to refer to the formal analysis by a computer of a sentence or other string of words into its constituents, resulting in a parse tree showing their syntactic relation to each other, which may also contain semantic and other information. Some parsing algorithms may generate a parse forest or list of parse trees for a syntactically ambiguous input.The term is also used in psycholinguistics when describing language comprehension.  In this context, parsing refers to the way that human beings analyze a sentence or phrase (in spoken language or text) "in terms of grammatical constituents, identifying the parts of speech, syntactic relations, etc."  This term is especially common when discussing what linguistic cues help speakers to interpret garden-path sentences.\nWithin computer science, the term is used in the analysis of computer languages, referring to the syntactic analysis of the input code into its component parts in order to facilitate the writing of compilers and interpreters. The term may also be used to describe a split or separation.',
  },
  {
    id: 146,
    content:
      'The Knowledge Graph is a knowledge base used by Google and its services to enhance its search engine\'s results with information gathered from a variety of sources. The information is presented to users in an infobox next to the search results. Knowledge Graph infoboxes were added to Google\'s search engine in May 2012, starting in the United States, with international expansion by the end of the year. The information covered by the Knowledge Graph grew significantly after launch, tripling its size within seven months (covering 570 million entities and 18 billion facts) and answering "roughly one-third" of the 100 billion monthly searches Google processed in May 2016. The Knowledge Graph has been criticized for providing answers without source attribution or citation.\nInformation from the Knowledge Graph is presented as a box, which Google has referred to as the "knowledge panel", to the right (top on mobile) of search results. According to Google, this information is retrieved from many sources, including the CIA World Factbook, Wikidata, and Wikipedia. In October 2016, Google announced that the Knowledge Graph held over 70 billion facts. There is no official documentation on the technology used for the Knowledge Graph implementation.Information from the Knowledge Graph is used to answer direct spoken questions in Google Assistant and Google Home voice queries.',
  },
  {
    id: 147,
    content:
      "Question answering (QA) is a computer science discipline within the fields of information retrieval and natural language processing (NLP), which is concerned with building systems that automatically answer questions posed by humans in a natural language.",
  },
  { id: 148, content: "" },
  {
    id: 149,
    content:
      "A recurrent neural network (RNN) is a class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior. Derived from feedforward neural networks, RNNs can use their internal state (memory) to process variable length sequences of inputs. This makes them applicable to tasks such as unsegmented, connected handwriting recognition or speech recognition.The term \u201crecurrent neural network\u201d is used indiscriminately to refer to two broad classes of networks with a similar general structure, where one is finite impulse and the other is infinite impulse. Both classes of networks exhibit temporal dynamic behavior. A finite impulse recurrent network is a directed acyclic graph that can be unrolled and replaced with a strictly feedforward neural network, while an infinite impulse recurrent network is a directed cyclic graph that can not be unrolled.\nBoth finite impulse and infinite impulse recurrent networks can have additional stored states, and the storage can be under direct control by the neural network. The storage can also be replaced by another network or graph, if that incorporates time delays or has feedback loops. Such controlled states are referred to as gated state or gated memory, and are part of long short-term memory networks (LSTMs) and gated recurrent units. This is also called Feedback Neural Network.",
  },
  {
    id: 150,
    content:
      "Automatic summarization is the process of shortening a set of data computationally, to create a subset (a summary) that represents the most important or relevant information within the original content. \nIn addition to text, images and videos can also be summarized. Text summarization finds the most informative sentences in a document; image summarization finds the most representative images within an image collection; video summarization extracts the most important frames from the video content.\n\n",
  },
  {
    id: 151,
    content:
      "In general, bootstrapping usually refers to a self-starting process that is supposed to proceed without external input.  In computer technology the term (usually shortened to booting) usually refers to the process of loading the basic software into the memory of a computer after power-on or general reset, especially the operating system which will then take care of loading other software as needed.",
  },
  {
    id: 152,
    content:
      "In probability theory and statistics, Bayes' theorem (alternatively Bayes's theorem,  Bayes's law or Bayes's rule) describes the probability of an event, based on prior knowledge of conditions that might be related to the event. For example, if the risk of developing health problems is known to increase with age, Bayes\u2019s theorem allows the risk to an individual of a known age to be assessed more accurately than simply assuming that the individual is typical of the population as a whole.\nOne of the many applications of Bayes\u2019s theorem is Bayesian inference, a particular approach to statistical inference. When applied, the probabilities involved in Bayes\u2019 theorem may have different probability interpretations. With Bayesian probability interpretation, the theorem expresses how a degree of belief, expressed as a probability, should rationally change to account for the availability of related evidence. Bayesian inference is fundamental to Bayesian statistics.\nBayes\u2019s theorem is named after Reverend Thomas Bayes (; 1701?\u20131761), who first used conditional probability to provide an algorithm (his Proposition 9) that uses evidence to calculate limits on an unknown parameter, published as An Essay towards solving a Problem in the Doctrine of Chances (1763). In what he called a scholium, Bayes extended his algorithm to any unknown prior cause.  Independently of Bayes, Pierre-Simon Laplace in 1774, and later in his 1812 Th\u00e9orie analytique des probabilit\u00e9s, used conditional probability to formulate the relation of an updated posterior probability from a prior probability, given evidence. Sir Harold Jeffreys put Bayes's algorithm and Laplace\u2019s formulation on an axiomatic basis. Jeffreys wrote that Bayes\u2019s theorem \u201cis to the theory of probability what the Pythagorean theorem is to geometry.\u201d",
  },
  { id: 153, content: "" },
  {
    id: 154,
    content:
      'Cosine similarity is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them. The cosine of 0\u00b0 is 1, and it is less than 1 for any angle in the interval (0, \u03c0] radians. It is thus a judgment of orientation and not magnitude: two vectors with the same orientation have a cosine similarity of 1, two vectors oriented at 90\u00b0 relative to each other have a similarity of 0, and two vectors diametrically opposed have a similarity of -1, independent of their magnitude. The cosine similarity is particularly used in positive space, where the outcome is neatly bounded in \n  \n    \n      \n        [\n        0\n        ,\n        1\n        ]\n      \n    \n    {\\displaystyle [0,1]}\n  . The name derives from the term "direction cosine": in this case, unit vectors are maximally "similar" if they\'re parallel and maximally "dissimilar" if they\'re orthogonal (perpendicular). This is analogous to the cosine, which is unity (maximum value) when the segments subtend a zero angle and zero (uncorrelated) when the segments are perpendicular.\nThese bounds apply for any number of dimensions, and the cosine similarity is most commonly used in high-dimensional positive spaces. For example, in information retrieval and text mining, each term is notionally assigned a different dimension and a document is characterised by a vector where the value in each dimension corresponds to the number of times the term appears in the document. Cosine similarity then gives a useful measure of how similar two documents are likely to be in terms of their subject matter.The technique is also used to measure cohesion within clusters in the field of data mining.The term cosine distance is often used for the complement in positive space, that is: \n  \n    \n      \n        \n          D\n          \n            C\n          \n        \n        (\n        A\n        ,\n        B\n        )\n        =\n        1\n        \u2212\n        \n          S\n          \n            C\n          \n        \n        (\n        A\n        ,\n        B\n        )\n        ,\n      \n    \n    {\\displaystyle D_{C}(A,B)=1-S_{C}(A,B),}\n   where \n  \n    \n      \n        \n          D\n          \n            C\n          \n        \n      \n    \n    {\\displaystyle D_{C}}\n   is the cosine distance and \n  \n    \n      \n        \n          S\n          \n            C\n          \n        \n      \n    \n    {\\displaystyle S_{C}}\n   is the cosine similarity. It is important to note, however, that this is not a proper distance metric as it does not have the triangle inequality property\u2014or, more formally, the Schwarz inequality\u2014and it violates the coincidence axiom; to repair the triangle inequality property while maintaining the same ordering, it is necessary to convert to angular distance (see below).\nOne advantage of cosine similarity is its low-complexity, especially for sparse vectors: only the non-zero dimensions need to be considered.\nOther names of cosine similarity are Orchini similarity and the Tucker coefficient of congruence; Ochiai similarity (see below) is cosine similarity applied to binary data.',
  },
  {
    id: 155,
    content:
      'In machine learning, backpropagation (backprop, BP) is a widely used algorithm in training feedforward neural networks for supervised learning. Generalizations of backpropagation exist for other artificial neural networks (ANNs), and for functions generally \u2013 a class of algorithms referred to generically as "backpropagation". In fitting a neural network, backpropagation computes the gradient of the loss function with respect to the weights of the network for a single input\u2013output example, and does so efficiently, unlike a naive direct computation of the gradient with respect to each weight individually. This efficiency makes it feasible to use gradient methods for training multilayer networks, updating weights to minimize loss; gradient descent, or variants such as stochastic gradient descent, are commonly used. The backpropagation algorithm works by computing the gradient of the loss function with respect to each weight by the chain rule, computing the gradient one layer at a time, iterating backward from the last layer to avoid redundant calculations of intermediate terms in the chain rule; this is an example of dynamic programming.The term backpropagation strictly refers only to the algorithm for computing the gradient, not how the gradient is used; but the term is often used loosely to refer to the entire learning algorithm, including how the gradient is used, such as by stochastic gradient descent. Backpropagation generalizes the gradient computation in the delta rule, which is the single-layer version of backpropagation, and is in turn generalized by automatic differentiation, where backpropagation is a special case of reverse accumulation (or "reverse mode"). The term backpropagation and its general use in neural networks was announced in Rumelhart, Hinton & Williams (1986a), then elaborated and popularized in Rumelhart, Hinton & Williams (1986b), but the technique was independently rediscovered many times, and had many predecessors dating to the 1960s; see \u00a7 History. A modern overview is given in the deep learning textbook by Goodfellow, Bengio & Courville (2016).',
  },
  {
    id: 156,
    content:
      'The ImageNet project is a large visual database designed for use in visual object recognition software research. More than 14 million images have been hand-annotated by the project to indicate what objects are pictured and in at least one million of the images, bounding boxes are also provided. ImageNet contains more than 20,000 categories with a typical category, such as "balloon" or "strawberry", consisting of several hundred images. The database of annotations of third-party image URLs is freely available directly from ImageNet, though the actual images are not owned by ImageNet. Since 2010, the ImageNet project runs an annual software contest, the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), where software programs compete to correctly classify and detect objects and scenes. The challenge uses a "trimmed" list of one thousand non-overlapping classes.',
  },
  {
    id: 157,
    content:
      'Deductive reasoning, also deductive logic, is the process of reasoning from one or more statements (premises) to reach a logically certain conclusion.Deductive reasoning goes in the same direction as that of the conditionals, and links premises with conclusions. If all premises are true, the terms are clear, and the rules of deductive logic are followed, then the conclusion reached is necessarily true.\nDeductive reasoning ("top-down logic") contrasts with inductive reasoning ("bottom-up logic") in the following way; in deductive reasoning, a conclusion is reached reductively by applying general rules which hold over the entirety of a closed domain of discourse, narrowing the range under consideration until only the conclusion(s) is left (there is no epistemic uncertainty; i.e. unrecognized parts of the currently available set; all parts of the currently available set are available and recognized). In inductive reasoning, the conclusion is reached by generalizing or extrapolating from specific cases to general rules, i.e., there is epistemic uncertainty (unrecognized parts of the currently available set). However, the inductive reasoning mentioned here is not the same as induction used in mathematical proofs \u2013 mathematical induction is actually a form of deductive reasoning.\nDeductive reasoning differs from abductive reasoning by the direction of the reasoning relative to the conditionals. Deductive reasoning goes in the same direction as that of the conditionals, whereas abductive reasoning goes in the opposite direction to that of the conditionals.',
  },
  {
    id: 158,
    content:
      "Search engine optimisation indexing collects, parses, and stores data to facilitate fast and accurate information retrieval. Index design incorporates interdisciplinary concepts from linguistics, cognitive psychology, mathematics, informatics, and computer science.  An alternate name for the process in the context of search engines designed to find web pages on the Internet is web indexing.\nPopular engines focus on the full-text indexing of online, natural language documents. Media types such as video, audio, and graphics are also searchable.\nMeta search engines reuse the indices of other services and do not store a local index, whereas cache-based search engines permanently store the index along with the corpus. Unlike full-text indices, partial-text services restrict the depth indexed to reduce index size. Larger services typically perform indexing at a predetermined time interval due to the required time and processing costs, while agent-based search engines index in real time.",
  },
  {
    id: 159,
    content:
      "In corpus linguistics, part-of-speech tagging (POS tagging or PoS tagging or POST), also called grammatical tagging or word-category disambiguation, is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech, based on both its definition and its context\u2014i.e., its relationship with adjacent and related words in a phrase, sentence, or paragraph.\nA simplified form of this is commonly taught to school-age children, in the identification of words as nouns, verbs, adjectives, adverbs, etc.\nOnce performed by hand, POS tagging is now done in the context of computational linguistics, using algorithms which associate discrete terms, as well as hidden parts of speech, by a set of descriptive tags. POS-tagging algorithms fall into two distinctive groups: rule-based and stochastic. E. Brill's tagger, one of the first and most widely used English POS-taggers, employs rule-based algorithms.\n\n",
  },
  {
    id: 160,
    content:
      'In the fields of computational linguistics and probability, an n-gram is a contiguous sequence of n items from a given sample of text or speech. The items can be phonemes, syllables, letters, words or base pairs according to the application.  The n-grams typically are collected from a text or speech corpus. When the items are words, n-grams may also be called shingles.Using Latin numerical prefixes, an n-gram of size 1 is referred to as a "unigram"; size 2 is a "bigram" (or, less commonly, a "digram"); size 3 is a "trigram". English cardinal numbers are sometimes used, e.g., "four-gram", "five-gram", and so on.    In computational biology, a polymer or oligomer of a known size is called a k-mer instead of an n-gram, with specific names using Greek numerical prefixes such as "monomer", "dimer", "trimer", "tetramer", "pentamer", etc., or English cardinal numbers, "one-mer", "two-mer", "three-mer", etc.',
  },
  {
    id: 161,
    content:
      "A web search engine or Internet search engine is a software system that is designed to carry out web search (Internet search), which means to search the World Wide Web in a systematic way for particular information specified in a textual web search query. The search results are generally presented in a line of results, often referred to as search engine results pages (SERPs). The information may be a mix of links to web pages, images, videos, infographics, articles, research papers, and other types of files. Some search engines also mine data available in databases or open directories.  Unlike web directories, which are maintained only by human editors, search engines also maintain real-time information by running an algorithm on a web crawler.\nInternet content that is not capable of being searched by a web search engine is generally described as the deep web.",
  },
  {
    id: 162,
    content:
      'Statistical parsing is a group of parsing methods within natural language processing.  The methods have in common that they associate grammar rules with a probability.  Grammar rules are traditionally viewed in computational linguistics as defining the valid sentences in a language.  Within this mindset, the idea of associating each rule with a probability then provides the relative frequency of any given grammar rule and, by deduction, the probability of a complete parse for a sentence.  (The probability associated with a grammar rule may be induced, but the application of that grammar rule within a parse tree and the computation of the probability of the parse tree based on its component rules is a form of deduction.)  Using this concept, statistical parsers make use of a procedure to search over a space of all candidate parses, and the computation of each candidate\'s probability, to derive the most probable parse of a sentence.  The Viterbi algorithm is one popular method of searching for the most probable parse.\n"Search" in this context is an application of search algorithms in artificial intelligence.\nAs an example, think about the sentence "The can can hold water".  A reader would instantly see that there is an object called "the can" and that this object is performing the action \'can\' (i.e. is able to); and the thing the object is able to do is "hold"; and the thing the object is able to hold is "water".  Using more linguistic terminology, "The can" is a noun phrase composed of a determiner followed by a noun, and "can hold water" is a verb phrase which is itself composed of a verb followed by a verb phrase.  But is this the only interpretation of the sentence?  Certainly "The can can" is a perfectly valid noun-phrase referring to a type of dance, and "hold water" is also a valid verb-phrase, although the coerced meaning of the combined sentence is non-obvious.  This lack of meaning is not seen as a problem by most linguists (for a discussion on this point, see Colorless green ideas sleep furiously)  but from a pragmatic point of view it is desirable to obtain the first interpretation rather than the second and statistical parsers achieve this by ranking the interpretations based on their probability.\n(In this example various assumptions about the grammar have been made, such as a simple left-to-right derivation rather than head-driven, its use of noun-phrases rather than the currently fashionable determiner-phrases, and no type-check preventing a concrete noun being combined with an abstract verb phrase.  None of these assumptions affect the thesis of the argument and a comparable argument can be made using any other grammatical formalism.)\nThere are a number of methods that statistical parsing algorithms frequently use.  While few algorithms will use all of these they give a good overview of the general field.  Most statistical parsing algorithms are based on a modified form of chart parsing.  The modifications are necessary to support an extremely large number of grammatical rules and therefore search space, and essentially involve applying classical artificial intelligence algorithms to the traditionally exhaustive search.  Some examples of the optimisations are only searching a likely subset of the search space (stack search), for optimising the search probability (Baum-Welch algorithm) and for discarding parses that are too similar to be treated separately (Viterbi algorithm).\n\n',
  },
  { id: 163, content: "" },
  {
    id: 164,
    content:
      "Vector space model or term vector model is an algebraic model for representing text documents (and any objects, in general) as vectors of identifiers, such as, for example, index terms. It is used in information filtering, information retrieval, indexing and relevancy rankings.  Its first use was in the SMART Information Retrieval System.",
  },
  {
    id: 165,
    content:
      'In artificial neural networks, the activation function of a node defines the output of that node given an input or set of inputs. \nA standard integrated circuit can be seen as a digital network of activation functions that can be "ON" (1) or "OFF" (0), depending on input. This is similar to the behavior of the linear perceptron in neural networks. However, only nonlinear activation functions allow such networks to compute nontrivial problems using only a small number of nodes.',
  },
  {
    id: 166,
    content:
      "Machine translation, sometimes referred to by the abbreviation MT (not to be confused with computer-aided translation, machine-aided human translation (MAHT) or interactive translation), is a sub-field of computational linguistics that investigates the use of software to translate text or speech from one language to another.\nOn a basic level, MT performs simple substitution of words in one  language for words in another, but that alone usually cannot produce a good translation of a text because recognition of whole phrases and their closest counterparts in the target language is needed. Solving this problem with corpus statistical, and neural techniques is a rapidly growing field that is leading to better translations, handling differences in linguistic typology, translation of idioms, and the isolation of anomalies.Current machine translation software often allows for customization by domain or profession (such as weather reports), improving output by limiting the scope of allowable substitutions. This technique is particularly effective in domains where formal or formulaic language is used. It follows that machine translation of government and legal documents more readily produces usable output than conversation or less standardised text.\nImproved output quality can also be achieved by human intervention: for example, some systems are able to translate more accurately if the user has unambiguously identified which words in the text are proper names. With the assistance of these techniques, MT has proven useful as a tool to assist human translators and, in a very limited number of cases, can even produce output that can be used as is (e.g., weather reports).\nThe progress and potential of machine translation have been debated much through its history. Since the 1950s, a number of scholars have questioned the possibility of achieving fully automatic machine translation of high quality, first and most notably by Yehoshua Bar-Hillel. Some critics claim that there are in-principle obstacles to automating the translation process.\n\n",
  },
  {
    id: 167,
    content:
      'A context-sensitive grammar (CSG) is a formal grammar in which the left-hand sides and right-hand sides of any production rules may be surrounded by a context of terminal and nonterminal symbols. Context-sensitive grammars are more general than context-free grammars, in the sense that there are languages that can be described by CSG but not by context-free grammars. Context-sensitive grammars are less general (in the same sense) than unrestricted grammars. Thus, CSG are positioned between context-free and unrestricted grammars in the Chomsky hierarchy.\nA formal language that can be described by a context-sensitive grammar, or, equivalently, by a noncontracting grammar or a linear bounded automaton, is called a context-sensitive language. Some textbooks actually define CSGs as non-contracting, although this is not how Noam Chomsky defined them in 1959. This choice of definition makes no difference in terms of the languages generated (i.e. the two definitions are weakly equivalent), but it does make a difference in terms of what grammars are structurally considered context-sensitive; the latter issue was analyzed by Chomsky in 1963.Chomsky introduced context-sensitive grammars as a way to describe the syntax of natural language where it is often the case that a word may or may not be appropriate in a certain place depending on the context. Walter Savitch has criticized the terminology "context-sensitive" as misleading and proposed "non-erasing" as better explaining the distinction between a CSG and an unrestricted grammar.Although it is well known that certain features of languages (e.g. cross-serial dependency) are not context-free, it is an open question how much of CSG\'s expressive power is needed to capture the context sensitivity found in natural languages. Subsequent research in this area has focused on the more computationally tractable mildly context-sensitive languages. The syntaxes of some visual programming languages can be described by context-sensitive graph grammars.',
  },
  {
    id: 168,
    content:
      'A heuristic technique (; Ancient Greek: \u03b5\u1f51\u03c1\u03af\u03c3\u03ba\u03c9, "find" or "discover"), or a heuristic, is any approach to problem solving or self-discovery that employs a practical method that is not guaranteed to be optimal, perfect or rational, but which is nevertheless sufficient for reaching an immediate, short-term goal. Where finding an optimal solution is impossible or impractical, heuristic methods can be used to speed up the process of finding a satisfactory solution. Heuristics can be mental shortcuts that ease the cognitive load of making a decision. Examples that employ heuristics include using trial and error, a rule of thumb or an educated guess.',
  },
  { id: 169, content: "" },
  {
    id: 170,
    content:
      'In machine learning, a highway network is an approach to optimizing networks and increasing their depth. Highway networks use learned gating mechanisms to regulate information flow, inspired by Long Short-Term Memory (LSTM) recurrent neural networks. The gating mechanisms allow neural networks to have paths for information to follow across different layers ("information highways").Highway networks have been used as part of text sequence labeling and speech recognition tasks.',
  },
  {
    id: 171,
    content:
      "In computer science, the Earley parser is an algorithm for parsing strings that belong to a given context-free language, though (depending on the variant) it may suffer problems with certain nullable grammars. The algorithm, named after its inventor, Jay Earley, is a chart parser that uses dynamic programming; it is mainly used for parsing in computational linguistics. It was first introduced in his dissertation in 1968 (and later appeared in an abbreviated, more legible, form in a journal).\nEarley parsers are appealing because they can parse all context-free languages, unlike LR parsers and LL parsers, which are more typically used in compilers but which can only handle restricted classes of languages.  The Earley parser executes in cubic time in the general case \n  \n    \n      \n        \n          O\n        \n        (\n        \n          n\n          \n            3\n          \n        \n        )\n      \n    \n    {\\displaystyle {O}(n^{3})}\n  , where n is the length of the parsed string, quadratic time for unambiguous grammars \n  \n    \n      \n        \n          O\n        \n        (\n        \n          n\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle {O}(n^{2})}\n  , and linear time for all LR(k) grammars. It performs particularly well when the rules are written left-recursively.",
  },
  {
    id: 172,
    content:
      "In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, most commonly applied to analyzing visual imagery. They are also known as shift invariant or space invariant artificial neural networks (SIANN), based on their shared-weights architecture and translation invariance characteristics. They have applications in image and video recognition, recommender systems, image classification, medical image analysis, natural language processing, and financial time series.",
  },
  {
    id: 173,
    content:
      "In numerical analysis, Newton's method, also known as the Newton\u2013Raphson method, named after Isaac Newton and Joseph Raphson, is a root-finding algorithm which produces successively better approximations to the roots (or zeroes) of a real-valued function. The most basic version starts with a single-variable function f defined for a real variable x, the function's derivative f\u2009\u2032, and an initial guess x0 for a root of f. If the function satisfies sufficient assumptions and the initial guess is close, then\n\n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n        =\n        \n          x\n          \n            0\n          \n        \n        \u2212\n        \n          \n            \n              f\n              (\n              \n                x\n                \n                  0\n                \n              \n              )\n            \n            \n              \n                f\n                \u2032\n              \n              (\n              \n                x\n                \n                  0\n                \n              \n              )\n            \n          \n        \n        \n        .\n      \n    \n    {\\displaystyle x_{1}=x_{0}-{\\frac {f(x_{0})}{f'(x_{0})}}\\,.}\n  is a better approximation of the root than x0. Geometrically, (x1, 0) is the intersection of the x-axis and the tangent of the graph of f at (x0, f\u2009(x0)): that is, the improved guess is the unique root of the linear approximation at the initial point. The process is repeated as\n\n  \n    \n      \n        \n          x\n          \n            n\n            +\n            1\n          \n        \n        =\n        \n          x\n          \n            n\n          \n        \n        \u2212\n        \n          \n            \n              f\n              (\n              \n                x\n                \n                  n\n                \n              \n              )\n            \n            \n              \n                f\n                \u2032\n              \n              (\n              \n                x\n                \n                  n\n                \n              \n              )\n            \n          \n        \n        \n      \n    \n    {\\displaystyle x_{n+1}=x_{n}-{\\frac {f(x_{n})}{f'(x_{n})}}\\,}\n  until a sufficiently precise value is reached. This algorithm is first in the class of Householder's methods, succeeded by Halley's method. The method can also be extended to complex functions and to systems of equations.",
  },
  {
    id: 174,
    content:
      "Reading comprehension is the ability to process text, understand its meaning, and to integrate with what the reader already knows. Fundamental skills required in efficient reading comprehension are knowing meaning of words, ability to understand meaning of a word from discourse context, ability to follow organization of passage and to identify antecedents and references in it, ability to draw inferences from a passage about its contents, ability to identify the main thought of a passage, ability to answer questions answered in a passage, ability to recognize the literary devices or propositional structures used in a passage and determine its tone, to understand the situational mood (agents, objects, temporal and spatial reference points, casual and intentional inflections, etc.) conveyed for assertions, questioning, commanding, refraining etc. and finally ability to determine writer's purpose, intent and point of view, and draw inferences about the writer (discourse-semantics).Ability to comprehend text is influenced by readers' skills and their ability to process information. If word recognition is difficult, students use too much of their processing capacity to read individual words, which interferes with their ability to comprehend what is read. There are many reading strategies to improve reading comprehension and inferences, including improving one's vocabulary, critical text analysis (intertextuality, actual events vs. narration of events, etc.) and practicing deep reading.",
  },
  {
    id: 175,
    content:
      "In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.\nUnlike a statistical ensemble in statistical mechanics, which is usually infinite, a machine learning ensemble consists of only a concrete finite set of alternative models, but typically allows for much more flexible structure to exist among those alternatives.",
  },
  {
    id: 176,
    content:
      "Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually it involves a mathematical embedding from a space with many dimensions per word to a continuous vector space with a much lower dimension.\nMethods to generate this mapping include neural networks, dimensionality reduction on the word co-occurrence matrix, probabilistic models, explainable knowledge base method, and explicit representation in terms of the context in which words appear.Word and phrase embeddings, when used as the underlying input representation, have been shown to boost the performance in NLP tasks such as syntactic parsing and sentiment analysis.",
  },
  {
    id: 177,
    content:
      "In natural language processing, the latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's presence is attributable to one of the document's topics. LDA is an example of a topic model and belongs to the machine learning toolbox and in wider sense to the artificial intelligence toolbox.",
  },
  {
    id: 178,
    content:
      'Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum and first released in 1991, Python\'s design philosophy emphasizes code readability with its notable use of significant whitespace. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.Python is dynamically typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly, procedural), object-oriented, and functional programming. Python is often described as a "batteries included" language due to its comprehensive standard library.Python was conceived in the late 1980s as a successor to the ABC language. Python 2.0, released in 2000, introduced features like list comprehensions and a garbage collection system capable of collecting reference cycles. Python 3.0, released in 2008, was a major revision of the language that is not completely backward-compatible, and much Python 2 code does not run unmodified on Python 3.\nThe Python 2 language was officially discontinued in 2020 (first planned for 2015), and "Python 2.7.18 is the last Python 2.7 release and therefore the last Python 2 release." No more security patches or other improvements will be released for it. With Python 2\'s end-of-life, only  Python 3.5.x and later are supported.\nPython interpreters are available for many operating systems. A global community of programmers develops and maintains CPython, an open source reference implementation. A non-profit organization, the Python Software Foundation, manages and directs resources for Python and CPython development.',
  },
  {
    id: 179,
    content:
      "In machine learning, the radial basis function kernel, or RBF kernel, is a popular kernel function used in various kernelized learning algorithms. In particular, it is commonly used in support vector machine classification.The RBF kernel on two samples x and x', represented as feature vectors in some input space, is defined as\n\n  \n    \n      \n        K\n        (\n        \n          x\n        \n        ,\n        \n          \n            x\n            \u2032\n          \n        \n        )\n        =\n        exp\n        \u2061\n        \n          (\n          \n            \u2212\n            \n              \n                \n                  \u2016\n                  \n                    x\n                  \n                  \u2212\n                  \n                    \n                      x\n                      \u2032\n                    \n                  \n                  \n                    \u2016\n                    \n                      2\n                    \n                  \n                \n                \n                  2\n                  \n                    \u03c3\n                    \n                      2\n                    \n                  \n                \n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle K(\\mathbf {x} ,\\mathbf {x'} )=\\exp \\left(-{\\frac {\\|\\mathbf {x} -\\mathbf {x'} \\|^{2}}{2\\sigma ^{2}}}\\right)}\n  \n  \n    \n      \n        \n          \u2016\n          \n            x\n          \n          \u2212\n          \n            \n              x\n              \u2032\n            \n          \n          \n            \u2016\n            \n              2\n            \n          \n        \n      \n    \n    {\\displaystyle \\textstyle \\|\\mathbf {x} -\\mathbf {x'} \\|^{2}}\n   may be recognized as the squared Euclidean distance between the two feature vectors. \n  \n    \n      \n        \u03c3\n      \n    \n    {\\displaystyle \\sigma }\n   is a free parameter. An equivalent definition involves a parameter \n  \n    \n      \n        \n          \u03b3\n          =\n          \n            \n              \n                1\n                \n                  2\n                  \n                    \u03c3\n                    \n                      2\n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\textstyle \\gamma ={\\tfrac {1}{2\\sigma ^{2}}}}\n  :\n\n  \n    \n      \n        K\n        (\n        \n          x\n        \n        ,\n        \n          \n            x\n            \u2032\n          \n        \n        )\n        =\n        exp\n        \u2061\n        (\n        \u2212\n        \u03b3\n        \u2016\n        \n          x\n        \n        \u2212\n        \n          \n            x\n            \u2032\n          \n        \n        \n          \u2016\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle K(\\mathbf {x} ,\\mathbf {x'} )=\\exp(-\\gamma \\|\\mathbf {x} -\\mathbf {x'} \\|^{2})}\n  Since the value of the RBF kernel decreases with distance and ranges between zero (in the limit) and one (when x = x'), it has a ready interpretation as a similarity measure.\nThe feature space of the kernel has an infinite number of dimensions; for \n  \n    \n      \n        \u03c3\n        =\n        1\n      \n    \n    {\\displaystyle \\sigma =1}\n  , its expansion is:\n\n  \n    \n      \n        \n          \n            \n              \n                exp\n                \u2061\n                \n                  (\n                  \n                    \u2212\n                    \n                      \n                        1\n                        2\n                      \n                    \n                    \u2016\n                    \n                      x\n                    \n                    \u2212\n                    \n                      \n                        x\n                        \u2032\n                      \n                    \n                    \n                      \u2016\n                      \n                        2\n                      \n                    \n                  \n                  )\n                \n              \n              \n                \n                =\n                \n                  \u2211\n                  \n                    j\n                    =\n                    0\n                  \n                  \n                    \u221e\n                  \n                \n                \n                  \n                    \n                      (\n                      \n                        \n                          x\n                        \n                        \n                          \u22a4\n                        \n                      \n                      \n                        \n                          x\n                          \u2032\n                        \n                      \n                      \n                        )\n                        \n                          j\n                        \n                      \n                    \n                    \n                      j\n                      !\n                    \n                  \n                \n                exp\n                \u2061\n                \n                  (\n                  \n                    \u2212\n                    \n                      \n                        1\n                        2\n                      \n                    \n                    \u2016\n                    \n                      x\n                    \n                    \n                      \u2016\n                      \n                        2\n                      \n                    \n                  \n                  )\n                \n                exp\n                \u2061\n                \n                  (\n                  \n                    \u2212\n                    \n                      \n                        1\n                        2\n                      \n                    \n                    \u2016\n                    \n                      \n                        x\n                        \u2032\n                      \n                    \n                    \n                      \u2016\n                      \n                        2\n                      \n                    \n                  \n                  )\n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  \u2211\n                  \n                    j\n                    =\n                    0\n                  \n                  \n                    \u221e\n                  \n                \n                \n                  \u2211\n                  \n                    \u2211\n                    \n                      n\n                      \n                        i\n                      \n                    \n                    =\n                    j\n                  \n                \n                exp\n                \u2061\n                \n                  (\n                  \n                    \u2212\n                    \n                      \n                        1\n                        2\n                      \n                    \n                    \u2016\n                    \n                      x\n                    \n                    \n                      \u2016\n                      \n                        2\n                      \n                    \n                  \n                  )\n                \n                \n                  \n                    \n                      \n                        x\n                        \n                          1\n                        \n                        \n                          \n                            n\n                            \n                              1\n                            \n                          \n                        \n                      \n                      \u22ef\n                      \n                        x\n                        \n                          k\n                        \n                        \n                          \n                            n\n                            \n                              k\n                            \n                          \n                        \n                      \n                    \n                    \n                      \n                        n\n                        \n                          1\n                        \n                      \n                      !\n                      \u22ef\n                      \n                        n\n                        \n                          k\n                        \n                      \n                      !\n                    \n                  \n                \n                exp\n                \u2061\n                \n                  (\n                  \n                    \u2212\n                    \n                      \n                        1\n                        2\n                      \n                    \n                    \u2016\n                    \n                      \n                        x\n                        \u2032\n                      \n                    \n                    \n                      \u2016\n                      \n                        2\n                      \n                    \n                  \n                  )\n                \n                \n                  \n                    \n                      \n                        \n                          \n                            x\n                            \u2032\n                          \n                        \n                        \n                          1\n                        \n                        \n                          \n                            n\n                            \n                              1\n                            \n                          \n                        \n                      \n                      \u22ef\n                      \n                        \n                          \n                            x\n                            \u2032\n                          \n                        \n                        \n                          k\n                        \n                        \n                          \n                            n\n                            \n                              k\n                            \n                          \n                        \n                      \n                    \n                    \n                      \n                        n\n                        \n                          1\n                        \n                      \n                      !\n                      \u22ef\n                      \n                        n\n                        \n                          k\n                        \n                      \n                      !\n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{alignedat}{2}\\exp \\left(-{\\frac {1}{2}}\\|\\mathbf {x} -\\mathbf {x'} \\|^{2}\\right)&=\\sum _{j=0}^{\\infty }{\\frac {(\\mathbf {x} ^{\\top }\\mathbf {x'} )^{j}}{j!}}\\exp \\left(-{\\frac {1}{2}}\\|\\mathbf {x} \\|^{2}\\right)\\exp \\left(-{\\frac {1}{2}}\\|\\mathbf {x'} \\|^{2}\\right)\\\\&=\\sum _{j=0}^{\\infty }\\sum _{\\sum n_{i}=j}\\exp \\left(-{\\frac {1}{2}}\\|\\mathbf {x} \\|^{2}\\right){\\frac {x_{1}^{n_{1}}\\cdots x_{k}^{n_{k}}}{\\sqrt {n_{1}!\\cdots n_{k}!}}}\\exp \\left(-{\\frac {1}{2}}\\|\\mathbf {x'} \\|^{2}\\right){\\frac {{x'}_{1}^{n_{1}}\\cdots {x'}_{k}^{n_{k}}}{\\sqrt {n_{1}!\\cdots n_{k}!}}}\\end{alignedat}}}",
  },
  {
    id: 180,
    content:
      "A log-linear model is a mathematical model that takes the form of a function whose logarithm equals a linear combination of the parameters of the model, which makes it possible to apply (possibly multivariate) linear regression. That is, it has the general form\n\n  \n    \n      \n        exp\n        \u2061\n        \n          (\n          \n            c\n            +\n            \n              \u2211\n              \n                i\n              \n            \n            \n              w\n              \n                i\n              \n            \n            \n              f\n              \n                i\n              \n            \n            (\n            X\n            )\n          \n          )\n        \n      \n    \n    {\\displaystyle \\exp \\left(c+\\sum _{i}w_{i}f_{i}(X)\\right)}\n  ,in which the fi(X) are quantities that are functions of the variable X, in general a vector of values, while c and the wi stand for the model parameters.\nThe term may specifically be used for:\n\nA log-linear plot or graph, which is a type of semi-log plot.\nPoisson regression for contingency tables, a type of generalized linear model.The specific applications of log-linear models are where the output quantity lies in the range 0 to \u221e, for values of the independent variables X,  or more immediately, the transformed quantities fi(X)  in the range \u2212\u221e to +\u221e. This may be contrasted to logistic models, similar to the logistic function, for which the output quantity lies in the range 0 to 1. Thus the contexts where these models are useful or realistic often depends on the range of the values being modelled.",
  },
  {
    id: 181,
    content:
      'Parsing, syntax analysis, or syntactic analysis is the process of analyzing a string of symbols, either in natural language, computer languages or data structures, conforming to the rules of a formal grammar.  The term parsing comes from Latin pars (orationis), meaning part (of speech).The term has slightly different meanings in different branches of linguistics and computer science.  Traditional sentence parsing is often performed as a method of understanding the exact meaning of a sentence or word, sometimes with the aid of devices such as sentence diagrams.  It usually emphasizes the importance of grammatical divisions such as subject and predicate.\nWithin computational linguistics the term is used to refer to the formal analysis by a computer of a sentence or other string of words into its constituents, resulting in a parse tree showing their syntactic relation to each other, which may also contain semantic and other information. Some parsing algorithms may generate a parse forest or list of parse trees for a syntactically ambiguous input.The term is also used in psycholinguistics when describing language comprehension.  In this context, parsing refers to the way that human beings analyze a sentence or phrase (in spoken language or text) "in terms of grammatical constituents, identifying the parts of speech, syntactic relations, etc."  This term is especially common when discussing what linguistic cues help speakers to interpret garden-path sentences.\nWithin computer science, the term is used in the analysis of computer languages, referring to the syntactic analysis of the input code into its component parts in order to facilitate the writing of compilers and interpreters. The term may also be used to describe a split or separation.',
  },
  {
    id: 182,
    content:
      'Parsing, syntax analysis, or syntactic analysis is the process of analyzing a string of symbols, either in natural language, computer languages or data structures, conforming to the rules of a formal grammar.  The term parsing comes from Latin pars (orationis), meaning part (of speech).The term has slightly different meanings in different branches of linguistics and computer science.  Traditional sentence parsing is often performed as a method of understanding the exact meaning of a sentence or word, sometimes with the aid of devices such as sentence diagrams.  It usually emphasizes the importance of grammatical divisions such as subject and predicate.\nWithin computational linguistics the term is used to refer to the formal analysis by a computer of a sentence or other string of words into its constituents, resulting in a parse tree showing their syntactic relation to each other, which may also contain semantic and other information. Some parsing algorithms may generate a parse forest or list of parse trees for a syntactically ambiguous input.The term is also used in psycholinguistics when describing language comprehension.  In this context, parsing refers to the way that human beings analyze a sentence or phrase (in spoken language or text) "in terms of grammatical constituents, identifying the parts of speech, syntactic relations, etc."  This term is especially common when discussing what linguistic cues help speakers to interpret garden-path sentences.\nWithin computer science, the term is used in the analysis of computer languages, referring to the syntactic analysis of the input code into its component parts in order to facilitate the writing of compilers and interpreters. The term may also be used to describe a split or separation.',
  },
  {
    id: 183,
    content:
      'A cognitive computer combines artificial intelligence and machine-learning algorithms, in an approach which attempts to reproduce the behaviour of the human brain. It generally adopts a Neuromorphic engineering approach. \nAn example of a cognitive computer implemented by using neural networks and deep learning is provided by the IBM company\'s Watson machine. A subsequent development by IBM is the TrueNorth microchip architecture, which is designed to be closer in structure to the human brain than the von Neumann architecture used in conventional computers. In 2017 Intel announced its own version of a cognitive chip in "Loihi", which will be available to university and research labs in 2018. Intel, Qualcomm, and others are improving neuromorphic processors steadily, Intel with its Pohoiki Beach and Springs systems',
  },
  {
    id: 184,
    content:
      "In multivariate statistics and the clustering of data, spectral clustering techniques make use of the spectrum (eigenvalues) of the similarity matrix of the data to perform dimensionality reduction before clustering in fewer dimensions. The similarity matrix is provided as an input and consists of a quantitative assessment of the relative similarity of each pair of points in the dataset.\nIn application to image segmentation, spectral clustering is known as segmentation-based object categorization.\n\n",
  },
  {
    id: 185,
    content:
      "Natural language processing (NLP) is a subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.\nChallenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.",
  },
  {
    id: 186,
    content:
      'A regular expression (shortened as regex or regexp; also referred to as rational expression) is a sequence of characters that define a search pattern. Usually such patterns are used by string searching algorithms for "find" or "find and replace" operations on strings, or for input validation. It is a technique developed in theoretical computer science and formal language theory.\nThe concept arose in the 1950s when the American mathematician Stephen Cole Kleene formalized the description of a regular language. The concept came into common use with Unix text-processing utilities. Different syntaxes for writing regular expressions have existed since the 1980s, one being the POSIX standard and another, widely used, being the Perl syntax.\nRegular expressions are used in search engines, search and replace dialogs of word processors and text editors, in text processing utilities such as sed and AWK and in lexical analysis. Many programming languages provide regex capabilities either built-in or via libraries.',
  },
  {
    id: 187,
    content:
      "Deep learning  (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised.Deep learning architectures such as deep neural networks, deep belief networks, recurrent neural networks and convolutional neural networks have been applied to fields including computer vision, speech recognition, natural language processing, audio recognition, social network filtering, machine translation, bioinformatics, drug design, medical image analysis, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.Artificial neural networks (ANNs) were inspired by information processing and distributed communication nodes in biological systems. ANNs have various differences from biological brains.  Specifically, neural networks tend to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) and analog.\n\n",
  },
  {
    id: 188,
    content:
      "For a broader class of literature, see Academic publishing.\n\nScientific literature comprises scholarly publications that report original empirical and theoretical work in the natural and social sciences, and within an academic field, often abbreviated as the literature.  Academic publishing is the process of contributing the results of one's research into the literature, which often requires a peer-review process. \nOriginal scientific research published for the first time in scientific journals is called the primary literature. Patents and technical reports, for minor research results and engineering and design work (including computer software), can also be considered primary literature. \nSecondary sources include review articles (which summarize the findings of published studies to highlight advances and new lines of research) and books (for large projects or broad arguments, including compilations of articles). \nTertiary sources might include encyclopedias and similar works intended for broad public consumption.",
  },
  {
    id: 189,
    content:
      'Conditional random fields (CRFs) are a class of statistical modeling method often applied in pattern recognition and machine learning and used for structured prediction. Whereas a classifier predicts a label for a single sample without considering "neighboring" samples, a CRF can take context into account. To do so, the prediction is modeled as a graphical model, which implements dependencies between the predictions. What kind of graph is used depends on the application. For example, in natural language processing, linear chain CRFs are popular, which implement sequential dependencies in the predictions. In image processing the graph typically connects locations to nearby and/or similar locations to enforce that they receive similar predictions.\nOther examples where CRFs are used are: labeling or parsing of sequential data for natural language processing or biological sequences, POS tagging, shallow parsing, named entity recognition, gene finding, peptide critical functional region finding, and object recognition and image segmentation in computer vision.',
  },
  {
    id: 190,
    content:
      'In statistics, the logistic model (or logit model) is used to model the probability of a certain class or event existing such as pass/fail, win/lose, alive/dead or healthy/sick.  This can be extended to model several classes of events such as determining whether an image contains a cat, dog, lion, etc.  Each object being detected in the image would be assigned a probability between 0 and 1 and the sum adding to one.\nLogistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (a form of binary regression). Mathematically, a binary logistic model has a dependent variable with two possible values, such as pass/fail which is represented by an indicator variable, where the two values are labeled "0" and "1". In the logistic model, the log-odds (the logarithm of the odds) for the value labeled "1" is a linear combination of one or more independent variables ("predictors"); the independent variables can each be a binary variable (two classes, coded by an indicator variable) or a continuous variable (any real value). The corresponding probability of the value labeled "1" can vary between 0 (certainly the value "0") and 1 (certainly the value "1"), hence the labeling; the function that converts log-odds to probability is the logistic function, hence the name. The unit of measurement for the log-odds scale is called a logit, from logistic unit, hence the alternative names. Analogous models with a different sigmoid function instead of the logistic function can also be used, such as the probit model; the defining characteristic of the logistic model is that increasing one of the independent variables multiplicatively scales the odds of the given outcome at a constant rate, with each independent variable having its own parameter; for a binary dependent variable this generalizes the odds ratio.\nIn a binary logistic regression model, the dependent variable has two levels (categorical). Outputs with more than two values are modeled by multinomial logistic regression and, if the multiple categories are ordered, by ordinal logistic regression (for example the proportional odds ordinal logistic model). The logistic regression model itself simply models probability of output in terms of input and does not perform statistical classification (it is not a classifier), though it can be used to make a classifier, for instance by choosing a cutoff value and classifying inputs with probability greater than the cutoff as one class, below the cutoff as the other; this is a common way to make a binary classifier. The coefficients are generally not computed by a closed-form expression, unlike linear least squares; see \u00a7 Model fitting. The logistic regression as a general statistical model was originally developed and popularized primarily by Joseph Berkson, beginning in Berkson (1944), where he coined "logit"; see \u00a7 History.',
  },
  {
    id: 191,
    content:
      "In computer science, a data structure is a data organization, management, and storage format that enables efficient access and modification. More precisely, a data structure is a collection of data values, the relationships among them, and the functions or operations that can be applied to the data.",
  },
  {
    id: 192,
    content:
      'Parsing, syntax analysis, or syntactic analysis is the process of analyzing a string of symbols, either in natural language, computer languages or data structures, conforming to the rules of a formal grammar.  The term parsing comes from Latin pars (orationis), meaning part (of speech).The term has slightly different meanings in different branches of linguistics and computer science.  Traditional sentence parsing is often performed as a method of understanding the exact meaning of a sentence or word, sometimes with the aid of devices such as sentence diagrams.  It usually emphasizes the importance of grammatical divisions such as subject and predicate.\nWithin computational linguistics the term is used to refer to the formal analysis by a computer of a sentence or other string of words into its constituents, resulting in a parse tree showing their syntactic relation to each other, which may also contain semantic and other information. Some parsing algorithms may generate a parse forest or list of parse trees for a syntactically ambiguous input.The term is also used in psycholinguistics when describing language comprehension.  In this context, parsing refers to the way that human beings analyze a sentence or phrase (in spoken language or text) "in terms of grammatical constituents, identifying the parts of speech, syntactic relations, etc."  This term is especially common when discussing what linguistic cues help speakers to interpret garden-path sentences.\nWithin computer science, the term is used in the analysis of computer languages, referring to the syntactic analysis of the input code into its component parts in order to facilitate the writing of compilers and interpreters. The term may also be used to describe a split or separation.',
  },
  {
    id: 193,
    content:
      'Machine learning (ML) is the study of computer algorithms that improve automatically through experience. It is seen as a subset of artificial intelligence. Machine learning algorithms build a mathematical model based on sample data, known as "training data", in order to make predictions or decisions without being explicitly programmed to do so. Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or infeasible to develop  conventional algorithms to perform the needed tasks.\nMachine learning is closely related to computational statistics, which focuses on making predictions using computers. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning. In its application across business problems, machine learning is also referred to as predictive analytics.\n\n',
  },
  {
    id: 194,
    content:
      "In artificial intelligence, an expert system is a computer system that emulates the decision-making ability of a human expert.\nExpert systems are designed to solve complex problems by reasoning through bodies of knowledge, represented mainly as if\u2013then rules rather than through conventional procedural code. The first expert systems were created in the 1970s and then proliferated in the 1980s. Expert systems were among the first truly successful forms of artificial intelligence (AI) software. \nAn expert system is divided into two subsystems: the inference engine and the knowledge base. The knowledge base represents facts and rules. The inference engine applies the rules to the known facts to deduce new facts. Inference engines can also include explanation and debugging abilities.\n\n",
  },
  {
    id: 195,
    content:
      "Linear programming (LP, also called linear optimization) is a method to achieve the best outcome (such as maximum profit or lowest cost) in a mathematical model whose requirements are represented by linear relationships. Linear programming is a special case of mathematical programming (also known as mathematical optimization).\nMore formally, linear programming is a technique for the optimization of a linear objective function, subject to linear equality and linear inequality constraints. Its feasible region is a convex polytope, which is a set defined as the intersection of finitely many half spaces, each of which is defined by a linear inequality. Its objective function is a real-valued affine (linear) function defined on this polyhedron. A linear programming algorithm finds a point in the polytope where this function has the smallest (or largest) value if such a point exists.\nLinear programs are problems that can be expressed in canonical form as\n\n  \n    \n      \n        \n          \n            \n              \n              \n                \n                  Maximize\n                \n              \n              \n              \n                \n                  \n                    c\n                  \n                  \n                    \n                      T\n                    \n                  \n                \n                \n                  x\n                \n              \n            \n            \n              \n              \n                \n                  subject to\n                \n              \n              \n              \n                A\n                \n                  x\n                \n                \u2264\n                \n                  b\n                \n              \n            \n            \n              \n              \n                \n                  and\n                \n              \n              \n              \n                \n                  x\n                \n                \u2265\n                \n                  0\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}&{\\text{Maximize}}&&\\mathbf {c} ^{\\mathrm {T} }\\mathbf {x} \\\\&{\\text{subject to}}&&A\\mathbf {x} \\leq \\mathbf {b} \\\\&{\\text{and}}&&\\mathbf {x} \\geq \\mathbf {0} \\end{aligned}}}\n  where x represents the vector of variables (to be determined), c and b are vectors of (known) coefficients, A is a (known) matrix of coefficients, and \n  \n    \n      \n        (\n        \u22c5\n        \n          )\n          \n            \n              T\n            \n          \n        \n      \n    \n    {\\displaystyle (\\cdot )^{\\mathrm {T} }}\n   is the matrix transpose. The expression to be maximized or minimized is called the objective function (cTx in this case). The inequalities Ax \u2264 b and x \u2265 0 are the constraints which specify a convex polytope over which the objective function is to be optimized. In this context, two vectors are comparable when they have the same dimensions. If every entry in the first is less-than or equal-to the corresponding entry in the second, then it can be said that the first vector is less-than or equal-to the second vector.\nLinear programming can be applied to various fields of study. It is widely used in mathematics, and to a lesser extent in business, economics, and for some engineering problems. Industries that use linear programming models include transportation, energy, telecommunications, and manufacturing. It has proven useful in modeling diverse types of problems in planning, routing, scheduling, assignment, and design.",
  },
  {
    id: 196,
    content:
      'Supervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs. It infers a function from labeled training data consisting of a set of training examples.  In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal).  A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a "reasonable" way (see inductive bias).\nThe parallel task in human and animal psychology is often referred to as concept learning.',
  },
  {
    id: 197,
    content:
      "Information retrieval (IR) is the activity of obtaining information system resources that are relevant to an information need from a collection of those resources.  Searches can be based on full-text or other content-based indexing. Information retrieval is the science of searching for information in a document, searching for documents themselves, and also searching for the metadata that describes data, and for databases of texts, images or sounds.\nAutomated information retrieval systems are used to reduce what has been called information overload. An IR system is a software system that provides access to books, journals and other documents; stores and manages those documents. Web search engines are the most visible IR applications.\n\n",
  },
  {
    id: 198,
    content:
      'Monte Carlo methods, or Monte Carlo experiments, are a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results. The underlying concept is to use randomness to solve problems that might be deterministic in principle. They are often used in physical and mathematical problems and are most useful when it is difficult or impossible to use other approaches. Monte Carlo methods are mainly used in three problem classes: optimization, numerical integration, and generating draws from a probability distribution.\nIn physics-related problems, Monte Carlo methods are useful for simulating systems with many coupled degrees of freedom, such as fluids, disordered materials, strongly coupled solids, and cellular structures (see cellular Potts model, interacting particle systems, McKean\u2013Vlasov processes, kinetic models of gases). Other examples include modeling phenomena with significant uncertainty in inputs such as the calculation of risk in business and, in mathematics, evaluation of multidimensional definite integrals  with complicated boundary conditions.  In application to systems engineering problems (space, oil exploration, aircraft design, etc.), Monte Carlo\u2013based predictions of failure, cost overruns and schedule overruns are routinely better than human intuition or alternative "soft" methods.In principle, Monte Carlo methods can be used to solve any problem having a probabilistic interpretation. By the law of large numbers, integrals described by the expected value of some random variable can be approximated by taking the empirical mean (a.k.a. the sample mean) of independent samples of the variable. When the probability distribution of the variable is parametrized, mathematicians often use a Markov chain Monte Carlo (MCMC) sampler. The central idea is to design a judicious Markov chain model with a prescribed stationary probability distribution. That is, in the limit, the samples being generated by the MCMC method will be samples from the desired (target) distribution. By the ergodic theorem, the stationary distribution is approximated by the empirical measures of the random states of the MCMC sampler.\nIn other problems, the objective is generating draws from a sequence of probability distributions satisfying a nonlinear evolution equation. These flows of probability distributions can always be interpreted as the distributions of the random states of a Markov process whose transition probabilities depend on the distributions of the current random states (see McKean\u2013Vlasov processes, nonlinear filtering equation). In other instances we are given a flow of probability distributions with an increasing level of sampling complexity (path spaces models with an increasing time horizon, Boltzmann\u2013Gibbs measures associated with decreasing temperature parameters, and many others). These models can also be seen as the evolution of the law of the random states of a nonlinear Markov chain. A natural way to simulate these sophisticated nonlinear Markov processes is to sample multiple copies of the process, replacing in the evolution equation the unknown distributions of the random states by the sampled empirical measures. In contrast with traditional Monte Carlo and MCMC methodologies these mean field particle techniques rely on sequential interacting samples. The terminology mean field reflects the fact that each of the samples (a.k.a. particles, individuals, walkers, agents, creatures, or phenotypes) interacts with the empirical measures of the process. When the size of the system tends to infinity, these random empirical measures converge to the deterministic distribution of the random states of the nonlinear Markov chain, so that the statistical interaction between particles vanishes.',
  },
  {
    id: 199,
    content:
      'In machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract "topics" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: "dog" and "bone" will appear more often in documents about dogs, "cat" and "meow" will appear in documents about cats, and "the" and "is" will appear equally in both. A document typically concerns multiple topics in different proportions; thus, in a document that is 10% about cats and 90% about dogs, there would probably be about 9 times more dog words than cat words. The "topics" produced by topic modeling techniques are clusters of similar words. A topic model captures this intuition in a mathematical framework, which allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each document\'s balance of topics is.\nTopic models are also referred to as probabilistic topic models, which refers to statistical algorithms for discovering the latent semantic structures of an extensive text body. In the age of information, the amount of the written material we encounter each day is simply beyond our processing capacity. Topic models can help to organize and offer insights for us to understand large collections of unstructured text bodies. Originally developed as a text-mining tool, topic models have been used to detect instructive structures in data such as genetic information, images, and networks. They also have applications in other fields such as bioinformatics.',
  },
  {
    id: 200,
    content:
      "Natural language processing (NLP) is a subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.\nChallenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.",
  },
  {
    id: 201,
    content:
      "Machine translation, sometimes referred to by the abbreviation MT (not to be confused with computer-aided translation, machine-aided human translation (MAHT) or interactive translation), is a sub-field of computational linguistics that investigates the use of software to translate text or speech from one language to another.\nOn a basic level, MT performs simple substitution of words in one  language for words in another, but that alone usually cannot produce a good translation of a text because recognition of whole phrases and their closest counterparts in the target language is needed. Solving this problem with corpus statistical, and neural techniques is a rapidly growing field that is leading to better translations, handling differences in linguistic typology, translation of idioms, and the isolation of anomalies.Current machine translation software often allows for customization by domain or profession (such as weather reports), improving output by limiting the scope of allowable substitutions. This technique is particularly effective in domains where formal or formulaic language is used. It follows that machine translation of government and legal documents more readily produces usable output than conversation or less standardised text.\nImproved output quality can also be achieved by human intervention: for example, some systems are able to translate more accurately if the user has unambiguously identified which words in the text are proper names. With the assistance of these techniques, MT has proven useful as a tool to assist human translators and, in a very limited number of cases, can even produce output that can be used as is (e.g., weather reports).\nThe progress and potential of machine translation have been debated much through its history. Since the 1950s, a number of scholars have questioned the possibility of achieving fully automatic machine translation of high quality, first and most notably by Yehoshua Bar-Hillel. Some critics claim that there are in-principle obstacles to automating the translation process.\n\n",
  },
  {
    id: 202,
    content:
      "Neural machine translation (NMT) is an approach to machine translation that uses an artificial neural network to predict the likelihood of a sequence of words, typically modeling entire sentences in a single integrated model.",
  },
  {
    id: 203,
    content:
      "k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells. It is popular for cluster analysis in data mining. k-means clustering minimizes within-cluster variances (squared Euclidean distances), but not regular Euclidean distances, which would be the more difficult Weber problem: the mean optimizes squared errors, whereas only the geometric median minimizes Euclidean distances. For instance, Better Euclidean solutions can be found using k-medians and k-medoids.\nThe problem is computationally difficult (NP-hard); however, efficient heuristic algorithms converge quickly to a local optimum. These are usually similar to the expectation-maximization algorithm for mixtures of Gaussian distributions via an iterative refinement approach employed by both k-means and Gaussian mixture modeling. They both use cluster centers to model the data; however, k-means clustering tends to find clusters of comparable spatial extent, while the expectation-maximization mechanism allows clusters to have different shapes.\nThe algorithm has a loose relationship to the k-nearest neighbor classifier, a popular machine learning technique for classification that is often confused with k-means due to the name. Applying the 1-nearest neighbor classifier to the cluster centers obtained by k-means classifies new data into the existing clusters. This is known as nearest centroid classifier or Rocchio algorithm.",
  },
  {
    id: 204,
    content:
      'A document is a written, drawn, presented, or memorialized representation of thought, often the manifestation of non-fictional, as well as fictional, content.  The word originates from the Latin documentum, which denotes a "teaching" or "lesson": the verb doce\u014d denotes "to teach". In the past, the word was usually used to denote a written proof useful as evidence of a truth or fact. In the computer age, "document" usually denotes a primarily textual computer file, including its structure and format, e.g. fonts, colors, and images. Contemporarily, "document" is not defined by its transmission medium, e.g., paper, given the existence of electronic documents. "Documentation" is distinct because it has more denotations than "document". Documents are also distinguished from "realia", which are three-dimensional objects that would otherwise satisfy the definition of "document" because they memorialize or represent thought; documents are considered more as 2 dimensional representations. While documents are able to have large varieties of customization, all documents are able to be shared freely, and have the right to do so, creativity can be represented by documents, also. History, events, examples, opinion, etc. all can be expressed in documents.',
  },
  {
    id: 205,
    content:
      "In linguistics, a treebank is a parsed text corpus that annotates syntactic or semantic sentence structure. The construction of parsed corpora in the early 1990s revolutionized computational linguistics, which benefitted from large-scale empirical data. The exploitation of treebank data has been important ever since the first large-scale treebank, The Penn Treebank, was published. However, although originating in computational linguistics, the value of treebanks is becoming more widely appreciated in linguistics research as a whole. For example, annotated treebank data has been crucial in syntactic research to test linguistic theories of sentence structure against large quantities of naturally occurring examples.\n\n",
  },
  {
    id: 206,
    content:
      "Citation Network is a social network which contains paper sources and linked by co-citation relationships. Egghe & Rousseau once (1990, p. 228) explain \"when a document di cites a document dj, we can show this by an arrow going from the node representing di to the document representing dj. In this way the documents from a collection D form a directed graph, which is called a 'citation graph' or 'citation network'\u2009\".",
  },
  {
    id: 207,
    content:
      "Lexicography is divided into two separate but equally important groups:\n\nPractical lexicography is the art or craft of compiling, writing and editing dictionaries.\nTheoretical lexicography is the scholarly discipline of analyzing and describing the semantic, syntagmatic, and paradigmatic relationships within the lexicon (vocabulary) of a language, developing theories of dictionary components and structures linking the data in dictionaries, the needs for information by users in specific types of situations, and how users may best access the data incorporated in printed and electronic dictionaries. This is sometimes referred to as 'metalexicography'.There is some disagreement on the definition of lexicology, as distinct from lexicography. Some use \"lexicology\" as a synonym for theoretical lexicography; others use it to mean a branch of linguistics pertaining to the inventory of words in a particular language.\nA person devoted to lexicography is called a lexicographer.",
  },
  {
    id: 208,
    content:
      'In statistics, latent variables (from Latin: present participle of lateo (\u201clie hidden\u201d), as opposed to observable variables) are variables that are not directly observed but are rather inferred (through a mathematical model) from other variables that are observed (directly measured). Mathematical models that aim to explain observed variables in terms of latent variables are called latent variable models. Latent variable models are used in many disciplines, including psychology, demography, economics, engineering, medicine, physics, machine learning/artificial intelligence, bioinformatics, chemometrics, natural language processing, econometrics, management and the social sciences.\nLatent variables may correspond to aspects of physical reality. These could in principle be measured, but may not be for practical reasons. In this situation, the term hidden variables is commonly used (reflecting the fact that the variables are meaningful, but not observable). Other latent variables correspond to abstract concepts, like categories, behavioral or mental states, or data structures. The terms hypothetical variables or hypothetical constructs may be used in these situations.\nThe use of latent variables can serve to reduce the dimensionality of data. Many observable variables can be aggregated in a model to represent an underlying concept, making it easier to understand the data. In this sense, they serve a function similar to that of scientific theories. At the same time, latent variables link observable ("sub-symbolic") data in the real world to symbolic data in the modeled world.',
  },
  {
    id: 209,
    content:
      "A modeling language is any artificial language that can be used to express information or knowledge or systems in a structure that is defined by a consistent set of rules. The rules are used for interpretation of the meaning of components in the structure.",
  },
  {
    id: 210,
    content:
      "Natural-language generation (NLG) is a software process that transforms structured data into natural language.  It can be used to produce long form content for organizations to automate custom reports, as well as produce custom content for a web or mobile application. It can also be used to generate short blurbs of text in interactive conversations (a chatbot) which might even be read out  by a text-to-speech system.\nAutomated NLG can be compared to the process humans use when they turn ideas into writing or speech. Psycholinguists prefer the term language production for this process, which can also be described in mathematical terms, or modeled in a computer for psychological research.  NLG systems can also be compared to translators of artificial computer languages, such as decompilers or transpilers, which also produce human-readable code generated from an intermediate representation. Human languages tend to be considerably more complex and allow for much more ambiguity and variety of expression than programming languages, which makes NLG more challenging.\nNLG may be viewed as the opposite of natural-language understanding: whereas in natural-language understanding, the system needs to disambiguate the input sentence to produce the machine representation language, in NLG the system needs to make decisions about how to put a concept into words.  The practical considerations in building NLU vs. NLG systems are not symmetrical.  NLU needs to deal with ambiguous or erroneous user input, whereas the ideas the system wants to express through NLG are generally known precisely. NLG needs to choose a specific, self-consistent textual representation from many potential representations, whereas NLU generally tries to produce a single, normalized representation of the idea expressed.NLG has existed for a long time but commercial NLG technology has only recently become widely available. NLG techniques range from simple template-based systems like a mail merge that generates form letters, to systems that have a complex understanding of human grammar.  NLG can also be accomplished by training a statistical model using machine learning, typically on a large corpus of human-written texts.",
  },
  {
    id: 211,
    content:
      'In theoretical computer science and mathematics, the theory of computation is the branch that deals with how efficiently problems can be solved on a model of computation, using an algorithm.  The field is divided into three major branches: automata theory and languages, computability theory, and computational complexity theory, which are linked by the question: "What are the fundamental capabilities and limitations of computers?".In order to perform a rigorous study of computation, computer scientists work with a mathematical abstraction of computers called a model of computation.  There are several models in use, but the most commonly examined is the Turing machine. Computer scientists study the Turing machine because it is simple to formulate, can be analyzed and used to prove results, and because it represents what many consider the most powerful possible "reasonable" model of computation (see Church\u2013Turing thesis). It might seem that the potentially infinite memory capacity is an unrealizable attribute, but any decidable problem solved by a Turing machine will always require only a finite amount of memory. So in principle, any problem that can be solved (decided) by a Turing machine can be solved by a computer that has a finite amount of memory.',
  },
  {
    id: 212,
    content:
      "Biomedical text mining (including biomedical natural language processing or BioNLP) refers to the methods and study of how text mining may be applied to texts and literature of the biomedical and molecular biology domains. As a field of research, biomedical text mining incorporates ideas from natural language processing, bioinformatics, medical informatics and computational linguistics. The strategies developed through studies in this field are frequently applied to the biomedical and molecular biology literature available through services such as PubMed.\n\n",
  },
  {
    id: 213,
    content:
      'A recommender system, or a recommendation system (sometimes replacing \'system\' with a synonym such as platform or engine), is a subclass of information filtering system that seeks to predict the "rating" or "preference" a user would give to an item. They are primarily used in commercial applications.\nRecommender systems are utilized in a variety of areas and are most commonly recognized as playlist generators for video and music services like Netflix, YouTube and Spotify, product recommenders for services such as Amazon, or content recommenders for social media platforms such as Facebook and Twitter. These systems can operate using a single input, like music, or multiple inputs within and across platforms like news, books, and search queries. There are also popular recommender systems for specific topics like restaurants and online dating. Recommender systems have also been developed to explore research articles and experts, collaborators, and financial services.\n\n',
  },
  {
    id: 214,
    content:
      "In mathematical optimization theory, duality or the duality principle is the principle that optimization problems may be viewed from either of two perspectives, the primal problem or the dual problem. The solution to the dual problem provides a lower bound to the solution of the primal (minimization) problem. However in general the optimal values of the primal and dual problems need not be equal. Their difference is called the duality gap. For convex optimization problems, the duality gap is zero under a constraint qualification condition.",
  },
  {
    id: 215,
    content:
      'A lexicon, word-hoard, wordbook, or word-stock is the vocabulary of a person, language, or branch of knowledge (such as nautical or medical).  In linguistics, a lexicon is a language\'s inventory of lexemes. The word "lexicon" derives from the Greek \u03bb\u03b5\u03be\u03b9\u03ba\u03cc\u03bd (lexicon), neuter of \u03bb\u03b5\u03be\u03b9\u03ba\u03cc\u03c2 (lexikos) meaning "of or for words."Linguistic theories generally regard human languages as consisting of two parts: a lexicon, essentially a catalogue of a language\'s words (its wordstock); and a grammar, a system of rules which allow for the combination of those words into meaningful sentences. The lexicon is also thought to include bound morphemes, which cannot stand alone as words (such as most affixes). In some analyses, compound words and certain classes of idiomatic expressions and other collocations are also considered to be part of the lexicon. Dictionaries represent attempts at listing, in alphabetical order, the lexicon of a given language; usually, however, bound morphemes are not included.',
  },
  {
    id: 216,
    content:
      "In computational linguistics and computer science, edit distance is a way of quantifying how dissimilar two strings (e.g., words) are to one another by counting the minimum number of operations required to transform one string into the other. Edit distances find applications in natural language processing, where automatic spelling correction can determine candidate corrections for a misspelled word by selecting words from a dictionary that have a low distance to the word in question. In bioinformatics, it can be used to quantify the similarity of DNA sequences, which can be viewed as strings of the letters A, C, G and T.\nDifferent definitions of an edit distance use different sets of string operations. Levenshtein distance operations are the removal, insertion, or substitution of a character in the string. Being the most common metric, the term Levenshtein distance is often used interchangeably with edit distance.",
  },
  {
    id: 217,
    content:
      'Data preprocessing is an important step in the data mining process. The phrase "garbage in, garbage out" is particularly applicable to data mining and machine learning projects. Data-gathering methods are often loosely controlled, resulting in out-of-range values (e.g., Income: −100), impossible data combinations (e.g., Sex: Male, Pregnant: Yes), missing values, etc. Analyzing data that has not been carefully screened for such problems can produce misleading results. Thus, the representation and quality of data is first and foremost before running an analysis.Often, data preprocessing is the most important phase of a machine learning project, especially in computational biology.',
  },
  {
    id: 218,
    content:
      "In mathematics, semantics, and philosophy of language, the principle of compositionality is the principle that the meaning of a complex expression is determined by the meanings of its constituent expressions and the rules used to combine them. This principle is also called Frege's principle, because Gottlob Frege is widely credited for the first modern formulation of it. The principle was never explicitly stated by Frege, and it was arguably already assumed by George Boole decades before Frege's work.",
  },
  {
    id: 219,
    content:
      "Grammar theory to model symbol strings originated from work in computational linguistics aiming to understand the structure of natural languages. Probabilistic context free grammars (PCFGs) have been applied in probabilistic modeling of RNA structures almost 40 years after they were introduced in computational linguistics.PCFGs extend context-free grammars similar to how hidden Markov models extend regular grammars. Each production is assigned a probability. The probability of a derivation (parse) is the product of the probabilities of the productions used in that derivation. These probabilities can be viewed as parameters of the model, and for large problems it is convenient to learn these parameters via machine learning. A probabilistic grammar's validity is constrained by context of its training dataset.\nPCFGs have application in areas as diverse as natural language processing to the study the structure of RNA molecules and design of programming languages. Designing efficient PCFGs has to weigh factors of scalability and generality. Issues such as grammar ambiguity must be resolved. The grammar design affects results accuracy. Grammar parsing algorithms have various time and memory requirements.",
  },
  { id: 220, content: "" },
  {
    id: 221,
    content:
      "In video games, artificial intelligence (AI) is used to generate responsive, adaptive or intelligent behaviors primarily in non-player characters (NPCs) similar to human-like intelligence. Artificial intelligence has been an integral part of video games since their inception in the 1950s. AI in video games is a distinct subfield and differs from academic AI. It serves to improve the game-player experience rather than machine learning or decision making. During the golden age of arcade video games, the idea of AI opponents was largely popularized, which sported an increasing difficulty level, distinct movement patterns, and in-game events dependent on hash functions based on the player's input. Modern games often implement existing techniques such as pathfinding and decision trees to guide the actions of NPCs. AI is often used in mechanisms which are not immediately visible to the user, such as data mining and procedural-content generation.",
  },
  {
    id: 222,
    content:
      "In computer science, lexical analysis, lexing or tokenization is the process of converting a sequence of characters (such as in a computer program or web page) into a sequence of tokens (strings with an assigned and thus identified meaning). A program that performs lexical analysis may be termed a lexer, tokenizer, or scanner, though scanner is also a term for the first stage of a lexer. A lexer is generally combined with a parser, which together analyze the syntax of programming languages, web pages, and so forth.",
  },
  {
    id: 223,
    content:
      "In natural language processing (NLP), a text graph is a graph representation of a text item (document, passage or sentence). It is typically created as a preprocessing step to support NLP tasks such as text condensationterm disambiguation\n(topic-based) text summarization, relation extraction  and textual entailment.\n\n",
  },
  {
    id: 224,
    content:
      'Handwriting recognition (HWR), also known as Handwritten Text Recognition (HTR), is the ability of a computer to receive and interpret intelligible handwritten input from sources such as paper documents, photographs, touch-screens and other devices. The image of the written text may be sensed "off line" from a piece of paper by optical scanning (optical character recognition) or intelligent word recognition. Alternatively, the movements of the pen tip may be sensed "on line", for example by a pen-based computer screen surface, a generally easier task as there are more clues available. A handwriting recognition system handles formatting, performs correct segmentation into characters, and finds the most plausible words.\n\n',
  },
  {
    id: 225,
    content:
      'Gated recurrent units (GRUs) are a gating mechanism in recurrent neural networks, introduced in 2014 by Kyunghyun Cho et al. The GRU is like a long short-term memory (LSTM) with forget gate but has fewer parameters than LSTM, as it lacks an output gate. \nGRU\'s performance on certain tasks of polyphonic music modeling and speech signal modeling was found to be similar to that of LSTM. GRUs have been shown to exhibit even better performance on certain smaller datasets.However, as shown by Gail Weiss & Yoav Goldberg & Eran Yahav, the LSTM  is "strictly stronger" than the GRU as it can  easily perform unbounded counting, while the GRU cannot. That\'s why the GRU fails to learn simple languages that are learnable by the LSTM.Similarly, as shown by Denny Britz & Anna Goldie & Minh-Thang Luong & Quoc Le of Google Brain, LSTM cells consistently outperform GRU cells in "the first large-scale analysis of architecture variations for Neural Machine Translation."',
  },
  {
    id: 226,
    content:
      "In  statistics, machine learning, and information theory, dimensionality reduction or dimension reduction is the process of reducing the number of random variables under consideration by obtaining a set of principal variables. Approaches can be divided into feature selection and feature extraction.",
  },
  { id: 227, content: "" },
  {
    id: 228,
    content:
      'Speech synthesis is the artificial production of human speech. A computer system used for this purpose is called a speech computer or speech synthesizer, and can be implemented in software or hardware products. A text-to-speech (TTS) system converts normal language text into speech; other systems render symbolic linguistic representations like phonetic transcriptions into speech.Synthesized speech can be created by concatenating pieces of recorded speech that are stored in a database. Systems differ in the size of the stored speech units; a system that stores phones or diphones provides the largest output range, but may lack clarity. For specific usage domains, the storage of entire words or sentences allows for high-quality output. Alternatively, a synthesizer can incorporate a model of the vocal tract and other human voice characteristics to create a completely "synthetic" voice output.The quality of a speech synthesizer is judged by its similarity to the human voice and by its ability to be understood clearly. An intelligible text-to-speech program allows people with visual impairments or reading disabilities to listen to written words on a home computer. Many computer operating systems have included speech synthesizers since the early 1990s.\n\nA text-to-speech system (or "engine") is composed of two parts: a front-end and a back-end. The front-end has two major tasks. First, it converts raw text containing symbols like numbers and abbreviations into the equivalent of written-out words. This process is often called text normalization, pre-processing, or tokenization. The front-end then assigns phonetic transcriptions to each word, and divides and marks the text into prosodic units, like phrases, clauses, and sentences. The process of assigning phonetic transcriptions to words is called text-to-phoneme or grapheme-to-phoneme conversion. Phonetic transcriptions and prosody information together make up the symbolic linguistic representation that is output by the front-end. The back-end\u2014often referred to as the synthesizer\u2014then converts the symbolic linguistic representation into sound. In certain systems, this part includes the computation of the target prosody (pitch contour, phoneme durations), which is then imposed on the output speech.',
  },
  {
    id: 229,
    content:
      "A relationship extraction task requires the detection and classification of semantic relationship mentions within a set of artifacts, typically from text or XML documents. The task is very similar to that of information extraction (IE), but IE additionally requires the removal of repeated relations (disambiguation) and generally refers to the extraction of many different relationships.",
  },
  {
    id: 230,
    content:
      "In statistics, Markov chain Monte Carlo (MCMC) methods comprise a class of algorithms for sampling from a probability distribution. By constructing a Markov chain that has the desired distribution as its equilibrium distribution, one can obtain a sample of the desired distribution by recording states from the chain. The more steps that are included, the more closely the distribution of the sample matches the actual desired distribution. Various algorithms exist for constructing chains, including the Metropolis\u2013Hastings algorithm.",
  },
  {
    id: 231,
    content:
      "Statistical machine translation (SMT) is a machine translation paradigm where translations are generated on the basis of statistical models whose parameters are derived from the analysis of bilingual text corpora.  The statistical approach contrasts with the rule-based approaches to machine translation as well as with example-based machine translation.The first ideas of statistical machine translation were introduced by Warren Weaver in 1949, including the ideas of applying Claude Shannon's information theory. Statistical machine translation was re-introduced in the late 1980s and early 1990s by researchers at IBM's Thomas J. Watson Research Center and has contributed to the significant resurgence in interest in machine translation in recent years. Before the introduction of neural machine translation, it was by far the most widely studied machine translation method.\n\n",
  },
  {
    id: 232,
    content:
      "Andrew James Viterbi (born Andrea Giacomo Viterbi; March 9, 1935) is an American electrical engineer and businessman who co-founded Qualcomm Inc. and invented the Viterbi algorithm.  He is currently Presidential Chair Professor of Electrical Engineering at the University of Southern California's Viterbi School of Engineering, which was named in his honor in 2004 in recognition of his $52 million gift.",
  },
  {
    id: 233,
    content:
      "Phonetics is a branch of linguistics that studies the sounds of human speech, or in the case of sign languages, the equivalent aspects of sign. Phoneticians\u2014linguists who specialize in phonetics\u2014study the physical properties of speech. The field of phonetics is traditionally divided into three subdisciplines based on the research questions involved such as how humans plan and execute movements to produce speech (articulatory phonetics), how different movements affect the properties of the resulting sound (acoustic phonetics), or how humans convert sound waves to linguistic information (auditory phonetics). Traditionally, the minimal linguistic unit of phonetics is the phone\u2014a speech sound in a language\u2014which differs from the phonological unit of phoneme; the phoneme is an abstract categorization of phones.\nPhonetics broadly deals with two aspects of human speech: production\u2014the ways humans make sounds\u2014and perception\u2014the way speech is understood. The modality of a language describes the method by which a language produces and perceives languages. Languages with oral-aural modalities such as English produce speech orally (using the mouth) and perceive speech aurally (using the ears). Many sign languages such as Auslan have a manual-visual modality and produce speech manually (using the hands) and perceive speech visually (using the eyes), while some languages like American Sign have manual-manual dialect for use in tactile signing by deafblind speakers where signs are produced with the hands and perceived with the hands as well.\nLanguage production consists of several interdependent processes which transform a nonlinguistic message into a spoken or signed linguistic signal. After identifying a message to be linguistically encoded, a speaker must select the individual words\u2014known as lexical items\u2014to represent that message in a process called lexical selection. During phonological encoding, the mental representation of the words are assigned their phonological content as a sequence of phonemes to be produced. The phonemes are specified for articulatory features which denote particular goals such as closed lips or the tongue in a particular location. These phonemes are then coordinated into a sequence of muscle commands that can be sent to the muscles, and when these commands are executed properly the intended sounds are produced.\nThese movements disrupt and modify an airstream which results in a sound wave. The modification is done by the articulators, with different places and manners of articulation producing different acoustic results. For example, the words tack and sack both begin with alveolar sounds in English, but differ in how far the tongue is from the alveolar ridge. This difference has large effects on the air stream and thus the sound that is produced. Similarly, the direction and source of the airstream can affect the sound. The most common airstream mechanism is pulmonic\u2014using the lungs\u2014but the glottis and tongue can also be used to produce airstreams.\nLanguage perception is the process by which a linguistic signal is decoded and understood by a listener. In order to perceive speech the continuous acoustic signal must be converted into discrete linguistic units such as phonemes, morphemes, and words. In order to correctly identify and categorize sounds, listeners prioritize certain aspects of the signal that can reliably distinguish between linguistic categories. While certain cues are prioritized over others, many aspects of the signal can contribute to perception. For example, though oral languages prioritize acoustic information, the McGurk effect shows that visual information is used to distinguish ambiguous information when the acoustic cues are unreliable.\nModern phonetics has three main branches:\n\nArticulatory phonetics which studies the way sounds are made with the articulators\nAcoustic phonetics which studies the acoustic results of different articulations\nAuditory phonetics which studies the way listeners perceive and understand linguistic signalsThe first known phonetic studies occurred in the Indic subcontinent during the 6th century BCE, among which was Hindu scholar P\u0101\u1e47ini's articulatory description of voicing, though this pioneering work was primarily concerned with the relationship between written Vedic texts and spoken vernacular languages. With the advent of modern phonetics in the 19th century CE, the focus of scholarship shifted to the physical properties of speech itself. Before the widespread availability of recording devices, phoneticians relied upon phonetic transcription systems to collect and share data. Some systems, such as the International Phonetic Alphabet are still in wide use among phoneticians.",
  },
  { id: 234, content: "" },
  { id: 235, content: "" },
  {
    id: 236,
    content:
      'First-order logic\u2014also known as predicate logic, quantificational logic, and first-order predicate calculus\u2014is a collection of formal systems used in mathematics, philosophy, linguistics, and computer science. First-order logic uses quantified variables over non-logical objects and allows the use of sentences that contain variables, so that rather than propositions such as Socrates is a man one can have expressions in the form "there exists x such that x is Socrates and x is a man" and there exists is a quantifier while x is a variable. This distinguishes it from propositional logic, which does not use quantifiers or relations; in this sense, propositional logic is the foundation of first-order logic.\nA theory about a topic is usually a first-order logic together with a specified domain of discourse over which the quantified variables range, finitely many functions from that domain to itself, finitely many predicates defined on that domain, and a set of axioms believed to hold for those things. Sometimes "theory" is understood in a more formal sense, which is just a set of sentences in first-order logic.\nThe adjective "first-order" distinguishes first-order logic from higher-order logic in which there are predicates having predicates or functions as arguments, or in which one or both of predicate quantifiers or function quantifiers are permitted. In first-order theories, predicates are often associated with sets. In interpreted higher-order theories, predicates may be interpreted as sets of sets.\nThere are many deductive systems for first-order logic which are both sound (all provable statements are true in all models) and complete (all statements which are true in all models are provable). Although the logical consequence relation is only semidecidable, much progress has been made in automated theorem proving in first-order logic. First-order logic also satisfies several metalogical theorems that make it amenable to analysis in proof theory, such as the L\u00f6wenheim\u2013Skolem theorem and the compactness theorem.\nFirst-order logic is the standard for the formalization of mathematics into axioms and is studied in the foundations of mathematics.\nPeano arithmetic and Zermelo\u2013Fraenkel set theory are axiomatizations of number theory and set theory, respectively, into first-order logic.\nNo first-order theory, however, has the strength to uniquely describe  a structure with an infinite domain, such as the natural numbers or the real line. Axiom systems that do fully describe these two structures (that is, categorical axiom systems) can be obtained in stronger logics such as second-order logic.\nThe foundations of first-order logic were developed independently by Gottlob Frege and Charles Sanders Peirce. For a history of first-order logic and how it came to dominate formal logic, see Jos\u00e9 Ferreir\u00f3s (2001).',
  },
  {
    id: 237,
    content:
      "Non-negative matrix factorization (NMF or NNMF), also non-negative matrix approximation is a group of algorithms in multivariate analysis and linear algebra where a matrix V is factorized into (usually) two matrices W and H, with the property that all three matrices have no negative elements. This non-negativity makes the resulting matrices easier to inspect. Also, in applications such as processing of audio spectrograms or muscular activity, non-negativity is inherent to the data being considered. Since the problem is not exactly solvable in general, it is commonly approximated numerically.\nNMF finds applications in such fields as astronomy, computer vision, document clustering, chemometrics, audio signal processing, recommender systems, and bioinformatics.",
  },
  {
    id: 238,
    content:
      'In statistics and image processing, to smooth a data set is to create an approximating function that attempts to capture important patterns in the data, while leaving out noise or other fine-scale structures/rapid phenomena. In smoothing, the data points of a signal are modified so individual points higher than the adjacent points (presumably because of noise) are reduced, and points that are lower than the adjacent points are increased leading to a smoother signal. Smoothing may be used in two important ways that can aid in data analysis (1) by being able to extract more information from the data as long as the assumption of smoothing is reasonable and (2) by being able to provide analyses that are both flexible and robust. Many different algorithms are used in smoothing.\nSmoothing may be distinguished from the related and partially overlapping concept of curve fitting in the following ways:\n\ncurve fitting often involves the use of an explicit function form for the result, whereas the immediate results from smoothing are the "smoothed" values with no later use made of a functional form if there is one;\nthe aim of smoothing is to give a general idea of relatively slow changes of value with little attention paid to the close matching of data values, while curve fitting concentrates on achieving as close a match as possible.\nsmoothing methods often have an associated tuning parameter which is used to control the extent of smoothing. Curve fitting will adjust any number of parameters of the function to obtain the \'best\' fit.However, the terminology used across applications is mixed. For example, use of an interpolating spline fits a smooth curve exactly through the given data points and is sometimes called "smoothing".',
  },
  {
    id: 239,
    content:
      'Neuro-linguistic programming (NLP) is a pseudoscientific approach to communication, personal development, and psychotherapy created by Richard Bandler and John Grinder in California, United States, in the 1970s. NLP\'s creators claim there is a connection between neurological processes (neuro-), language (linguistic) and behavioral patterns learned through experience (programming), and that these can be changed to achieve specific goals in life. Bandler and Grinder also claim that NLP methodology can "model" the skills of exceptional people, allowing anyone to acquire those skills. They claim as well that, often in a single session, NLP can treat problems such as phobias, depression, tic disorders, psychosomatic illnesses, near-sightedness, allergy, common cold, and learning disorders. NLP has been adopted by some hypnotherapists and also by companies that run seminars marketed as leadership training to businesses and government agencies.There is no scientific evidence supporting the claims made by NLP advocates and it has been discredited as a pseudoscience. Scientific reviews state that NLP is based on outdated metaphors of how the brain works that are inconsistent with current neurological theory and contain numerous factual errors. Reviews also found that all of the supportive research on NLP contained significant methodological flaws and that there were three times as many studies of a much higher quality that failed to reproduce the "extraordinary claims" made by Bandler, Grinder, and other NLP practitioners.',
  },
  {
    id: 240,
    content:
      'T-distributed Stochastic Neighbor Embedding (t-SNE) is a machine learning algorithm for visualization developed by Laurens van der Maaten and Geoffrey Hinton. It is a nonlinear dimensionality reduction technique well-suited for embedding high-dimensional data for visualization in a low-dimensional space of two or three dimensions. Specifically, it models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points with high probability.\nThe t-SNE algorithm comprises two main stages. First, t-SNE constructs a probability distribution over pairs of high-dimensional objects in such a way that similar objects have a high probability of being picked while dissimilar points have an extremely small probability of being picked. Second, t-SNE defines a similar probability distribution over the points in the low-dimensional map, and it minimizes the Kullback\u2013Leibler divergence (KL divergence) between the two distributions with respect to the locations of the points in the map. Note that while the original algorithm uses the Euclidean distance between objects as the base of its similarity metric, this should be changed as appropriate.\nt-SNE has been used for visualization in a wide range of applications, including computer security research, music analysis, cancer research, bioinformatics, and biomedical signal processing. It is often used to visualize high-level representations learned by an artificial neural network.While t-SNE plots often seem to display clusters, the visual clusters can be influenced strongly by the chosen parameterization and therefore a good understanding of the parameters for t-SNE is necessary. Such "clusters" can be shown to even appear in non-clustered data, and thus may be false findings. Interactive exploration may thus be necessary to choose parameters and validate results. It has been demonstrated that t-SNE is often able to recover well-separated clusters, and with special parameter choices, approximates a simple form of spectral clustering.\n\n',
  },
  {
    id: 241,
    content:
      'A statistical language model is a probability distribution over sequences of words. Given such a sequence, say of length m, it assigns a probability \n  \n    \n      \n        P\n        (\n        \n          w\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          w\n          \n            m\n          \n        \n        )\n      \n    \n    {\\displaystyle P(w_{1},\\ldots ,w_{m})}\n   to the whole sequence.\nThe language model provides context to distinguish between words and phrases that sound similar. For example, in American English, the phrases "recognize speech" and "wreck a nice beach" sound similar, but mean different things.\nData sparsity is a major problem in building language models. Most possible word sequences are not observed in training. One solution is to make the assumption that the probability of a word only depends on the previous n words. This is known as an n-gram model or unigram model when n = 1. The unigram model is also known as the bag of words model.\nEstimating the relative likelihood of different phrases is useful in many natural language processing applications, especially those that generate text as an output. Language modeling is used in speech recognition, machine translation, part-of-speech tagging, parsing, Optical Character Recognition, handwriting recognition, information retrieval and other applications.\nIn speech recognition, sounds are matched with word sequences. Ambiguities are easier to resolve when evidence from the language model is integrated with a pronunciation model and an acoustic model.\nLanguage models are used in information retrieval in the query likelihood model. There, a separate language model is associated with each document in a collection. Documents are ranked based on the probability of the query Q in the document\'s language model \n  \n    \n      \n        \n          M\n          \n            d\n          \n        \n      \n    \n    {\\displaystyle M_{d}}\n  : \n  \n    \n      \n        P\n        (\n        Q\n        \u2223\n        \n          M\n          \n            d\n          \n        \n        )\n      \n    \n    {\\displaystyle P(Q\\mid M_{d})}\n  . Commonly, the unigram language model is used for this purpose.',
  },
  {
    id: 242,
    content:
      "According to Hotho et al. (2005) we can differ three different perspectives of text mining, namely text mining as information extraction, text mining as text data mining, and text mining as KDD (Knowledge Discovery in Databases) process. Text mining is \"the discovery by computer of new, previously unknown information, by automatically extracting information from different written resources.\" Written resources can be websites, books, emails, reviews, articles.\nText mining, also referred to as text data mining, roughly equivalent to text analytics, is the process of deriving high-quality information from text. High-quality information is typically derived through the devising of patterns and trends through means such as statistical pattern learning. Text mining usually involves the process of structuring the input text (usually parsing, along with the addition of some derived linguistic features and the removal of others, and subsequent insertion into a database), deriving patterns within the structured data, and finally evaluation and interpretation of the output. 'High quality' in text mining usually refers to some combination of relevance, novelty, and interest. Typical text mining tasks include text categorization, text clustering, concept/entity extraction, production of granular taxonomies, sentiment analysis, document summarization, and entity relation modeling (i.e., learning relations between named entities).\nText analysis involves information retrieval, lexical analysis to study word frequency distributions, pattern recognition, tagging/annotation, information extraction, data mining techniques including link and association analysis, visualization, and predictive analytics. The overarching goal is, essentially, to turn text into data for analysis, via application of natural language processing (NLP), different types of algorithms and analytical methods. An important phase of this process is the interpretation of the gathered information.\nA typical application is to scan a set of documents written in a natural language and either model the document set for predictive classification purposes or populate a database or search index with the information extracted.\nThe document is the basic element while starting with text mining. Here, we define a document as a unit of textual data, which normally exists in many types of collections.\n\n",
  },
  {
    id: 243,
    content:
      "A graphical model or probabilistic graphical model (PGM) or structured probabilistic model is a probabilistic model for which a graph expresses the conditional dependence structure between random variables. They are commonly used in probability theory, statistics\u2014particularly Bayesian statistics\u2014and machine learning.",
  },
  {
    id: 244,
    content:
      'Semantic similarity is a metric defined over a set of documents or terms, where the idea of distance between items is based on the likeness of their meaning or semantic content as opposed to  lexicographical similarity. These are mathematical tools used to estimate the strength of the semantic relationship between units of language, concepts or instances, through a numerical description obtained according to the comparison of information supporting their meaning or describing their nature. The term semantic similarity is often confused with semantic relatedness. Semantic relatedness includes any relation between two terms, while semantic similarity only includes "is a" relations.\nFor example, "car" is similar to "bus", but is also related to "road" and "driving".\nComputationally, semantic similarity can be estimated by defining a topological similarity, by using ontologies to define the distance between terms/concepts. For example, a naive metric for the comparison of concepts ordered in a partially ordered set and represented as nodes of a directed acyclic graph (e.g., a taxonomy), would be the shortest-path linking the two concept nodes. Based on text analyses, semantic relatedness between units of language (e.g., words, sentences) can also be estimated using statistical means such as a vector space model to correlate words and textual contexts from a suitable text corpus. The evalutation of the proposed semantic similarity / relatedness measures are evalutaed through two main ways. The former is based on the use of datasets designed by experts and composed of word pairs with semantic similarity / relatedness degree estimation. The second way is based on the integration of the measures inside specific applications such the information retrieval, recommender systems, natural language processing, etc.',
  },
  {
    id: 245,
    content:
      "The noisy channel model is a framework used in spell checkers,\nquestion answering, speech recognition, and machine translation.\nIn this model, the goal is to find the intended word given a word where the\nletters have been scrambled in some manner.",
  },
  {
    id: 246,
    content:
      "PageRank (PR) is an algorithm used by Google Search to rank web pages in their search engine results. PageRank was named after Larry Page, one of the founders of Google. PageRank is a way of measuring the importance of website pages. According to Google: PageRank works by counting the number and quality of links to a page to determine a rough estimate of how important the website is. The underlying assumption is that more important websites are likely to receive more links from other websites. Currently, PageRank is not the only algorithm used by Google to order search results, but it is the first algorithm that was used by the company, and it is the best known. As of September 24, 2019, PageRank and all associated patents are expired.",
  },
  {
    id: 247,
    content:
      "Query expansion (QE) is the process of reformulating a given query to improve retrieval performance in information retrieval operations, particularly in the context of query understanding.\nIn the context of search engines, query expansion involves evaluating a user's input (what words were typed into the search query area, and sometimes other types of data) and expanding the search query to match additional documents.  Query expansion involves techniques such as:\n\nFinding synonyms of words, and searching for the synonyms as well\nFinding semantically related words (e.g. antonyms, meronyms, hyponyms, hypernyms)\nFinding all the various morphological forms of words by stemming each word in the search query\nFixing spelling errors and automatically searching for the corrected form or suggesting it in the results\nRe-weighting the terms in the original queryQuery expansion is a methodology studied in the field of computer science, particularly within the realm of natural language processing and information retrieval.",
  },
  {
    id: 248,
    content:
      "Sentence boundary disambiguation (SBD), also known as sentence breaking, sentence boundary detection, and sentence segmentation, is the problem in natural language processing of deciding where sentences begin and end. Natural language processing tools often require their input to be divided into sentences; however, sentence boundary identification can be challenging due to the potential ambiguity of punctuation marks. In written English, a period may indicate the end of a sentence, or may denote an abbreviation, a decimal point, an ellipsis, or an email address, among other possibilities. About 47% of the periods in the Wall Street Journal corpus denote abbreviations. Question marks and exclamation marks can be similarly ambiguous due to use in emoticons, computer code, and slang.\nLanguages like Japanese and Chinese have unambiguous sentence-ending markers.\n\n",
  },
  {
    id: 249,
    content:
      'Speech synthesis is the artificial production of human speech. A computer system used for this purpose is called a speech computer or speech synthesizer, and can be implemented in software or hardware products. A text-to-speech (TTS) system converts normal language text into speech; other systems render symbolic linguistic representations like phonetic transcriptions into speech.Synthesized speech can be created by concatenating pieces of recorded speech that are stored in a database. Systems differ in the size of the stored speech units; a system that stores phones or diphones provides the largest output range, but may lack clarity. For specific usage domains, the storage of entire words or sentences allows for high-quality output. Alternatively, a synthesizer can incorporate a model of the vocal tract and other human voice characteristics to create a completely "synthetic" voice output.The quality of a speech synthesizer is judged by its similarity to the human voice and by its ability to be understood clearly. An intelligible text-to-speech program allows people with visual impairments or reading disabilities to listen to written words on a home computer. Many computer operating systems have included speech synthesizers since the early 1990s.\n\nA text-to-speech system (or "engine") is composed of two parts: a front-end and a back-end. The front-end has two major tasks. First, it converts raw text containing symbols like numbers and abbreviations into the equivalent of written-out words. This process is often called text normalization, pre-processing, or tokenization. The front-end then assigns phonetic transcriptions to each word, and divides and marks the text into prosodic units, like phrases, clauses, and sentences. The process of assigning phonetic transcriptions to words is called text-to-phoneme or grapheme-to-phoneme conversion. Phonetic transcriptions and prosody information together make up the symbolic linguistic representation that is output by the front-end. The back-end\u2014often referred to as the synthesizer\u2014then converts the symbolic linguistic representation into sound. In certain systems, this part includes the computation of the target prosody (pitch contour, phoneme durations), which is then imposed on the output speech.',
  },
  {
    id: 250,
    content:
      "Dynamic programming is both a mathematical optimization method and a computer programming method. The method was developed by Richard Bellman in the 1950s and has found applications in numerous fields, from aerospace engineering to economics. In both contexts it refers to simplifying a complicated problem by breaking it down into simpler sub-problems in a recursive manner. While some decision problems cannot be taken apart this way, decisions that span several points in time do often break apart recursively. Likewise, in computer science, if a problem can be solved optimally by breaking it into sub-problems and then recursively finding the optimal solutions to the sub-problems, then it is said to have optimal substructure.\nIf sub-problems can be nested recursively inside larger problems, so that dynamic programming methods are applicable, then there is a relation between the value of the larger problem and the values of the sub-problems. In the optimization literature this relationship is called the Bellman equation.",
  },
  {
    id: 251,
    content:
      "IBM alignment models are a sequence of increasingly complex models used in statistical machine translation to train a translation model and an alignment model, starting with lexical translation probabilities and moving to reordering and word duplication. They underpinned the majority of statistical machine translation systems for almost twenty years starting in the early 1990s, until neural machine translation began to dominate. These models offer principled probabilistic formulation and (mostly) tractable inference.The original work on statistical machine translation at IBM proposed five models, and a model 6 was proposed later. The sequence of the six models can be summarized as:\n\nModel 1: lexical translation\nModel 2: additional absolute alignment model\nModel 3: extra fertility model\nModel 4: added relative alignment model\nModel 5: fixed deficiency problem.\nModel 6: Model 4 combined with a HMM alignment model in a log linear way",
  },
  {
    id: 252,
    content:
      "In corpus linguistics, part-of-speech tagging (POS tagging or PoS tagging or POST), also called grammatical tagging or word-category disambiguation, is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech, based on both its definition and its context\u2014i.e., its relationship with adjacent and related words in a phrase, sentence, or paragraph.\nA simplified form of this is commonly taught to school-age children, in the identification of words as nouns, verbs, adjectives, adverbs, etc.\nOnce performed by hand, POS tagging is now done in the context of computational linguistics, using algorithms which associate discrete terms, as well as hidden parts of speech, by a set of descriptive tags. POS-tagging algorithms fall into two distinctive groups: rule-based and stochastic. E. Brill's tagger, one of the first and most widely used English POS-taggers, employs rule-based algorithms.\n\n",
  },
  {
    id: 253,
    content:
      "Linear algebra is the branch of mathematics concerning linear equations such as \n\n  \n    \n      \n        \n          a\n          \n            1\n          \n        \n        \n          x\n          \n            1\n          \n        \n        +\n        \u22ef\n        +\n        \n          a\n          \n            n\n          \n        \n        \n          x\n          \n            n\n          \n        \n        =\n        b\n        ,\n      \n    \n    {\\displaystyle a_{1}x_{1}+\\cdots +a_{n}x_{n}=b,}\n  linear functions such as\n\n  \n    \n      \n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            n\n          \n        \n        )\n        \u21a6\n        \n          a\n          \n            1\n          \n        \n        \n          x\n          \n            1\n          \n        \n        +\n        \u2026\n        +\n        \n          a\n          \n            n\n          \n        \n        \n          x\n          \n            n\n          \n        \n        ,\n      \n    \n    {\\displaystyle (x_{1},\\ldots ,x_{n})\\mapsto a_{1}x_{1}+\\ldots +a_{n}x_{n},}\n  and their representations in vector spaces and through matrices.Linear algebra is central to almost all areas of mathematics. For instance, linear algebra is fundamental in modern presentations of geometry, including for defining basic objects such as lines, planes and rotations. Also, functional analysis may be basically viewed as the application of linear algebra to spaces of functions. Linear algebra is also used in most sciences and engineering areas, because it allows modeling many natural phenomena, and efficiently computing with such models. For nonlinear systems, which cannot be modeled with linear algebra, linear algebra is often used for dealing with first-order approximations, using the fact that the differential of a multivariate function at a point is the linear map that best approximates the function near that point.",
  },
  {
    id: 254,
    content:
      "Semantic parsing is the task of converting a natural language utterance to a logical form: a machine-understandable representation of its meaning. Semantic parsing can thus be understood as extracting the precise meaning of an utterance. Applications of semantic parsing include machine translation, question answering, ontology induction, automated reasoning, and code generation. The phrase was first used in the 1970s by Yorick Wilks as the basis for machine translation programs working with only semantic representations.",
  },
  {
    id: 255,
    content:
      "A self-driving car, also known as an autonomous vehicle (AV), connected and autonomous vehicle (CAV), driverless car, robo-car, or robotic car, is a vehicle that is capable of sensing its environment and moving safely with little or no human input.Self-driving cars combine a variety of sensors to perceive their surroundings, such as radar, lidar, sonar, GPS, odometry and inertial measurement units. Advanced control systems interpret sensory information to identify appropriate navigation paths, as well as obstacles and relevant signage.Long-distance trucking is seen as being at the forefront of adopting and implementing the technology.\n\n",
  },
  { id: 256, content: "" },
  {
    id: 257,
    content:
      "In computational linguistics, word-sense disambiguation (WSD) is an open problem concerned with identifying which sense of a word is used in a sentence. The solution to this problem impacts other computer-related writing, such as discourse, improving relevance of search engines, anaphora resolution, coherence, and inference.\nThe human brain is quite proficient at word-sense disambiguation. That natural language is formed in a way that requires so much of it is a reflection of that neurologic reality. In other words, human language developed in a way that reflects (and also has helped to shape) the innate ability provided by the brain's neural networks. In computer science and the information technology that it enables, it has been a long-term challenge to develop the ability in computers to do natural language processing and machine learning.\nA rich variety of techniques have been researched, from dictionary-based methods that use the knowledge encoded in lexical resources, to supervised machine learning methods in which a classifier is trained for each distinct word on a corpus of manually sense-annotated examples, to completely unsupervised methods that cluster occurrences of words, thereby inducing word senses. Among these, supervised learning approaches have been the most successful algorithms to date.\nAccuracy of current algorithms is difficult to state without a host of caveats. In English, accuracy at the coarse-grained (homograph) level is routinely above 90%, with some methods on particular homographs achieving over 96%. On finer-grained sense distinctions, top accuracies from 59.1% to 69.0% have been reported in evaluation exercises (SemEval-2007, Senseval-2), where the baseline accuracy of the simplest possible algorithm of always choosing the most frequent sense was 51.4% and 57%, respectively.",
  },
  {
    id: 258,
    content:
      'Document classification or document categorization is a problem in library science, information science and computer science. The task is to assign a document to one or more classes or categories. This may be done "manually" (or "intellectually") or algorithmically. The intellectual classification of documents has mostly been the province of library science, while the algorithmic classification of documents is mainly in information science and computer science. The problems are overlapping, however, and there is therefore interdisciplinary research on document classification.\nThe documents to be classified may be texts, images, music, etc. Each kind of document possesses its special classification problems. When not otherwise specified, text classification is implied.\nDocuments may be classified according to their subjects or according to other attributes (such as document type, author, printing year etc.). In the rest of this article only subject classification is considered. There are two main philosophies of subject classification of documents: the content-based approach and the request-based approach.',
  },
  {
    id: 259,
    content:
      'In natural language processing, semantic role labeling (also called shallow semantic parsing or slot-filling) is the process that assigns labels to words or phrases in a sentence that indicate their semantic role in the sentence, such as that of an agent, goal, or result.It consists of the detection of the semantic arguments associated with the predicate or verb of a sentence and their classification into their specific roles. For example, given a sentence like "Mary sold the book to John", the task would be to recognize the verb "to sell" as representing the predicate, "Mary" as representing the seller (agent), "the book" as representing the goods (theme), and "John" as representing the recipient. This is an important step towards making sense of the meaning of a sentence. A semantic analysis of this sort is at a lower-level of abstraction than a syntax tree, i.e. it has more categories, thus groups fewer clauses in each category. For instance, "the book belongs to me" would need two labels such as "possessed" and "possessor" whereas "the book was sold to John" would need two other labels such as "goal" (or "theme") and "receiver" (or "recipient") even though these two clauses would be very similar as far as "subject" and "object" functions are concerned.',
  },
  {
    id: 260,
    content:
      'Supervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs. It infers a function from labeled training data consisting of a set of training examples.  In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal).  A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a "reasonable" way (see inductive bias).\nThe parallel task in human and animal psychology is often referred to as concept learning.',
  },
  {
    id: 261,
    content:
      "In information theory, the cross entropy between two probability distributions \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   and \n  \n    \n      \n        q\n      \n    \n    {\\displaystyle q}\n   over the same underlying set of events measures the average number of bits needed to identify an event drawn from the set if a coding scheme used for the set is optimized for an estimated probability distribution \n  \n    \n      \n        q\n      \n    \n    {\\displaystyle q}\n  , rather than the true distribution \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  .",
  },
  {
    id: 262,
    content:
      "A finite-state machine (FSM) or finite-state automaton (FSA, plural: automata), finite automaton, or simply a state machine, is a mathematical model of computation. It is an abstract machine that can be in exactly one of a finite number of states at any given time. The FSM can change from one state to another in response to some inputs; the change from one state to another is called a transition. An FSM is defined by a list of its states, its initial state, and the inputs that trigger each transition. Finite-state machines are of two types\u2014deterministic finite-state machines and non-deterministic finite-state machines. A deterministic finite-state machine can be constructed equivalent to any non-deterministic one.\nThe behavior of state machines can be observed in many devices in modern society that perform a predetermined sequence of actions depending on a sequence of events with which they are presented. Simple examples are vending machines, which dispense products when the proper combination of coins is deposited, elevators, whose sequence of stops is determined by the floors requested by riders, traffic lights, which change sequence when cars are waiting, and combination locks, which require the input of a sequence of numbers in the proper order.\nThe finite-state machine has less computational power than some other models of computation such as the Turing machine. The computational power distinction means there are computational tasks that a Turing machine can do but a FSM cannot. This is because a FSM's memory is limited by the number of states it has. FSMs are studied in the more general field of automata theory.",
  },
  {
    id: 263,
    content:
      "In formal language theory, computer science and linguistics, the Chomsky hierarchy (occasionally referred to as the Chomsky\u2013Sch\u00fctzenberger hierarchy) is a containment hierarchy of classes of formal grammars.\nThis hierarchy of grammars was described by Noam Chomsky in 1956. It is also named after Marcel-Paul Sch\u00fctzenberger, who played a crucial role in the development of the theory of formal languages.",
  },
  {
    id: 264,
    content:
      "Named-entity recognition (NER) (also known as entity identification, entity chunking and entity extraction) is a subtask of information extraction that seeks to locate and classify named entity mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc.\nMost research on NER systems has been structured as taking an unannotated block of text, such as this one:\n\nJim bought 300 shares of Acme Corp. in 2006.\nAnd producing an annotated block of text that highlights the names of entities:\n\n[Jim]Person bought 300 shares of [Acme Corp.]Organization in [2006]Time.\nIn this example, a person name consisting of one token, a two-token company name and a temporal expression have been detected and classified.\nState-of-the-art NER systems for English produce near-human performance. For example, the best system entering MUC-7 scored 93.39% of F-measure while human annotators scored 97.60% and 96.95%.",
  },
  {
    id: 265,
    content:
      'A parse tree or parsing tree or derivation tree or concrete syntax tree is an ordered, rooted tree that represents the syntactic structure of a string according to some context-free grammar. The term parse tree itself is used primarily in computational linguistics; in theoretical syntax, the term syntax tree is more common.\nParse trees concretely reflect the syntax of the input language, making them distinct from the abstract syntax trees used in computer programming. Unlike Reed-Kellogg sentence diagrams used for teaching grammar, parse trees do not use distinct symbol shapes for different types of constituents.\nParse trees are usually constructed based on either the constituency relation of constituency grammars (phrase structure grammars) or the dependency relation of dependency grammars. Parse trees may be generated for sentences in natural languages (see natural language processing), as well as during processing of computer languages, such as programming languages.A related concept is that of phrase marker or P-marker, as used in transformational generative grammar. A phrase marker is a linguistic expression marked as to its phrase structure. This may be presented in the form of a tree, or as a bracketed expression. Phrase markers are generated by applying phrase structure rules, and themselves are subject to further transformational rules. A set of possible parse trees for a syntactically ambiguous sentence is called a "parse forest."',
  },
  {
    id: 266,
    content:
      "Signal processing is an electrical engineering subfield that focuses on analysing, modifying, and synthesizing signals such as sound, images, and scientific measurements. Signal processing techniques can be used to improve transmission, storage efficiency and subjective quality and to also emphasize or detect components of interest in a measured signal.\n\n",
  },
  { id: 267, content: "" },
  {
    id: 268,
    content:
      "A grammar checker, in computing terms, is a program, or part of a program, that attempts to verify written text for grammatical correctness. Grammar checkers are most often implemented as a feature of a larger program, such as a word processor, but are also available as a stand-alone application that can be activated from within programs that work with editable text.\nThe implementation of a grammar checker makes use of natural language processing.",
  },
  {
    id: 269,
    content:
      'Information extraction (IE), information retrieval (IR) is the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents and other electronically represented sources. In most of the cases this activity concerns processing human language texts by means of natural language processing (NLP). Recent activities in multimedia document processing like automatic annotation and content extraction out of images/audio/video/documents could be seen as information extraction\nDue to the difficulty of the problem, current approaches to IE (IR) focus on narrowly restricted domains.  An example is the extraction from newswire reports of corporate mergers, such as denoted by the formal relation: \n\n  \n    \n      \n        \n          M\n          e\n          r\n          g\n          e\n          r\n          B\n          e\n          t\n          w\n          e\n          e\n          n\n        \n        (\n        c\n        o\n        m\n        p\n        a\n        n\n        \n          y\n          \n            1\n          \n        \n        ,\n        c\n        o\n        m\n        p\n        a\n        n\n        \n          y\n          \n            2\n          \n        \n        ,\n        d\n        a\n        t\n        e\n        )\n      \n    \n    {\\displaystyle \\mathrm {MergerBetween} (company_{1},company_{2},date)}\n  ,from an online news sentence such as:\n\n"Yesterday, New York based Foo Inc. announced their acquisition of Bar Corp."A broad goal of IE is to allow computation to be done on the previously unstructured data.  A more specific goal is to allow logical reasoning to draw inferences based on the logical content of the input data.  Structured data is semantically well-defined data from a chosen target domain, interpreted with respect to category and context.\nInformation Extraction is the part of a greater puzzle which deals with the problem of devising automatic methods for text management, beyond its transmission, storage and display. The discipline of information retrieval (IR) has developed automatic methods, typically of a statistical flavor, for indexing large document collections and classifying documents. Another complementary approach is that of natural language processing (NLP) which has solved the problem of modelling human language processing with considerable success when taking into account the magnitude of the task. In terms of both difficulty and emphasis,  IE deals with tasks in between both IR and NLP. In terms of input, IE assumes the existence of a set of documents in which each document follows a template, i.e. describes one or more entities or events  in a manner that is similar to those in other documents but differing in the details. An example, consider a group of newswire articles on Latin American terrorism with each article presumed to be based upon one or more terroristic acts. We also define for any given IE task a template, which is a(or a set of) case frame(s) to hold the information contained in a single document. For the terrorism example, a template would have slots corresponding to the perpetrator, victim, and weapon of the terroristic act, and the date on which the event happened. An IE system for this problem is required to \u201cunderstand\u201d an attack article only enough to find data corresponding to the slots in this template.\n\n',
  },
  {
    id: 270,
    content:
      "Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics, pattern recognition, and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification.\nLDA is closely related to analysis of variance (ANOVA) and regression analysis, which also attempt to express one dependent variable as a linear combination of other features or measurements. However, ANOVA uses categorical independent variables and a continuous dependent variable, whereas discriminant analysis has continuous independent variables and a categorical dependent variable (i.e. the class label). Logistic regression and probit regression are more similar to LDA than ANOVA is, as they also explain a categorical variable by the values of continuous independent variables. These other methods are preferable in applications where it is not reasonable to assume that the independent variables are normally distributed, which is a fundamental assumption of the LDA method.\nLDA is also closely related to principal component analysis (PCA) and factor analysis in that they both look for linear combinations of variables which best explain the data. LDA explicitly attempts to model the difference between the classes of data. PCA, in contrast, does not take into account any difference in class, and factor analysis builds the feature combinations based on differences rather than similarities. Discriminant analysis is also different from factor analysis in that it is not an interdependence technique: a distinction between independent variables and dependent variables (also called criterion variables) must be made.\nLDA works when the measurements made on independent variables for each observation are continuous quantities. When dealing with categorical independent variables, the equivalent technique is discriminant correspondence analysis.Discriminant analysis is used when groups are known a priori (unlike in cluster analysis). Each case must have a score on one or more quantitative predictor measures, and a score on a group measure. In simple terms, discriminant function analysis is classification - the act of distributing things into groups, classes or categories of the same type.",
  },
  {
    id: 271,
    content:
      "In computer science, a search algorithm is any algorithm which solves the search problem, namely, to retrieve information stored within some data structure, or calculated in the search space of a problem domain, either with discrete or continuous values. Specific applications of search algorithms include:\n\nProblems in combinatorial optimization, such as:\nThe vehicle routing problem, a form of shortest path problem\nThe knapsack problem: Given a set of items, each with a weight and a value, determine the number of each item to include in a collection so that the total weight is less than or equal to a given limit and the total value is as large as possible.\nThe nurse scheduling problem\nProblems in constraint satisfaction, such as:\nThe map coloring problem\nFilling in a sudoku or crossword puzzle\nIn game theory and especially combinatorial game theory, choosing the best move to make next (such as with the minmax algorithm)\nFinding a combination or password from the whole set of possibilities\nFactoring an integer (an important problem in cryptography)\nOptimizing an industrial process, such as a chemical reaction, by changing the parameters of the process (like temperature, pressure, and pH)\nRetrieving a record from a database\nFinding the maximum or minimum value in a list or array\nChecking to see if a given value is present in a set of valuesThe classic search problems described above and web search are both problems in information retrieval, but are generally studied as separate subfields and are solved and evaluated differently. Web search problems are generally focused on filtering and finding documents that are most relevant to human queries. Classic search algorithms are typically evaluated on how fast they can find a solution, and whether that solution is guaranteed to be optimal. Though information retrieval algorithms must be fast, the quality of ranking is more important, as is whether good results have been left out and bad results included.\nThe appropriate search algorithm often depends on the data structure being searched, and may also include prior knowledge about the data. Some database structures are specially constructed to make search algorithms faster or more efficient, such as a search tree, hash map, or a database index. Search algorithms can be classified based on their mechanism of searching. Linear search algorithms check every record for the one associated with a target key in a linear fashion. Binary, or half interval searches, repeatedly target the center of the search structure and divide the search space in half. Comparison search algorithms improve on linear searching by successively eliminating records based on comparisons of the keys until the target record is found, and can be applied on data structures with a defined order. Digital search algorithms work based on the properties of digits in data structures that use numerical keys. Finally, hashing directly maps keys to records based on a hash function. Searches outside a linear search require that the data be sorted in some way.\nAlgorithms are often evaluated by their computational complexity, or maximum theoretical run time. Binary search functions, for example, have a maximum complexity of O(log n), or logarithmic time. This means that the maximum number of operations needed to find the search target is a logarithmic function of the size of the search space.",
  },
  {
    id: 272,
    content:
      "Propositional calculus is a branch of logic.  It is also called propositional logic, statement logic, sentential calculus, sentential logic, or sometimes zeroth-order logic. It deals with propositions (which can be true or false) and argument flow. Compound propositions are formed by connecting propositions by logical connectives. The propositions without logical connectives are called atomic propositions.\nUnlike first-order logic, propositional logic does not deal with non-logical objects, predicates about them, or quantifiers. However, all the machinery of propositional logic is included in first-order logic and higher-order logics. In this sense, propositional logic is the foundation of first-order logic and higher-order logic.",
  },
  { id: 273, content: "" },
  {
    id: 274,
    content:
      'Parsing, syntax analysis, or syntactic analysis is the process of analyzing a string of symbols, either in natural language, computer languages or data structures, conforming to the rules of a formal grammar.  The term parsing comes from Latin pars (orationis), meaning part (of speech).The term has slightly different meanings in different branches of linguistics and computer science.  Traditional sentence parsing is often performed as a method of understanding the exact meaning of a sentence or word, sometimes with the aid of devices such as sentence diagrams.  It usually emphasizes the importance of grammatical divisions such as subject and predicate.\nWithin computational linguistics the term is used to refer to the formal analysis by a computer of a sentence or other string of words into its constituents, resulting in a parse tree showing their syntactic relation to each other, which may also contain semantic and other information. Some parsing algorithms may generate a parse forest or list of parse trees for a syntactically ambiguous input.The term is also used in psycholinguistics when describing language comprehension.  In this context, parsing refers to the way that human beings analyze a sentence or phrase (in spoken language or text) "in terms of grammatical constituents, identifying the parts of speech, syntactic relations, etc."  This term is especially common when discussing what linguistic cues help speakers to interpret garden-path sentences.\nWithin computer science, the term is used in the analysis of computer languages, referring to the syntactic analysis of the input code into its component parts in order to facilitate the writing of compilers and interpreters. The term may also be used to describe a split or separation.',
  },
  {
    id: 275,
    content:
      "An image retrieval system is a computer system for browsing, searching and retrieving images from a large database of digital images. Most traditional and common methods of image retrieval utilize some method of adding metadata such as captioning, keywords, title or descriptions to the images so that retrieval can be performed over the annotation words. Manual image annotation is time-consuming, laborious and expensive; to address this, there has been a large amount of research done on automatic image annotation. Additionally, the increase in social web applications and the semantic web have inspired the development of several web-based image annotation tools. \nThe first microcomputer-based image database retrieval system was developed at MIT, in the 1990s, by Banireddy Prasaad, Amar Gupta, Hoo-min Toong, and Stuart Madnick.A 2008 survey article documented progresses after 2007.",
  },
  {
    id: 276,
    content:
      'In mathematical optimization and decision theory, a loss function or cost function is a function that maps an event or values of one or more variables onto a real number intuitively representing some "cost" associated with the event. An optimization problem seeks to minimize a loss function. An objective function is either a loss function or its negative (in specific domains, variously called a reward function, a profit function, a utility function, a fitness function, etc.), in which case it is to be maximized.\nIn statistics, typically a  loss function is used for parameter estimation, and the event in question is some function of the difference between estimated and true values for an instance of data. The concept, as old as Laplace, was reintroduced in statistics by Abraham Wald in the middle of the 20th century.  In the context of economics, for example, this is usually economic cost or regret.  In classification, it is the penalty for an incorrect classification of an example. In actuarial science, it is used in an insurance context to model benefits paid over premiums, particularly since the works of Harald Cram\u00e9r in the 1920s. In optimal control, the loss is the penalty for failing to achieve a desired value. In financial risk management, the function is mapped to a monetary loss.\nIn classical statistics (both frequentist and Bayesian), a loss function is typically treated as something of a background mathematical convention. Critics such as W. Edwards Deming and Nassim Nicholas Taleb have argued that loss functions require much greater attention than they have traditionally been given and that loss functions used in real world decision making need to reflect actual empirical experience. They argue that real-world loss functions are often very different from the smooth, symmetric ones used by classical convention, and are often highly asymmetric, nonlinear, and discontinuous.',
  },
  {
    id: 277,
    content:
      "In machine learning, feature learning or representation learning is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data. This replaces manual feature engineering and allows a machine to both learn the features  and use them to perform  a specific task.\nFeature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensor data has not yielded to attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms.\nFeature learning can be either supervised or unsupervised.\n\nIn supervised feature learning, features are learned using labeled input data. Examples include supervised neural networks, multilayer perceptron and (supervised) dictionary learning.\nIn unsupervised feature learning, features are learned with unlabeled input data.  Examples include dictionary learning, independent component analysis, autoencoders, matrix factorization and various forms of clustering.",
  },
  {
    id: 278,
    content:
      "John Thomas Grinder, Jr. ( GRIN-d\u0259r; born January 10, 1940) is an American linguist, author, management consultant, trainer and speaker. Grinder is credited with co-creating Neuro-linguistic programming, with Richard Bandler. He is co-director of Quantum Leap Inc., a management consulting firm founded by his partner Carmen Bostic St. Clair in 1987 (Grinder joined in 1989). Grinder and Bostic St. Clair also run workshops and seminars on NLP internationally.",
  },
  {
    id: 279,
    content:
      "Reinforcement learning (RL) is an area of machine learning concerned with how software agents ought to take actions in an environment in order to maximize the notion of cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning.\nReinforcement learning differs from supervised learning in not needing labelled input/output pairs be presented, and in not needing sub-optimal actions to be explicitly corrected. Instead the focus is on finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge).\nThe environment is typically stated in the form of a Markov decision process (MDP), because many reinforcement learning algorithms for this context utilize dynamic programming techniques. The main difference between the classical dynamic programming methods  and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the MDP and they target large MDPs where exact methods become infeasible.",
  },
  {
    id: 280,
    content:
      'In computer science, artificial intelligence (AI), sometimes called machine intelligence, is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and animals. Leading AI textbooks define the field as the study of "intelligent agents": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals. Colloquially, the term "artificial intelligence" is often used to describe machines (or computers) that mimic "cognitive" functions that humans associate with the human mind, such as "learning" and "problem solving".As machines become increasingly capable, tasks considered to require "intelligence" are often removed from the definition of AI, a phenomenon known as the AI effect. A quip in Tesler\'s Theorem says "AI is whatever hasn\'t been done yet." For instance, optical character recognition is frequently excluded from things considered to be AI, having become a routine technology. Modern machine capabilities generally classified as AI include successfully understanding human speech, competing at the highest level in strategic game systems (such as chess and Go), autonomously operating cars, intelligent routing in content delivery networks, and military simulations.\nArtificial intelligence was founded as an academic discipline in 1955, and in the years since has experienced several waves of optimism, followed by disappointment and the loss of funding (known as an "AI winter"), followed by new approaches, success and renewed funding. For most of its history, AI research has been divided into sub-fields that often fail to communicate with each other. These sub-fields are based on technical considerations, such as particular goals (e.g. "robotics" or "machine learning"), the use of particular tools ("logic" or artificial neural networks), or deep philosophical differences. Sub-fields have also been based on social factors (particular institutions or the work of particular researchers).The traditional problems (or goals) of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception and the ability to move and manipulate objects. General intelligence is among the field\'s long-term goals. Approaches include statistical methods, computational intelligence, and traditional symbolic AI. Many tools are used in AI, including versions of search and mathematical optimization, artificial neural networks, and methods based on statistics, probability and economics. The AI field draws upon computer science, information engineering, mathematics, psychology, linguistics, philosophy, and many other fields.\nThe field was founded on the assumption that human intelligence "can be so precisely described that a machine can be made to simulate it". This raises philosophical arguments about the nature of the mind and the ethics of creating artificial beings endowed with human-like intelligence. These issues have been explored by myth, fiction and philosophy since antiquity. Some people also consider AI to be a danger to humanity if it progresses unabated. Others believe that AI, unlike previous technological revolutions, will create a risk of mass unemployment.In the twenty-first century, AI techniques have experienced a resurgence following concurrent advances in computer power, large amounts of data, and theoretical understanding; and AI techniques have become an essential part of the technology industry, helping to solve many challenging problems in computer science, software engineering and operations research.',
  },
  {
    id: 281,
    content:
      "Latent semantic analysis (LSA) is a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms.  LSA assumes that words that are close in meaning will occur in similar pieces of text (the distributional hypothesis).  A matrix containing word counts per document (rows represent unique words and columns represent each document) is constructed from a large piece of text and a mathematical technique called singular value decomposition (SVD) is used to reduce the number of rows while preserving the similarity structure among columns.  Documents are then compared by taking the cosine of the angle between the two vectors (or the dot product between the normalizations of the two vectors) formed by any two columns.  Values close to 1 represent very similar documents while values close to 0 represent very dissimilar documents.An information retrieval technique using latent semantic structure was patented in 1988 (US Patent 4,839,853, now expired) by Scott Deerwester, Susan Dumais, George Furnas, Richard Harshman, Thomas Landauer, Karen Lochbaum and Lynn Streeter. In the context of its application to information retrieval, it is sometimes called latent semantic indexing (LSI).",
  },
  {
    id: 282,
    content:
      "A shift-reduce parser is a class of efficient, table-driven bottom-up parsing methods for computer languages and other notations formally defined by a grammar.  The parsing methods most commonly used for parsing programming languages, LR parsing and its variations, are shift-reduce methods.  The precedence parsers used before the invention of LR parsing are also shift-reduce methods.  All shift-reduce parsers have similar outward effects, in the incremental order in which they build a parse tree or call specific output actions.",
  },
  {
    id: 283,
    content:
      "Attachment theory is a psychological, evolutionary and ethological theory concerning relationships between humans. The most important tenet is that young children need to develop a relationship with at least one primary caregiver for normal social and emotional development. The theory was formulated by psychiatrist and psychoanalyst John Bowlby.Within attachment theory, infant behaviour associated with attachment is primarily the seeking of proximity to an attachment figure in stressful situations. Infants become attached to adults who are sensitive and responsive in social interactions with them, and who remain as consistent caregivers for some months during the period from about six months to two years of age. During the latter part of this period, children begin to use attachment figures (familiar people) as a secure base to explore from and return to. Parental responses lead to the development of patterns of attachment; these, in turn, lead to internal working models which will guide the individual's feelings, thoughts and expectations in later relationships. Separation anxiety or grief following the loss of an attachment figure is considered to be a normal and adaptive response for an attached infant. These behaviours may have evolved because they increase the probability of survival of the child.Research by developmental psychologist Mary Ainsworth in the 1960s and 70s underpinned the basic concepts, introduced the concept of the \"secure base\" and developed a theory of a number of attachment patterns in infants: secure attachment, avoidant attachment and anxious attachment. A fourth pattern, disorganised attachment, was identified later. In the 1980s, the theory was extended to attachments in adults. Other interactions may be construed as including components of attachment behaviour; these include peer relationships at all ages, romantic and sexual attraction and responses to the care needs of infants or the sick and elderly.\nTo formulate a comprehensive theory of the nature of early attachments, Bowlby explored a range of fields, including evolutionary biology, object relations theory (a tenet of psychoanalysis), control systems theory, and the fields of ethology and cognitive psychology. After preliminary papers from 1958 onwards, Bowlby published the full theory in the trilogy Attachment and Loss (1969\u201382). In the early days of the theory, academic psychologists criticized Bowlby, and the psychoanalytic community ostracised him for his departure from psychoanalytical tenets; however, attachment theory has since become the dominant approach to understanding early social development, and has given rise to a great surge of empirical research into the formation of children's close relationships. Later criticisms of attachment theory relate to temperament, the complexity of social relationships, and the limitations of discrete patterns for classifications. Attachment theory has been significantly modified as a result of empirical research, but the concepts have become generally accepted. Attachment theory has formed the basis of new therapies and informed existing ones, and its concepts have been used in the formulation of social and childcare policies to support the early attachment relationships of children.\n\n",
  },
  {
    id: 284,
    content:
      'A multi-agent system (MAS or "self-organized system") is a computerized system composed of multiple interacting intelligent agents. Multi-agent systems can solve problems that are difficult or impossible for an individual agent or a monolithic system to solve. Intelligence may include methodic, functional, procedural approaches, algorithmic search or reinforcement learning.Despite considerable overlap, a multi-agent system is not always the same as an agent-based model (ABM).  The goal of an ABM is to search for explanatory insight into the collective behavior of agents (which don\'t necessarily need to be "intelligent") obeying simple rules, typically in natural systems, rather than in solving specific practical or engineering problems. The terminology of ABM tends to be used more often in the science, and MAS in engineering and technology. Applications where multi-agent systems research may deliver an appropriate approach include online trading, disaster response and social structure modelling.\n\n',
  },
  {
    id: 285,
    content:
      "In machine learning, support-vector machines (SVMs, also support-vector networks) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis.  Given a set of training examples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on the side of the gap on which they fall.\nIn addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.\nWhen data are unlabelled, supervised learning is not possible, and an unsupervised learning approach is required, which attempts to find natural clustering of the data to groups, and then map new data to these formed groups. The support-vector clustering algorithm, created by Hava Siegelmann and Vladimir Vapnik, applies the statistics of support vectors, developed in the support vector machines algorithm, to categorize unlabeled data, and is one of the most widely used clustering algorithms in industrial applications.",
  },
  {
    id: 286,
    content:
      'In statistics, linear regression is a linear approach to modeling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables). The case of one explanatory variable is called simple linear regression. For more than one explanatory variable, the process is called multiple linear regression. This term is distinct from multivariate linear regression, where multiple correlated dependent variables are predicted, rather than a single scalar variable.In linear regression, the relationships are modeled using linear predictor functions whose unknown model parameters are estimated from the data. Such models are called linear models. Most commonly, the conditional mean of the response given the values of the explanatory variables (or predictors) is assumed to be an affine function of those values; less commonly, the conditional median or some other quantile is used. Like all forms of regression analysis, linear regression focuses on the conditional probability distribution of the response given the values of the predictors, rather than on the joint probability distribution of all of these variables, which is the domain of multivariate analysis.\nLinear regression was the first type of regression analysis to be studied rigorously, and to be used extensively in practical applications. This is because models which depend linearly on their unknown parameters are easier to fit than models which are non-linearly related to their parameters and because the statistical properties of the resulting estimators are easier to determine.\nLinear regression has many practical uses. Most applications fall into one of the following two broad categories:\n\nIf the goal is prediction, forecasting, or error reduction, linear regression can be used to fit a predictive model to an observed data set of values of the response and explanatory variables. After developing such a model, if additional values of the explanatory variables are collected without an accompanying response value, the fitted model can be used to make a prediction of the response.\nIf the goal is to explain variation in the response variable that can be attributed to variation in the explanatory variables, linear regression analysis can be applied to quantify the strength of the relationship between the response and the explanatory variables, and in particular to determine whether some explanatory variables may have no linear relationship with the response at all, or to identify which subsets of explanatory variables may contain redundant information about the response.Linear regression models are often fitted using the least squares approach, but they may also be fitted in other ways, such as by minimizing the "lack of fit" in some other norm (as with least absolute deviations regression), or by minimizing a penalized version of the least squares cost function as in ridge regression (L2-norm penalty) and lasso (L1-norm penalty). Conversely, the least squares approach can be used to fit models that are not linear models. Thus, although the terms "least squares" and "linear model" are closely linked, they are not synonymous.',
  },
  {
    id: 287,
    content:
      "Knowledge representation and reasoning (KR\u00b2, KR&R) is the field of artificial intelligence (AI) dedicated to representing information about the world in a form that a computer system can utilize to solve complex tasks such as diagnosing a medical condition or having a dialog in a natural language. Knowledge representation incorporates findings from psychology about how humans solve problems and represent knowledge in order to design formalisms that will make complex systems easier to design and build.  Knowledge representation and reasoning also incorporates findings from logic to automate various kinds of reasoning, such as the application of rules or the relations of sets and subsets.\nExamples of knowledge representation formalisms include semantic nets, systems architecture, frames, rules, and ontologies. Examples of automated reasoning engines include inference engines, theorem provers, and classifiers.\n\n",
  },
  {
    id: 288,
    content:
      "In statistics, an expectation\u2013maximization (EM) algorithm is an iterative method to find maximum likelihood or maximum a posteriori (MAP) estimates of parameters in statistical models, where the model depends on unobserved latent variables. The EM iteration alternates between performing an expectation (E) step, which creates a function for the expectation of the log-likelihood evaluated using the current estimate for the parameters, and a maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the E step. These parameter-estimates are then used to determine the distribution of the latent variables in the next E step.",
  },
  {
    id: 289,
    content:
      "Logical consequence (also entailment) is a fundamental concept in logic, which describes the relationship between statements that hold true when one statement logically follows from one or more statements. A valid logical argument is one in which the conclusion is entailed by the premises, because the conclusion is the consequence of the premises. The philosophical analysis of logical consequence involves the questions: In what sense does a conclusion follow from its premises?  and What does it mean for a conclusion to be a consequence of premises?  All of philosophical logic is meant to provide accounts of the nature of logical consequence and the nature of logical truth.Logical consequence is necessary and formal, by way of examples that explain with formal proof and models of interpretation. A sentence is said to be a logical consequence of a set of sentences, for a given language, if and only if, using only logic (i.e. without regard to any personal interpretations of the sentences) the sentence must be true if every sentence in the set is true.Logicians make precise accounts of logical consequence regarding a given language \n  \n    \n      \n        \n          \n            L\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {L}}}\n  , either by constructing a deductive system for \n  \n    \n      \n        \n          \n            L\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {L}}}\n   or by formal intended semantics for language \n  \n    \n      \n        \n          \n            L\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {L}}}\n  .  The Polish logician Alfred Tarski identified three features of an adequate characterization of entailment: (1) The logical consequence relation relies on the logical form of the sentences, (2) The relation is a priori, i.e. it can be determined with or without regard to empirical evidence (sense experience), and (3) The logical consequence relation has a modal component.",
  },
  {
    id: 290,
    content:
      "In mathematical statistics, the Kullback\u2013Leibler divergence (also called relative entropy) is a measure of how one probability distribution is different from a second, reference probability distribution. Applications include characterizing the relative (Shannon) entropy in information systems, randomness in continuous time-series, and information gain when comparing statistical models of inference. In contrast to variation of information, it is a distribution-wise asymmetric measure and thus does not qualify as a statistical metric of spread - it also does not satisfy the triangle inequality. In the simple case, a Kullback\u2013Leibler divergence of 0 indicates that the two distributions in question are identical. In simplified terms, it is a measure of surprise, with diverse applications such as applied statistics, fluid mechanics, neuroscience and machine learning.",
  },
  { id: 291, content: "" },
  {
    id: 292,
    content:
      "In linear algebra, the singular value decomposition (SVD) is a factorization of a real or complex matrix that generalizes the eigendecomposition of a square normal matrix to any \n  \n    \n      \n        m\n        \u00d7\n        n\n      \n    \n    {\\displaystyle m\\times n}\n   matrix via an extension of the polar decomposition.\nSpecifically, the singular value decomposition of an \n  \n    \n      \n        m\n        \u00d7\n        n\n      \n    \n    {\\displaystyle m\\times n}\n   real or complex matrix \n  \n    \n      \n        \n          M\n        \n      \n    \n    {\\displaystyle \\mathbf {M} }\n   is a factorization of the form \n  \n    \n      \n        \n          U\n          \u03a3\n          \n            V\n            \n              \u2217\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {U\\Sigma V^{*}} }\n  , where \n  \n    \n      \n        \n          U\n        \n      \n    \n    {\\displaystyle \\mathbf {U} }\n   is an \n  \n    \n      \n        m\n        \u00d7\n        m\n      \n    \n    {\\displaystyle m\\times m}\n   real or complex unitary matrix, \n  \n    \n      \n        \n          \u03a3\n        \n      \n    \n    {\\displaystyle \\mathbf {\\Sigma } }\n   is an \n  \n    \n      \n        m\n        \u00d7\n        n\n      \n    \n    {\\displaystyle m\\times n}\n   rectangular diagonal matrix with non-negative real numbers on the diagonal, and \n  \n    \n      \n        \n          V\n        \n      \n    \n    {\\displaystyle \\mathbf {V} }\n   is an \n  \n    \n      \n        n\n        \u00d7\n        n\n      \n    \n    {\\displaystyle n\\times n}\n   real or complex unitary matrix.  If \n  \n    \n      \n        \n          M\n        \n      \n    \n    {\\displaystyle \\mathbf {M} }\n   is real, \n  \n    \n      \n        \n          U\n        \n      \n    \n    {\\displaystyle \\mathbf {U} }\n   and \n  \n    \n      \n        \n          \n            V\n            \n              T\n            \n          \n        \n        =\n        \n          \n            V\n            \n              \u2217\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {V^{T}} =\\mathbf {V^{*}} }\n   are real orthonormal matrices. \nThe diagonal entries \n  \n    \n      \n        \n          \u03c3\n          \n            i\n          \n        \n        =\n        \n          \u03a3\n          \n            i\n            i\n          \n        \n      \n    \n    {\\displaystyle \\sigma _{i}=\\Sigma _{ii}}\n   of \n  \n    \n      \n        \n          \u03a3\n        \n      \n    \n    {\\displaystyle \\mathbf {\\Sigma } }\n   are known as the singular values of \n  \n    \n      \n        \n          M\n        \n      \n    \n    {\\displaystyle \\mathbf {M} }\n  .  The number of non-zero singular values is equal to the rank of \n  \n    \n      \n        \n          M\n        \n      \n    \n    {\\displaystyle \\mathbf {M} }\n  .  The columns of \n  \n    \n      \n        \n          U\n        \n      \n    \n    {\\displaystyle \\mathbf {U} }\n   and the columns of \n  \n    \n      \n        \n          V\n        \n      \n    \n    {\\displaystyle \\mathbf {V} }\n   are called the left-singular vectors and right-singular vectors of \n  \n    \n      \n        \n          M\n        \n      \n    \n    {\\displaystyle \\mathbf {M} }\n  , respectively.\nThe SVD is not unique. It is always possible to choose the decomposition so that the singular values \n  \n    \n      \n        \n          \u03a3\n          \n            i\n            i\n          \n        \n      \n    \n    {\\displaystyle \\Sigma _{ii}}\n  are in descending order.  In this case, \u03a3 (but not always U and V) is uniquely determined by M.  \nThe term sometimes refers to the compact SVD, a similar decomposition \n  \n    \n      \n        \n          M\n        \n        =\n        \n          U\n          \u03a3\n          \n            V\n            \n              \u2217\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {M} =\\mathbf {U\\Sigma V^{*}} }\n   in which \u03a3 is square diagonal of size \n  \n    \n      \n        r\n        \u00d7\n        r\n      \n    \n    {\\displaystyle r\\times r}\n  , where \n  \n    \n      \n        r\n        \u2264\n        min\n        {\n        m\n        ,\n        n\n        }\n      \n    \n    {\\displaystyle r\\leq \\min\\{m,n\\}}\n   is the rank of M, and has only the non-zero singular values. In this variant, \n  \n    \n      \n        \n          U\n        \n      \n    \n    {\\displaystyle \\mathbf {U} }\n   is an \n  \n    \n      \n        m\n        \u00d7\n        r\n      \n    \n    {\\displaystyle m\\times r}\n   matrix and \n  \n    \n      \n        \n          V\n        \n      \n    \n    {\\displaystyle \\mathbf {V} }\n   is an \n  \n    \n      \n        n\n        \u00d7\n        r\n      \n    \n    {\\displaystyle n\\times r}\n   matrix, such that \n  \n    \n      \n        \n          \n            U\n            \n              \u2217\n            \n          \n          U\n        \n        =\n        \n          \n            V\n            \n              \u2217\n            \n          \n          V\n        \n        =\n        \n          \n            I\n          \n          \n            r\n            \u00d7\n            r\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {U^{*}U} =\\mathbf {V^{*}V} =\\mathbf {I} _{r\\times r}}\n  .\nMathematical applications of the SVD include computing the pseudoinverse, matrix approximation, and determining the rank, range, and null space of a matrix.  The SVD is also extremely useful in all areas of science, engineering, and statistics, such as signal processing, least squares fitting of data, and process control.",
  },
  {
    id: 293,
    content:
      "In the domain of physics and probability, a Markov random field (often abbreviated as MRF), Markov network or undirected graphical model is a set of random variables having a Markov property described by an undirected graph. In other words, a random field is said to be a Markov random field if it satisfies Markov properties.\nA Markov network or MRF is similar to a Bayesian network in its representation of dependencies; the differences being that Bayesian networks are directed and acyclic, whereas Markov networks are undirected and may be cyclic. Thus, a Markov network can represent certain dependencies that a Bayesian network cannot (such as cyclic dependencies); on the other hand, it can't represent certain dependencies that a Bayesian network can (such as induced dependencies). The underlying graph of a Markov random field may be finite or infinite.\nWhen the joint probability density of the random variables is strictly positive, it is also referred to as a Gibbs random field, because, according to the Hammersley\u2013Clifford theorem, it can then be represented by a Gibbs measure for an appropriate (locally defined) energy function. The prototypical Markov random field is the Ising model; indeed, the Markov random field was introduced as the general setting for the Ising model.\nIn the domain of artificial intelligence, a Markov random field is used to model various low- to mid-level tasks in image processing and computer vision.",
  },
  {
    id: 294,
    content:
      "Variable elimination (VE) is a simple and general exact inference algorithm in probabilistic graphical models, such as Bayesian networks and Markov random fields. It can be used for inference of maximum a posteriori (MAP) state or estimation of conditional or marginal distributions over a subset of variables. The algorithm has exponential time complexity, but could be efficient in practice for the low-treewidth graphs, if the proper elimination order is used.\n\n",
  },
  {
    id: 295,
    content:
      "In computer science, message passing is a technique for invoking behavior (i.e., running a program) on a computer. The invoking program sends a message to a process (which may be an actor or object) and relies on that process and its supporting infrastructure to select and then run the code it selects. Message passing differs from conventional programming where a process, subroutine, or function is directly invoked by name. Message passing is key to some models of concurrency and object-oriented programming.\nMessage passing is used ubiquitously in modern computer software. It is used as a way for the objects that make up a program to work with each other and as a means for objects and systems running on different computers (e.g., the Internet) to interact. Message passing may be implemented by various mechanisms, including channels.\n\n",
  },
  {
    id: 296,
    content:
      "In statistics, the graphical lasso is a sparse penalized maximum likelihood estimator for the concentration or precision matrix (inverse of covariance matrix) of a multivariate elliptical distribution. The original variant was formulated to solve Dempster's covariance selection problem for the multivariate Gaussian distribution when observations were limited. Subsequently, the optimization algorithms to solve this problem were improved and extended  to other types of estimators and distributions.",
  },
  {
    id: 297,
    content:
      'In control engineering, a state-space representation is a mathematical model of a physical system as a set of input, output and state variables related by first-order differential equations or difference equations. State variables are variables whose values evolve through time in a way that depends on the values they have at any given time and also depends on the externally imposed values of input variables. Output variables\u2019 values depend on the values of the state variables.\nThe "state space" is the Euclidean space in which the variables on the axes are the state variables. The state of the system can be represented as a vector within that space.\nTo abstract from the number of inputs, outputs and states, these variables are expressed as vectors. Additionally, if the dynamical system is linear, time-invariant, and finite-dimensional, then the differential and algebraic equations may be written in matrix form.\nThe state-space method is characterized by significant algebraization of general system theory, which makes it possible to use Kronecker vector-matrix structures. The capacity of these structures can be efficiently applied to research systems with modulation or without it.         \nThe state-space representation (also known as the "time-domain approach") provides a convenient and compact way to model and analyze systems with multiple inputs and outputs. With \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   inputs and \n  \n    \n      \n        q\n      \n    \n    {\\displaystyle q}\n   outputs, we would otherwise have to write down \n  \n    \n      \n        q\n        \u00d7\n        p\n      \n    \n    {\\displaystyle q\\times p}\n   Laplace transforms to encode all the information about a system. Unlike the frequency domain approach, the use of the state-space representation is not limited to systems with linear components and zero initial conditions. The state-space model is used in many different areas. In econometrics, the state-space model can be used for forecasting stock prices and numerous other variables.',
  },
  {
    id: 298,
    content:
      "Belief propagation, also known as sum-product message passing, is a message-passing algorithm for performing inference on graphical models, such as Bayesian networks and Markov random fields. It calculates the marginal distribution for each unobserved node (or variable), conditional on any observed nodes (or variables). Belief propagation is commonly used in artificial intelligence and information theory and has demonstrated empirical success in numerous applications including low-density parity-check codes, turbo codes, free energy approximation, and satisfiability.The algorithm was first proposed by Judea Pearl in 1982, who formulated it as an exact inference algorithm on trees, which was later extended to polytrees. While it is not exact on general graphs, it has been shown to be a useful approximate algorithm.If X={Xi} is a set of discrete random variables with a joint mass function p, the marginal distribution of a single Xi is simply the summation of p over all other variables:\n\n  \n    \n      \n        \n          p\n          \n            \n              X\n              \n                i\n              \n            \n          \n        \n        (\n        \n          x\n          \n            i\n          \n        \n        )\n        =\n        \n          \u2211\n          \n            \n              \n                x\n              \n              \u2032\n            \n            :\n            \n              x\n              \n                i\n              \n              \u2032\n            \n            =\n            \n              x\n              \n                i\n              \n            \n          \n        \n        p\n        (\n        \n          \n            x\n          \n          \u2032\n        \n        )\n        .\n      \n    \n    {\\displaystyle p_{X_{i}}(x_{i})=\\sum _{\\mathbf {x} ':x'_{i}=x_{i}}p(\\mathbf {x} ').}\n  However, this quickly becomes computationally prohibitive: if there are 100 binary variables, then one needs to sum over 299 \u2248 6.338 \u00d7 1029 possible values. By exploiting the polytree structure, belief propagation allows the marginals to be computed much more efficiently.",
  },
  {
    id: 299,
    content:
      "In physics and probability theory, mean-field theory (aka MFT or rarely self-consistent field theory) studies the behavior of high-dimensional random (stochastic) models by studying a simpler model that approximates the original by averaging over degrees of freedom. Such models consider many individual components that interact with each other. In MFT, the effect of all the other individuals on any given individual is approximated by a single averaged effect, thus reducing a many-body problem to a one-body problem.\nThe main idea of MFT is to replace all interactions to any one body with an average or effective interaction, sometimes called a molecular field. This reduces any multi-body problem into an effective one-body problem.  The ease of solving MFT problems means that some insight into the behavior of the system can be obtained at a lower computational cost.\nMFT has since been applied to a wide range of fields outside of physics, including statistical inference, graphical models, neuroscience, artificial intelligence, epidemic models, queueing theory, computer network performance and game theory, as in the Quantal response equilibrium.",
  },
  {
    id: 300,
    content:
      'In probability theory, Dirichlet processes (after Peter Gustav Lejeune Dirichlet) are a family of stochastic processes whose realizations are probability distributions. In other words, a Dirichlet process is a probability distribution whose range is itself a set of probability distributions. It is often used in Bayesian inference to describe the prior knowledge about the distribution of random variables\u2014how likely it is that the random variables are distributed according to one or another particular distribution.\nThe Dirichlet process is specified by a base distribution \n  \n    \n      \n        H\n      \n    \n    {\\displaystyle H}\n   and a positive real number \n  \n    \n      \n        \u03b1\n      \n    \n    {\\displaystyle \\alpha }\n   called the concentration parameter (also known as scaling parameter). The base distribution is the expected value of the process, i.e., the Dirichlet process draws distributions "around" the base distribution the way a normal distribution draws real numbers around its mean. However, even if the base distribution is continuous, the distributions drawn from the Dirichlet process are almost surely discrete. The scaling parameter specifies how strong this discretization is: in the limit of \n  \n    \n      \n        \u03b1\n        \u2192\n        0\n      \n    \n    {\\displaystyle \\alpha \\rightarrow 0}\n  , the realizations are all concentrated at a single value, while in the limit of \n  \n    \n      \n        \u03b1\n        \u2192\n        \u221e\n      \n    \n    {\\displaystyle \\alpha \\rightarrow \\infty }\n   the realizations become continuous. Between the two extremes the realizations are discrete distributions with less and less concentration as \n  \n    \n      \n        \u03b1\n      \n    \n    {\\displaystyle \\alpha }\n   increases.\nThe Dirichlet process can also be seen as the infinite-dimensional generalization of the Dirichlet distribution. In the same way as the Dirichlet distribution is the conjugate prior for the categorical distribution, the Dirichlet process is the conjugate prior for infinite, nonparametric discrete distributions. A particularly important application of Dirichlet processes is as a prior probability distribution in infinite mixture models.\nThe Dirichlet process was formally introduced by Thomas Ferguson in 1973\nand has since been applied in data mining and machine learning, among others for natural language processing, computer vision and bioinformatics.',
  },
  {
    id: 301,
    content:
      'The mathematical concept of a Hilbert space, named after David Hilbert, generalizes the notion of Euclidean space. It extends the methods of vector algebra and calculus from the two-dimensional Euclidean plane and three-dimensional space to spaces with any finite or infinite number of dimensions. A Hilbert space is an abstract vector space possessing the structure of an inner product that allows length and angle to be measured. Furthermore, Hilbert spaces are complete: there are enough limits in the space to allow the techniques of calculus to be used.\nHilbert spaces arise naturally and frequently in mathematics and physics, typically as infinite-dimensional function spaces. The earliest Hilbert spaces were studied from this point of view in the first decade of the 20th century by David Hilbert, Erhard Schmidt, and Frigyes Riesz. They are indispensable tools in the theories of partial differential equations, quantum mechanics, Fourier analysis (which includes applications to signal processing and heat transfer), and ergodic theory (which forms the mathematical underpinning of thermodynamics). John von Neumann coined the term Hilbert space for the abstract concept that underlies many of these diverse applications. The success of Hilbert space methods ushered in a very fruitful era for functional analysis. Apart from the classical Euclidean spaces, examples of Hilbert spaces include spaces of square-integrable functions, spaces of sequences, Sobolev spaces consisting of generalized functions, and Hardy spaces of holomorphic functions.\nGeometric intuition plays an important role in many aspects of Hilbert space theory. Exact analogs of the Pythagorean theorem and parallelogram law hold in a Hilbert space. At a deeper level, perpendicular projection onto a subspace (the analog of "dropping the altitude" of a triangle) plays a significant role in optimization problems and other aspects of the theory. An element of a Hilbert space can be uniquely specified by its coordinates with respect to a set of coordinate axes (an orthonormal basis), in analogy with Cartesian coordinates in the plane. When that set of axes is countably infinite, the Hilbert space can also be usefully thought of in terms of the space of infinite sequences that are square-summable. The latter space is often in the older literature referred to as the Hilbert space. Linear operators on a Hilbert space are likewise fairly concrete objects: in good cases, they are simply transformations that stretch the space by different factors in mutually perpendicular directions in a sense that is made precise by the study of their spectrum.',
  },
  {
    id: 302,
    content:
      "The Graphical Kernel System (GKS) was the first ISO standard for low-level computer graphics, introduced in 1977. A draft international standard was circulated for review in September 1983.\nFinal ratification of the standard was achieved in 1985.",
  },
  {
    id: 303,
    content:
      "In mathematical optimization, the Karush\u2013Kuhn\u2013Tucker (KKT) conditions, also known as the Kuhn\u2013Tucker conditions, are first derivative tests (sometimes called first-order necessary conditions) for a solution in nonlinear programming to be optimal, provided that some regularity conditions are satisfied. \nAllowing inequality constraints, the KKT approach to nonlinear programming generalizes the method of Lagrange multipliers, which allows only equality constraints. Similar to the Lagrange approach, the constrained maximization (minimization) problem is rewritten as a Lagrange function whose optimal point is a saddle point, i.e. a global maximum (minimum) over the domain of the choice variables and a global minimum (maximum) over the multipliers, which is why the Karush\u2013Kuhn\u2013Tucker theorem is sometimes referred to as the saddle-point theorem.The KKT conditions were originally named after Harold W. Kuhn and Albert W. Tucker, who first published the conditions in 1951. Later scholars discovered that the necessary conditions for this problem had been stated by William Karush in his  master's thesis in 1939.\n\n",
  },
  {
    id: 304,
    content:
      "In mathematical optimization theory, duality or the duality principle is the principle that optimization problems may be viewed from either of two perspectives, the primal problem or the dual problem. The solution to the dual problem provides a lower bound to the solution of the primal (minimization) problem. However in general the optimal values of the primal and dual problems need not be equal. Their difference is called the duality gap. For convex optimization problems, the duality gap is zero under a constraint qualification condition.",
  },
  {
    id: 305,
    content:
      "A Markov chain is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. In continuous-time, it is known as a Markov process. It is named after the Russian mathematician Andrey Markov.\nMarkov chains have many applications as statistical models of real-world processes, such as studying cruise control systems in motor vehicles, queues or lines of customers arriving at an airport, currency exchange rates and animal population dynamics.Markov processes are the basis for general stochastic simulation methods known as Markov chain Monte Carlo, which are used for simulating sampling from complex probability distributions, and have found application in Bayesian statistics and artificial intelligence.The adjective Markovian is used to describe something that is related to a Markov process.",
  },
  {
    id: 306,
    content:
      "Unsupervised learning is a type of machine learning that looks for previously undetected patterns in a data set with no pre-existing labels and with a minimum of human supervision. In contrast to supervised learning that usually makes use of human-labeled data, unsupervised learning, also known as self-organization allows for modeling of probability densities over inputs. It forms one of the three main categories of machine learning, along with supervised and reinforcement learning. Semi-supervised learning, a related variant, makes use of supervised and unsupervised techniques.\nTwo of the main methods used in unsupervised learning are principal component and cluster analysis. Cluster analysis is used in unsupervised learning to group, or segment, datasets with shared attributes in order to extrapolate algorithmic relationships.  Cluster analysis is a branch of machine learning that groups the data that has not been labelled, classified or categorized. Instead of responding to feedback, cluster analysis identifies commonalities in the data and reacts based on the presence or absence of such commonalities in each new piece of data. This approach helps detect anomalous data points that do not fit into either group.\nA central application of unsupervised learning is in the field of density estimation in statistics, though unsupervised learning encompasses many other domains involving summarizing and explaining data features.  It could be contrasted with supervised learning by saying that whereas supervised learning intends to infer a conditional probability distribution \n  \n    \n      \n        \n          p\n          \n            X\n          \n        \n        (\n        x\n        \n        \n          |\n        \n        \n        y\n        )\n      \n    \n    {\\textstyle p_{X}(x\\,|\\,y)}\n   conditioned on the label \n  \n    \n      \n        y\n      \n    \n    {\\textstyle y}\n   of input data; unsupervised learning intends to infer an a priori probability distribution \n  \n    \n      \n        \n          p\n          \n            X\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\textstyle p_{X}(x)}\n  .\nGenerative adversarial networks can also be used with supervised learning, though they can also be applied to unsupervised and reinforcement techniques.",
  },
  {
    id: 307,
    content:
      "The Lambretta GP/DL range was the final range of classic Lambrettas to be produced before Lambretta was sold to British Leyland Motor Corporation in 1971. The range was called the DL in most countries, but was called the GP (standing for Grand Prix) in Britain and some other countries. This was in order to associate the scooters with Formula One which was extremely popular and successful in the late 1960s.",
  },
  { id: 308, content: "" },
  {
    id: 309,
    content:
      "Meta learning is a branch of metacognition concerned with learning about one's own learning and learning processes. \nThe term comes from the meta prefix's modern meaning of an abstract recursion, or \"X about X,\" similar to its use in metaknowledge, metamemory, and meta-emotion.",
  },
  {
    id: 310,
    content:
      'In statistics, a mixture model is a probabilistic model for representing the presence of subpopulations within an overall population, without requiring that an observed data set should identify the sub-population to which an individual observation belongs. Formally a mixture model corresponds to the mixture distribution that represents the probability distribution of observations in the overall population. However, while problems associated with "mixture distributions" relate to deriving the properties of the overall population from those of the sub-populations, "mixture models" are used to make statistical inferences about the properties of the sub-populations given only observations on the pooled population, without sub-population identity information.\nSome ways of implementing mixture models involve steps that attribute postulated sub-population-identities to individual observations (or weights towards such sub-populations), in which case these can be regarded as types of unsupervised learning or clustering procedures. However, not all inference procedures involve such steps.\nMixture models should not be confused with models for compositional data, i.e., data whose components are constrained to sum to a constant value (1, 100%, etc.). However, compositional models can be thought of as mixture models, where members of the population are sampled at random. Conversely, mixture models can be thought of as compositional models, where the total size reading population has been normalized to 1.',
  },
  {
    id: 311,
    content:
      "High-dimensional data, meaning data that requires more than two or three dimensions to represent, can be  difficult to interpret. One approach to simplification is to assume that the data of interest lie on an embedded non-linear manifold within the higher-dimensional space. If the manifold is of low enough dimension, the data can be visualised in the low-dimensional space.\n\nBelow is a summary of some of the important algorithms from the history of manifold learning and nonlinear dimensionality reduction (NLDR). Many of these non-linear dimensionality reduction methods are related to the linear methods listed below. Non-linear methods can be broadly classified into two groups: those that provide a mapping (either from the high-dimensional space to the low-dimensional embedding or vice versa), and those that just give a visualisation.  In the context of machine learning, mapping methods may be viewed as a preliminary feature extraction step, after which pattern recognition algorithms are applied. Typically those that just give a visualisation are based on proximity data \u2013 that is, distance measurements.",
  },
  {
    id: 312,
    content:
      "AlphaGo is a computer program that plays the board game Go. It was developed by DeepMind Technologies which was later acquired by Google. AlphaGo had three far more powerful successors, called AlphaGo Master, AlphaGo Zero and AlphaZero.\nIn October 2015, the original AlphaGo became the first computer Go program to beat a human professional Go player without handicap on a full-sized 19\u00d719 board. In March 2016, it beat Lee Sedol in a five-game match, the first time a computer Go program has beaten a 9-dan professional without handicap. Although it lost to Lee Sedol in the fourth game, Lee resigned in the final game, giving a final score of 4 games to 1 in favour of AlphaGo. In recognition of the victory, AlphaGo was awarded an honorary 9-dan by the Korea Baduk Association. The lead up and the challenge match with Lee Sedol were documented in a documentary film also titled AlphaGo, directed by Greg Kohs. It was chosen by Science as one of the Breakthrough of the Year runners-up on 22 December 2016.At the 2017 Future of Go Summit, its successor AlphaGo Master beat Ke Jie, the world No.1 ranked player at the time, in a three-game match (the even more powerful AlphaGo Zero already existed but was not yet announced). After this, AlphaGo was awarded professional 9-dan by the Chinese Weiqi Association.AlphaGo and its successors use a Monte Carlo tree search algorithm to find its moves based on knowledge previously \"learned\" by machine learning, specifically by an artificial neural network (a deep learning method) by extensive training, both from human and computer play.  A neural network is trained to predict AlphaGo's own move selections and also the winner's games. This neural net improves the strength of tree search, resulting in higher quality of move selection and stronger self-play in the next iteration.\nAfter the match between AlphaGo and Ke Jie, DeepMind retired AlphaGo, while continuing AI research in other areas. Starting from a 'blank page', with only a short training period, AlphaGo Zero achieved a 100-0 victory against the champion-defeating AlphaGo, while its successor, the self-taught AlphaZero, is currently perceived as the world's top player in Go as well as possibly in  chess.",
  },
  {
    id: 313,
    content:
      "An autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore signal \u201cnoise\u201d. Along with the reduction side, a reconstructing side is learnt, where the autoencoder tries to generate from the reduced encoding a representation as close as possible to its original input, hence its name. Several variants exist to the basic model, with the aim of forcing the learned representations of the input to assume useful properties. Examples are the regularized autoencoders (Sparse, Denoising and Contractive autoencoders), proven effective in learning representations for subsequent classification tasks, and Variational autoencoders, with their recent applications as generative models. Autoencoders are effectively used for solving many applied problems, from face recognition to acquiring the semantic meaning of words.",
  },
  {
    id: 314,
    content:
      'A restricted Boltzmann machine (RBM) is a generative stochastic artificial neural network that can learn a probability distribution over its set of inputs.\nRBMs were initially invented under the name Harmonium by Paul Smolensky in 1986,\nand rose to prominence after Geoffrey Hinton and collaborators invented fast learning algorithms for them in the mid-2000. RBMs have found applications in dimensionality reduction,classification,collaborative filtering,  feature learning,topic modelling\nand even many body quantum mechanics. They can be trained in either supervised or unsupervised ways, depending on the task.\nAs their name implies, RBMs are a variant of Boltzmann machines, with the restriction that their neurons must form a bipartite graph: \na pair of nodes from each of the two groups of units (commonly referred to as the "visible" and "hidden" units respectively) may have a symmetric connection between them; and there are no connections between nodes within a group. By contrast, "unrestricted" Boltzmann machines may have connections between hidden units. This restriction allows for more efficient training algorithms than are available for the general class of Boltzmann machines, in particular the gradient-based contrastive divergence algorithm.Restricted Boltzmann machines can also be used in deep learning networks. In particular, deep belief networks can be formed by "stacking" RBMs and optionally fine-tuning the resulting deep network with gradient descent and backpropagation.',
  },
  {
    id: 315,
    content:
      'Given a collection of points in two, three, or higher dimensional space, a "best fitting" line can be defined as one that minimizes the average squared distance from a point to the line. The next best-fitting line can be similarly chosen from directions perpendicular to the first. Repeating this process yields an orthogonal basis in which different individual dimensions of the data are uncorrelated. These basis vectors are called principal components, and several related procedures principal component analysis (PCA). \nPCA is mostly used as a tool in exploratory data analysis and for making predictive models. It is often used to visualize genetic distance and relatedness between populations. PCA is either done in the following 2 steps:\n\ncalculating the data covariance (or correlation) matrix of the original data\nperforming eigenvalue decomposition on the covariance matrixor by singular value decomposition of a design matrix. Usually the original data is normalized before performing the PCA. The normalization of each attribute consists of mean centering \u2013 subtracting each data value from its variable\'s measured mean so that its empirical mean (average) is zero. Some fields, in addition to normalizing the mean, do so for each variable\'s variance (to make it equal to 1); see z-scores. The results of a PCA are usually discussed in terms of component scores, sometimes called factor scores (the transformed variable values corresponding to a particular data point), and loadings (the weight by which each standardized original variable should be multiplied to get the component score). If component scores are standardized to unit variance, loadings must contain the data variance in them (and that is the magnitude of eigenvalues). If component scores are not standardized (therefore they contain the data variance) then loadings must be unit-scaled, ("normalized") and these weights are called eigenvectors; they are the cosines of orthogonal rotation of variables into principal components or back.\nPCA is the simplest of the true eigenvector-based multivariate analyses. Often, its operation can be thought of as revealing the internal structure of the data in a way that best explains the variance in the data. If a multivariate dataset is visualised as a set of coordinates in a high-dimensional data space (1 axis per variable), PCA can supply the user with a lower-dimensional picture, a projection of this object when viewed from its most informative viewpoint. This is done by using only the first few principal components so that the dimensionality of the transformed data is reduced.\nPCA is closely related to factor analysis.  Factor analysis typically incorporates more domain specific assumptions about the underlying structure and solves eigenvectors of a slightly different matrix.\nPCA is also related to canonical correlation analysis (CCA). CCA defines coordinate systems that optimally describe the cross-covariance between two datasets while PCA defines a new orthogonal coordinate system that optimally describes variance in a single dataset.Robust and L1-norm-based variants of standard PCA have also been proposed.',
  },
  {
    id: 316,
    content:
      "In statistics, quality assurance, and survey methodology, sampling is the selection of a subset (a statistical sample) of individuals from within a statistical population to estimate characteristics of the whole population. Statisticians attempt for the samples to represent the population in question. Two advantages of sampling are lower cost and faster data collection than measuring the entire population.",
  },
  {
    id: 317,
    content:
      'In statistics, canonical-correlation analysis (CCA), also called canonical variates analysis, is a way of inferring information from cross-covariance matrices. If we have two vectors X = (X1, ..., Xn) and Y = (Y1, ..., Ym)  of random variables, and there are correlations among the variables, then canonical-correlation analysis will find linear combinations of X and Y which have maximum correlation with each other. T. R. Knapp notes that "virtually all of the commonly encountered parametric tests of significance can be treated as special cases of canonical-correlation analysis, which is the general procedure for investigating the relationships between two sets of variables." The method was first introduced by Harold Hotelling in 1936, although in the context of angles between flats the mathematical concept was published by Jordan in 1875.',
  },
  {
    id: 318,
    content:
      "A visual dictionary is a dictionary that primarily uses pictures to illustrate the meaning of words.  Visual dictionaries are often organized by themes, instead of being an alphabetical list of words.  For each theme, an image is labeled with the correct word to identify each component of the item in question.  Visual dictionaries can be monolingual or multilingual, providing the names of items in several languages.  An index of all defined words is usually included to assist finding the correct illustration that defines the word.\nSome international visual dictionary publishers include Oxford University Press, QA International and Dorling Kindersley.",
  },
  {
    id: 319,
    content:
      "In linguistics, coreference, sometimes written co-reference, occurs when two or more expressions in a text refer to the same person or thing; they have the same referent, e.g. Bill said he would come; the proper noun Bill and the pronoun he refer to the same person, namely to Bill. Coreference is the main concept underlying binding phenomena in the field of syntax. The theory of binding explores the syntactic relationship that exists between coreferential expressions in sentences and texts. When two expressions are coreferential, the one  is usually a full form (the antecedent) and the other is an abbreviated form (a proform or anaphor). Linguists use indices to show coreference, as with the i index in the example Billi said hei would come. The two expressions with the same reference are coindexed, hence in this example Bill and he are coindexed, indicating that they should be interpreted as coreferential.",
  },
  {
    id: 320,
    content:
      "A residual neural network (ResNet) is an artificial neural network (ANN) of a kind that builds on constructs known from pyramidal cells in the cerebral cortex. Residual neural networks do this by utilizing skip connections, or shortcuts to jump over some layers. Typical ResNet models are implemented with double- or triple- layer skips that contain nonlinearities (ReLU) and batch normalization in between. An additional weight matrix may be used to learn the skip weights; these models are known as HighwayNets.Models with several parallel skips are referred to as DenseNets. In the context of residual neural networks, a non-residual network may be described as a plain network.",
  },
  {
    id: 321,
    content:
      "In machine learning, na\u00efve Bayes classifiers are a family of simple \"probabilistic classifiers\" based on applying Bayes' theorem with strong (na\u00efve) independence assumptions between the features. They are among the simplest Bayesian network models. But they could be coupled with Kernel density estimation and achieve higher accuracy levels.Na\u00efve Bayes has been studied extensively since the 1960s. It was introduced (though not under that name) into the text retrieval community in the early 1960s, and remains a popular (baseline) method for text categorization, the problem of judging documents as belonging to one category or the other (document categorization)(such as spam or legitimate, sports or politics, etc.) with word frequencies as the features. With appropriate pre-processing, it is competitive in this domain with more advanced methods including support vector machines. It also finds application in automatic medical diagnosis.Na\u00efve Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features/predictors) in a learning problem. Maximum-likelihood training can be done by evaluating a closed-form expression, which takes linear time, rather than by expensive iterative approximation as used for many other types of classifiers.\nIn the statistics and computer science literature, naive Bayes models are known under a variety of names, including simple Bayes and independence Bayes. All these names reference the use of Bayes' theorem in the classifier's decision rule, but na\u00efve Bayes is not (necessarily) a Bayesian method.\n\n",
  },
  {
    id: 322,
    content:
      "Word2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space. Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located close to one another in the space.Word2vec was created and published in 2013 by a team of researchers led by Tomas Mikolov at Google and patented. The algorithm has been subsequently analysed and explained by other researchers. Embedding vectors created using the Word2vec algorithm have some advantages compared to earlier algorithms such as latent semantic analysis.\n\n",
  },
];
